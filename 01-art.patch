diff --git a/Android.mk b/Android.mk
index bebe759..412673e 100644
--- a/Android.mk
+++ b/Android.mk
@@ -371,9 +371,11 @@ include $(BUILD_PHONY_PACKAGE)
 include $(CLEAR_VARS)
 LOCAL_MODULE := com.android.runtime
 LOCAL_IS_HOST_MODULE := true
+ifneq ($(DISABLE_APEX_TEST_MODULE),true)
 ifneq ($(HOST_OS),darwin)
   LOCAL_REQUIRED_MODULES += $(APEX_TEST_MODULE)
 endif
+endif
 include $(BUILD_PHONY_PACKAGE)
 
 # Create canonical name -> file name symlink in the symbol directory
diff --git a/benchmark/micro-native/micro_native.cc b/benchmark/micro-native/micro_native.cc
index dffbf3b..e70366c 100644
--- a/benchmark/micro-native/micro_native.cc
+++ b/benchmark/micro-native/micro_native.cc
@@ -38,7 +38,7 @@
 static void NativeMethods_emptyJniStaticSynchronizedMethod0(JNIEnv*, jclass) { }
 static void NativeMethods_emptyJniSynchronizedMethod0(JNIEnv*, jclass) { }
 
-static JNINativeMethod gMethods_NormalOnly[] = {
+static const JNINativeMethod gMethods_NormalOnly[] = {
   NATIVE_METHOD(NativeMethods, emptyJniStaticSynchronizedMethod0, "()V"),
   NATIVE_METHOD(NativeMethods, emptyJniSynchronizedMethod0, "()V"),
 };
@@ -53,7 +53,7 @@ static void NativeMethods_emptyJniStaticMethod6L(JNIEnv*, jclass, jobject, jarra
 static void NativeMethods_emptyJniStaticMethod0(JNIEnv*, jclass) { }
 static void NativeMethods_emptyJniStaticMethod6(JNIEnv*, jclass, int, int, int, int, int, int) { }
 
-static JNINativeMethod gMethods[] = {
+static const JNINativeMethod gMethods[] = {
   NATIVE_METHOD(NativeMethods, emptyJniMethod0, "()V"),
   NATIVE_METHOD(NativeMethods, emptyJniMethod6, "(IIIIII)V"),
   NATIVE_METHOD(NativeMethods, emptyJniMethod6L, "(Ljava/lang/String;[Ljava/lang/String;[[ILjava/lang/Object;[Ljava/lang/Object;[[[[Ljava/lang/Object;)V"),
@@ -72,7 +72,7 @@ static void NativeMethods_emptyJniStaticMethod6L_Fast(JNIEnv*, jclass, jobject,
 static void NativeMethods_emptyJniStaticMethod0_Fast(JNIEnv*, jclass) { }
 static void NativeMethods_emptyJniStaticMethod6_Fast(JNIEnv*, jclass, int, int, int, int, int, int) { }
 
-static JNINativeMethod gMethods_Fast[] = {
+static const JNINativeMethod gMethods_Fast[] = {
   NATIVE_METHOD(NativeMethods, emptyJniMethod0_Fast, "()V"),
   NATIVE_METHOD(NativeMethods, emptyJniMethod6_Fast, "(IIIIII)V"),
   NATIVE_METHOD(NativeMethods, emptyJniMethod6L_Fast, "(Ljava/lang/String;[Ljava/lang/String;[[ILjava/lang/Object;[Ljava/lang/Object;[[[[Ljava/lang/Object;)V"),
@@ -88,7 +88,7 @@ DEFINE_CRITICAL_JNI_METHOD(void, emptyJniStaticMethod0_1Critical)() { }
 DEFINE_NORMAL_JNI_METHOD(void,   emptyJniStaticMethod6_1Critical)(JNIEnv*, jclass, int, int, int, int, int, int) { }
 DEFINE_CRITICAL_JNI_METHOD(void, emptyJniStaticMethod6_1Critical)(int, int, int, int, int, int) { }
 
-static JNINativeMethod gMethods_Critical[] = {
+static const JNINativeMethod gMethods_Critical[] = {
   // Don't use NATIVE_METHOD because the name is mangled differently.
   { "emptyJniStaticMethod0_Critical", "()V",
         reinterpret_cast<void*>(NAME_CRITICAL_JNI_METHOD(emptyJniStaticMethod0_1Critical)) },
diff --git a/compiler/Android.bp b/compiler/Android.bp
index 52bd89f..1847995 100644
--- a/compiler/Android.bp
+++ b/compiler/Android.bp
@@ -388,6 +388,7 @@ art_cc_test {
         "jni/jni_cfi_test.cc",
         "optimizing/codegen_test.cc",
         "optimizing/load_store_analysis_test.cc",
+        "optimizing/load_store_elimination_test.cc",
         "optimizing/optimizing_cfi_test.cc",
         "optimizing/scheduler_test.cc",
     ],
diff --git a/compiler/driver/compiled_method_storage.cc b/compiler/driver/compiled_method_storage.cc
index 31062fb..03c906b 100644
--- a/compiler/driver/compiled_method_storage.cc
+++ b/compiler/driver/compiled_method_storage.cc
@@ -216,6 +216,9 @@ CompiledMethodStorage::ThunkMapKey CompiledMethodStorage::GetThunkMapKey(
   uint32_t custom_value1 = 0u;
   uint32_t custom_value2 = 0u;
   switch (linker_patch.GetType()) {
+    case linker::LinkerPatch::Type::kCallEntrypoint:
+      custom_value1 = linker_patch.EntrypointOffset();
+      break;
     case linker::LinkerPatch::Type::kBakerReadBarrierBranch:
       custom_value1 = linker_patch.GetBakerCustomValue1();
       custom_value2 = linker_patch.GetBakerCustomValue2();
diff --git a/compiler/linker/linker_patch.h b/compiler/linker/linker_patch.h
index f9e3930..1c523de 100644
--- a/compiler/linker/linker_patch.h
+++ b/compiler/linker/linker_patch.h
@@ -52,6 +52,7 @@ class LinkerPatch {
     kTypeBssEntry,
     kStringRelative,
     kStringBssEntry,
+    kCallEntrypoint,
     kBakerReadBarrierBranch,
   };
 
@@ -141,6 +142,15 @@ class LinkerPatch {
     return patch;
   }
 
+  static LinkerPatch CallEntrypointPatch(size_t literal_offset,
+                                         uint32_t entrypoint_offset) {
+    LinkerPatch patch(literal_offset,
+                      Type::kCallEntrypoint,
+                      /* target_dex_file= */ nullptr);
+    patch.entrypoint_offset_ = entrypoint_offset;
+    return patch;
+  }
+
   static LinkerPatch BakerReadBarrierBranchPatch(size_t literal_offset,
                                                  uint32_t custom_value1 = 0u,
                                                  uint32_t custom_value2 = 0u) {
@@ -216,6 +226,11 @@ class LinkerPatch {
     return pc_insn_offset_;
   }
 
+  uint32_t EntrypointOffset() const {
+    DCHECK(patch_type_ == Type::kCallEntrypoint);
+    return entrypoint_offset_;
+  }
+
   uint32_t GetBakerCustomValue1() const {
     DCHECK(patch_type_ == Type::kBakerReadBarrierBranch);
     return baker_custom_value1_;
@@ -249,6 +264,7 @@ class LinkerPatch {
     uint32_t type_idx_;           // Type index for Type patches.
     uint32_t string_idx_;         // String index for String patches.
     uint32_t intrinsic_data_;     // Data for IntrinsicObjects.
+    uint32_t entrypoint_offset_;  // Entrypoint offset in the Thread object.
     uint32_t baker_custom_value1_;
     static_assert(sizeof(method_idx_) == sizeof(cmp1_), "needed by relational operators");
     static_assert(sizeof(type_idx_) == sizeof(cmp1_), "needed by relational operators");
diff --git a/compiler/optimizing/code_generator.cc b/compiler/optimizing/code_generator.cc
index 2bbb570..3b5699b 100644
--- a/compiler/optimizing/code_generator.cc
+++ b/compiler/optimizing/code_generator.cc
@@ -64,6 +64,7 @@
 #include "ssa_liveness_analysis.h"
 #include "stack_map.h"
 #include "stack_map_stream.h"
+#include "string_builder_append.h"
 #include "thread-current-inl.h"
 #include "utils/assembler.h"
 
@@ -599,6 +600,57 @@ void CodeGenerator::GenerateInvokeCustomCall(HInvokeCustom* invoke) {
   InvokeRuntime(entrypoint, invoke, invoke->GetDexPc(), nullptr);
 }
 
+void CodeGenerator::CreateStringBuilderAppendLocations(HStringBuilderAppend* instruction,
+                                                       Location out) {
+  ArenaAllocator* allocator = GetGraph()->GetAllocator();
+  LocationSummary* locations =
+      new (allocator) LocationSummary(instruction, LocationSummary::kCallOnMainOnly);
+  locations->SetOut(out);
+  instruction->GetLocations()->SetInAt(instruction->FormatIndex(),
+                                       Location::ConstantLocation(instruction->GetFormat()));
+
+  uint32_t format = static_cast<uint32_t>(instruction->GetFormat()->GetValue());
+  uint32_t f = format;
+  PointerSize pointer_size = InstructionSetPointerSize(GetInstructionSet());
+  size_t stack_offset = static_cast<size_t>(pointer_size);  // Start after the ArtMethod*.
+  for (size_t i = 0, num_args = instruction->GetNumberOfArguments(); i != num_args; ++i) {
+    StringBuilderAppend::Argument arg_type =
+        static_cast<StringBuilderAppend::Argument>(f & StringBuilderAppend::kArgMask);
+    switch (arg_type) {
+      case StringBuilderAppend::Argument::kStringBuilder:
+      case StringBuilderAppend::Argument::kString:
+      case StringBuilderAppend::Argument::kCharArray:
+        static_assert(sizeof(StackReference<mirror::Object>) == sizeof(uint32_t), "Size check.");
+        FALLTHROUGH_INTENDED;
+      case StringBuilderAppend::Argument::kBoolean:
+      case StringBuilderAppend::Argument::kChar:
+      case StringBuilderAppend::Argument::kInt:
+      case StringBuilderAppend::Argument::kFloat:
+        locations->SetInAt(i, Location::StackSlot(stack_offset));
+        break;
+      case StringBuilderAppend::Argument::kLong:
+      case StringBuilderAppend::Argument::kDouble:
+        stack_offset = RoundUp(stack_offset, sizeof(uint64_t));
+        locations->SetInAt(i, Location::DoubleStackSlot(stack_offset));
+        // Skip the low word, let the common code skip the high word.
+        stack_offset += sizeof(uint32_t);
+        break;
+      default:
+        LOG(FATAL) << "Unexpected arg format: 0x" << std::hex
+            << (f & StringBuilderAppend::kArgMask) << " full format: 0x" << format;
+        UNREACHABLE();
+    }
+    f >>= StringBuilderAppend::kBitsPerArg;
+    stack_offset += sizeof(uint32_t);
+  }
+  DCHECK_EQ(f, 0u);
+
+  size_t param_size = stack_offset - static_cast<size_t>(pointer_size);
+  DCHECK_ALIGNED(param_size, kVRegSize);
+  size_t num_vregs = param_size / kVRegSize;
+  graph_->UpdateMaximumNumberOfOutVRegs(num_vregs);
+}
+
 void CodeGenerator::CreateUnresolvedFieldLocationSummary(
     HInstruction* field_access,
     DataType::Type field_type,
diff --git a/compiler/optimizing/code_generator.h b/compiler/optimizing/code_generator.h
index f70ecb6..3630c36 100644
--- a/compiler/optimizing/code_generator.h
+++ b/compiler/optimizing/code_generator.h
@@ -222,7 +222,19 @@ class CodeGenerator : public DeletableArenaObject<kArenaAllocCodeGenerator> {
   virtual Assembler* GetAssembler() = 0;
   virtual const Assembler& GetAssembler() const = 0;
   virtual size_t GetWordSize() const = 0;
-  virtual size_t GetFloatingPointSpillSlotSize() const = 0;
+
+  // Get FP register width in bytes for spilling/restoring in the slow paths.
+  //
+  // Note: In SIMD graphs this should return SIMD register width as all FP and SIMD registers
+  // alias and live SIMD registers are forced to be spilled in full size in the slow paths.
+  virtual size_t GetSlowPathFPWidth() const {
+    // Default implementation.
+    return GetCalleePreservedFPWidth();
+  }
+
+  // Get FP register width required to be preserved by the target ABI.
+  virtual size_t GetCalleePreservedFPWidth() const  = 0;
+
   virtual uintptr_t GetAddressOf(HBasicBlock* block) = 0;
   void InitializeCodeGeneration(size_t number_of_spill_slots,
                                 size_t maximum_safepoint_spill_size,
@@ -546,6 +558,8 @@ class CodeGenerator : public DeletableArenaObject<kArenaAllocCodeGenerator> {
 
   void GenerateInvokeCustomCall(HInvokeCustom* invoke);
 
+  void CreateStringBuilderAppendLocations(HStringBuilderAppend* instruction, Location out);
+
   void CreateUnresolvedFieldLocationSummary(
       HInstruction* field_access,
       DataType::Type field_type,
@@ -673,7 +687,7 @@ class CodeGenerator : public DeletableArenaObject<kArenaAllocCodeGenerator> {
   }
 
   uint32_t GetFpuSpillSize() const {
-    return POPCOUNT(fpu_spill_mask_) * GetFloatingPointSpillSlotSize();
+    return POPCOUNT(fpu_spill_mask_) * GetCalleePreservedFPWidth();
   }
 
   uint32_t GetCoreSpillSize() const {
@@ -788,6 +802,8 @@ class CodeGenerator : public DeletableArenaObject<kArenaAllocCodeGenerator> {
   std::unique_ptr<CodeGenerationData> code_generation_data_;
 
   friend class OptimizingCFITest;
+  ART_FRIEND_TEST(CodegenTest, ARM64FrameSizeSIMD);
+  ART_FRIEND_TEST(CodegenTest, ARM64FrameSizeNoSIMD);
 
   DISALLOW_COPY_AND_ASSIGN(CodeGenerator);
 };
diff --git a/compiler/optimizing/code_generator_arm64.cc b/compiler/optimizing/code_generator_arm64.cc
index a436b98..b35da62 100644
--- a/compiler/optimizing/code_generator_arm64.cc
+++ b/compiler/optimizing/code_generator_arm64.cc
@@ -224,12 +224,13 @@ void SlowPathCodeARM64::SaveLiveRegisters(CodeGenerator* codegen, LocationSummar
     stack_offset += kXRegSizeInBytes;
   }
 
+  const size_t fp_reg_size = codegen->GetGraph()->HasSIMD() ? kQRegSizeInBytes : kDRegSizeInBytes;
   const uint32_t fp_spills = codegen->GetSlowPathSpills(locations, /* core_registers= */ false);
   for (uint32_t i : LowToHighBits(fp_spills)) {
     DCHECK_LT(stack_offset, codegen->GetFrameSize() - codegen->FrameEntrySpillSize());
     DCHECK_LT(i, kMaximumNumberOfExpectedRegisters);
     saved_fpu_stack_offsets_[i] = stack_offset;
-    stack_offset += kDRegSizeInBytes;
+    stack_offset += fp_reg_size;
   }
 
   SaveRestoreLiveRegistersHelper(codegen,
@@ -887,10 +888,6 @@ CodeGeneratorARM64::CodeGeneratorARM64(HGraph* graph,
       move_resolver_(graph->GetAllocator(), this),
       assembler_(graph->GetAllocator(),
                  compiler_options.GetInstructionSetFeatures()->AsArm64InstructionSetFeatures()),
-      uint32_literals_(std::less<uint32_t>(),
-                       graph->GetAllocator()->Adapter(kArenaAllocCodeGenerator)),
-      uint64_literals_(std::less<uint64_t>(),
-                       graph->GetAllocator()->Adapter(kArenaAllocCodeGenerator)),
       boot_image_method_patches_(graph->GetAllocator()->Adapter(kArenaAllocCodeGenerator)),
       method_bss_entry_patches_(graph->GetAllocator()->Adapter(kArenaAllocCodeGenerator)),
       boot_image_type_patches_(graph->GetAllocator()->Adapter(kArenaAllocCodeGenerator)),
@@ -898,7 +895,12 @@ CodeGeneratorARM64::CodeGeneratorARM64(HGraph* graph,
       boot_image_string_patches_(graph->GetAllocator()->Adapter(kArenaAllocCodeGenerator)),
       string_bss_entry_patches_(graph->GetAllocator()->Adapter(kArenaAllocCodeGenerator)),
       boot_image_intrinsic_patches_(graph->GetAllocator()->Adapter(kArenaAllocCodeGenerator)),
+      call_entrypoint_patches_(graph->GetAllocator()->Adapter(kArenaAllocCodeGenerator)),
       baker_read_barrier_patches_(graph->GetAllocator()->Adapter(kArenaAllocCodeGenerator)),
+      uint32_literals_(std::less<uint32_t>(),
+                       graph->GetAllocator()->Adapter(kArenaAllocCodeGenerator)),
+      uint64_literals_(std::less<uint64_t>(),
+                       graph->GetAllocator()->Adapter(kArenaAllocCodeGenerator)),
       jit_string_patches_(StringReferenceValueComparator(),
                           graph->GetAllocator()->Adapter(kArenaAllocCodeGenerator)),
       jit_class_patches_(TypeReferenceValueComparator(),
@@ -1089,27 +1091,42 @@ void CodeGeneratorARM64::GenerateFrameEntry() {
   }
 
   if (!HasEmptyFrame()) {
-    int frame_size = GetFrameSize();
     // Stack layout:
     //      sp[frame_size - 8]        : lr.
     //      ...                       : other preserved core registers.
     //      ...                       : other preserved fp registers.
     //      ...                       : reserved frame space.
     //      sp[0]                     : current method.
-
-    // Save the current method if we need it. Note that we do not
-    // do this in HCurrentMethod, as the instruction might have been removed
-    // in the SSA graph.
-    if (RequiresCurrentMethod()) {
+    int32_t frame_size = dchecked_integral_cast<int32_t>(GetFrameSize());
+    uint32_t core_spills_offset = frame_size - GetCoreSpillSize();
+    CPURegList preserved_core_registers = GetFramePreservedCoreRegisters();
+    DCHECK(!preserved_core_registers.IsEmpty());
+    uint32_t fp_spills_offset = frame_size - FrameEntrySpillSize();
+    CPURegList preserved_fp_registers = GetFramePreservedFPRegisters();
+
+    // Save the current method if we need it, or if using STP reduces code
+    // size. Note that we do not do this in HCurrentMethod, as the
+    // instruction might have been removed in the SSA graph.
+    CPURegister lowest_spill;
+    if (core_spills_offset == kXRegSizeInBytes) {
+      // If there is no gap between the method and the lowest core spill, use
+      // aligned STP pre-index to store both. Max difference is 512. We do
+      // that to reduce code size even if we do not have to save the method.
+      DCHECK_LE(frame_size, 512);  // 32 core registers are only 256 bytes.
+      lowest_spill = preserved_core_registers.PopLowestIndex();
+      __ Stp(kArtMethodRegister, lowest_spill, MemOperand(sp, -frame_size, PreIndex));
+    } else if (RequiresCurrentMethod()) {
       __ Str(kArtMethodRegister, MemOperand(sp, -frame_size, PreIndex));
     } else {
       __ Claim(frame_size);
     }
     GetAssembler()->cfi().AdjustCFAOffset(frame_size);
-    GetAssembler()->SpillRegisters(GetFramePreservedCoreRegisters(),
-        frame_size - GetCoreSpillSize());
-    GetAssembler()->SpillRegisters(GetFramePreservedFPRegisters(),
-        frame_size - FrameEntrySpillSize());
+    if (lowest_spill.IsValid()) {
+      GetAssembler()->cfi().RelOffset(DWARFReg(lowest_spill), core_spills_offset);
+      core_spills_offset += kXRegSizeInBytes;
+    }
+    GetAssembler()->SpillRegisters(preserved_core_registers, core_spills_offset);
+    GetAssembler()->SpillRegisters(preserved_fp_registers, fp_spills_offset);
 
     if (GetGraph()->HasShouldDeoptimizeFlag()) {
       // Initialize should_deoptimize flag to 0.
@@ -1124,12 +1141,30 @@ void CodeGeneratorARM64::GenerateFrameEntry() {
 void CodeGeneratorARM64::GenerateFrameExit() {
   GetAssembler()->cfi().RememberState();
   if (!HasEmptyFrame()) {
-    int frame_size = GetFrameSize();
-    GetAssembler()->UnspillRegisters(GetFramePreservedFPRegisters(),
-        frame_size - FrameEntrySpillSize());
-    GetAssembler()->UnspillRegisters(GetFramePreservedCoreRegisters(),
-        frame_size - GetCoreSpillSize());
-    __ Drop(frame_size);
+    int32_t frame_size = dchecked_integral_cast<int32_t>(GetFrameSize());
+    uint32_t core_spills_offset = frame_size - GetCoreSpillSize();
+    CPURegList preserved_core_registers = GetFramePreservedCoreRegisters();
+    DCHECK(!preserved_core_registers.IsEmpty());
+    uint32_t fp_spills_offset = frame_size - FrameEntrySpillSize();
+    CPURegList preserved_fp_registers = GetFramePreservedFPRegisters();
+
+    CPURegister lowest_spill;
+    if (core_spills_offset == kXRegSizeInBytes) {
+      // If there is no gap between the method and the lowest core spill, use
+      // aligned LDP pre-index to pop both. Max difference is 504. We do
+      // that to reduce code size even though the loaded method is unused.
+      DCHECK_LE(frame_size, 504);  // 32 core registers are only 256 bytes.
+      lowest_spill = preserved_core_registers.PopLowestIndex();
+      core_spills_offset += kXRegSizeInBytes;
+    }
+    GetAssembler()->UnspillRegisters(preserved_fp_registers, fp_spills_offset);
+    GetAssembler()->UnspillRegisters(preserved_core_registers, core_spills_offset);
+    if (lowest_spill.IsValid()) {
+      __ Ldp(xzr, lowest_spill, MemOperand(sp, frame_size, PostIndex));
+      GetAssembler()->cfi().Restore(DWARFReg(lowest_spill));
+    } else {
+      __ Drop(frame_size);
+    }
     GetAssembler()->cfi().AdjustCFAOffset(-frame_size);
   }
   __ Ret();
@@ -1245,16 +1280,18 @@ size_t CodeGeneratorARM64::RestoreCoreRegister(size_t stack_index, uint32_t reg_
   return kArm64WordSize;
 }
 
-size_t CodeGeneratorARM64::SaveFloatingPointRegister(size_t stack_index, uint32_t reg_id) {
-  FPRegister reg = FPRegister(reg_id, kDRegSize);
-  __ Str(reg, MemOperand(sp, stack_index));
-  return kArm64WordSize;
+size_t CodeGeneratorARM64::SaveFloatingPointRegister(size_t stack_index ATTRIBUTE_UNUSED,
+                                                     uint32_t reg_id ATTRIBUTE_UNUSED) {
+  LOG(FATAL) << "FP registers shouldn't be saved/restored individually, "
+             << "use SaveRestoreLiveRegistersHelper";
+  UNREACHABLE();
 }
 
-size_t CodeGeneratorARM64::RestoreFloatingPointRegister(size_t stack_index, uint32_t reg_id) {
-  FPRegister reg = FPRegister(reg_id, kDRegSize);
-  __ Ldr(reg, MemOperand(sp, stack_index));
-  return kArm64WordSize;
+size_t CodeGeneratorARM64::RestoreFloatingPointRegister(size_t stack_index ATTRIBUTE_UNUSED,
+                                                        uint32_t reg_id ATTRIBUTE_UNUSED) {
+  LOG(FATAL) << "FP registers shouldn't be saved/restored individually, "
+             << "use SaveRestoreLiveRegistersHelper";
+  UNREACHABLE();
 }
 
 void CodeGeneratorARM64::DumpCoreRegister(std::ostream& stream, int reg) const {
@@ -1687,14 +1724,25 @@ void CodeGeneratorARM64::InvokeRuntime(QuickEntrypointEnum entrypoint,
                                        SlowPathCode* slow_path) {
   ValidateInvokeRuntime(entrypoint, instruction, slow_path);
 
-  __ Ldr(lr, MemOperand(tr, GetThreadOffset<kArm64PointerSize>(entrypoint).Int32Value()));
-  {
+  ThreadOffset64 entrypoint_offset = GetThreadOffset<kArm64PointerSize>(entrypoint);
+  // Reduce code size for AOT by using shared trampolines for slow path runtime calls across the
+  // entire oat file. This adds an extra branch and we do not want to slow down the main path.
+  // For JIT, thunk sharing is per-method, so the gains would be smaller or even negative.
+  if (slow_path == nullptr || Runtime::Current()->UseJitCompilation()) {
+    __ Ldr(lr, MemOperand(tr, entrypoint_offset.Int32Value()));
     // Ensure the pc position is recorded immediately after the `blr` instruction.
     ExactAssemblyScope eas(GetVIXLAssembler(), kInstructionSize, CodeBufferCheckScope::kExactSize);
     __ blr(lr);
     if (EntrypointRequiresStackMap(entrypoint)) {
       RecordPcInfo(instruction, dex_pc, slow_path);
     }
+  } else {
+    // Ensure the pc position is recorded immediately after the `bl` instruction.
+    ExactAssemblyScope eas(GetVIXLAssembler(), kInstructionSize, CodeBufferCheckScope::kExactSize);
+    EmitEntrypointThunkCall(entrypoint_offset);
+    if (EntrypointRequiresStackMap(entrypoint)) {
+      RecordPcInfo(instruction, dex_pc, slow_path);
+    }
   }
 }
 
@@ -2693,16 +2741,59 @@ void LocationsBuilderARM64::VisitBoundsCheck(HBoundsCheck* instruction) {
   caller_saves.Add(Location::RegisterLocation(calling_convention.GetRegisterAt(0).GetCode()));
   caller_saves.Add(Location::RegisterLocation(calling_convention.GetRegisterAt(1).GetCode()));
   LocationSummary* locations = codegen_->CreateThrowingSlowPathLocations(instruction, caller_saves);
-  locations->SetInAt(0, Location::RequiresRegister());
-  locations->SetInAt(1, ARM64EncodableConstantOrRegister(instruction->InputAt(1), instruction));
+
+  // If both index and length are constant, we can check the bounds statically and
+  // generate code accordingly. We want to make sure we generate constant locations
+  // in that case, regardless of whether they are encodable in the comparison or not.
+  HInstruction* index = instruction->InputAt(0);
+  HInstruction* length = instruction->InputAt(1);
+  bool both_const = index->IsConstant() && length->IsConstant();
+  locations->SetInAt(0, both_const
+      ? Location::ConstantLocation(index->AsConstant())
+      : ARM64EncodableConstantOrRegister(index, instruction));
+  locations->SetInAt(1, both_const
+      ? Location::ConstantLocation(length->AsConstant())
+      : ARM64EncodableConstantOrRegister(length, instruction));
 }
 
 void InstructionCodeGeneratorARM64::VisitBoundsCheck(HBoundsCheck* instruction) {
+  LocationSummary* locations = instruction->GetLocations();
+  Location index_loc = locations->InAt(0);
+  Location length_loc = locations->InAt(1);
+
+  int cmp_first_input = 0;
+  int cmp_second_input = 1;
+  Condition cond = hs;
+
+  if (index_loc.IsConstant()) {
+    int64_t index = Int64FromLocation(index_loc);
+    if (length_loc.IsConstant()) {
+      int64_t length = Int64FromLocation(length_loc);
+      if (index < 0 || index >= length) {
+        BoundsCheckSlowPathARM64* slow_path =
+            new (codegen_->GetScopedAllocator()) BoundsCheckSlowPathARM64(instruction);
+        codegen_->AddSlowPath(slow_path);
+        __ B(slow_path->GetEntryLabel());
+      } else {
+        // BCE will remove the bounds check if we are guaranteed to pass.
+        // However, some optimization after BCE may have generated this, and we should not
+        // generate a bounds check if it is a valid range.
+      }
+      return;
+    }
+    // Only the index is constant: change the order of the operands and commute the condition
+    // so we can use an immediate constant for the index (only the second input to a cmp
+    // instruction can be an immediate).
+    cmp_first_input = 1;
+    cmp_second_input = 0;
+    cond = ls;
+  }
   BoundsCheckSlowPathARM64* slow_path =
       new (codegen_->GetScopedAllocator()) BoundsCheckSlowPathARM64(instruction);
+  __ Cmp(InputRegisterAt(instruction, cmp_first_input),
+         InputOperandAt(instruction, cmp_second_input));
   codegen_->AddSlowPath(slow_path);
-  __ Cmp(InputRegisterAt(instruction, 0), InputOperandAt(instruction, 1));
-  __ B(slow_path->GetEntryLabel(), hs);
+  __ B(slow_path->GetEntryLabel(), cond);
 }
 
 void LocationsBuilderARM64::VisitClinitCheck(HClinitCheck* check) {
@@ -4270,6 +4361,15 @@ vixl::aarch64::Label* CodeGeneratorARM64::NewStringBssEntryPatch(
   return NewPcRelativePatch(&dex_file, string_index.index_, adrp_label, &string_bss_entry_patches_);
 }
 
+void CodeGeneratorARM64::EmitEntrypointThunkCall(ThreadOffset64 entrypoint_offset) {
+  DCHECK(!__ AllowMacroInstructions());  // In ExactAssemblyScope.
+  DCHECK(!Runtime::Current()->UseJitCompilation());
+  call_entrypoint_patches_.emplace_back(/*dex_file*/ nullptr, entrypoint_offset.Uint32Value());
+  vixl::aarch64::Label* bl_label = &call_entrypoint_patches_.back().label;
+  __ bind(bl_label);
+  __ bl(static_cast<int64_t>(0));  // Placeholder, patched at link-time.
+}
+
 void CodeGeneratorARM64::EmitBakerReadBarrierCbnz(uint32_t custom_data) {
   DCHECK(!__ AllowMacroInstructions());  // In ExactAssemblyScope.
   if (Runtime::Current()->UseJitCompilation()) {
@@ -4426,6 +4526,7 @@ void CodeGeneratorARM64::EmitLinkerPatches(ArenaVector<linker::LinkerPatch>* lin
       boot_image_string_patches_.size() +
       string_bss_entry_patches_.size() +
       boot_image_intrinsic_patches_.size() +
+      call_entrypoint_patches_.size() +
       baker_read_barrier_patches_.size();
   linker_patches->reserve(size);
   if (GetCompilerOptions().IsBootImage()) {
@@ -4450,6 +4551,11 @@ void CodeGeneratorARM64::EmitLinkerPatches(ArenaVector<linker::LinkerPatch>* lin
       type_bss_entry_patches_, linker_patches);
   EmitPcRelativeLinkerPatches<linker::LinkerPatch::StringBssEntryPatch>(
       string_bss_entry_patches_, linker_patches);
+  for (const PatchInfo<vixl::aarch64::Label>& info : call_entrypoint_patches_) {
+    DCHECK(info.target_dex_file == nullptr);
+    linker_patches->push_back(linker::LinkerPatch::CallEntrypointPatch(
+        info.label.GetLocation(), info.offset_or_index));
+  }
   for (const BakerReadBarrierPatchInfo& info : baker_read_barrier_patches_) {
     linker_patches->push_back(linker::LinkerPatch::BakerReadBarrierBranchPatch(
         info.label.GetLocation(), info.custom_data));
@@ -4458,7 +4564,8 @@ void CodeGeneratorARM64::EmitLinkerPatches(ArenaVector<linker::LinkerPatch>* lin
 }
 
 bool CodeGeneratorARM64::NeedsThunkCode(const linker::LinkerPatch& patch) const {
-  return patch.GetType() == linker::LinkerPatch::Type::kBakerReadBarrierBranch ||
+  return patch.GetType() == linker::LinkerPatch::Type::kCallEntrypoint ||
+         patch.GetType() == linker::LinkerPatch::Type::kBakerReadBarrierBranch ||
          patch.GetType() == linker::LinkerPatch::Type::kCallRelative;
 }
 
@@ -4478,6 +4585,14 @@ void CodeGeneratorARM64::EmitThunkCode(const linker::LinkerPatch& patch,
       }
       break;
     }
+    case linker::LinkerPatch::Type::kCallEntrypoint: {
+      Offset offset(patch.EntrypointOffset());
+      assembler.JumpTo(ManagedRegister(arm64::TR), offset, ManagedRegister(arm64::IP0));
+      if (GetCompilerOptions().GenerateAnyDebugInfo()) {
+        *debug_name = "EntrypointCallThunk_" + std::to_string(offset.Uint32Value());
+      }
+      break;
+    }
     case linker::LinkerPatch::Type::kBakerReadBarrierBranch: {
       DCHECK_EQ(patch.GetBakerCustomValue2(), 0u);
       CompileBakerReadBarrierThunk(assembler, patch.GetBakerCustomValue1(), debug_name);
@@ -5411,6 +5526,15 @@ void InstructionCodeGeneratorARM64::VisitStaticFieldSet(HStaticFieldSet* instruc
   HandleFieldSet(instruction, instruction->GetFieldInfo(), instruction->GetValueCanBeNull());
 }
 
+void LocationsBuilderARM64::VisitStringBuilderAppend(HStringBuilderAppend* instruction) {
+  codegen_->CreateStringBuilderAppendLocations(instruction, LocationFrom(x0));
+}
+
+void InstructionCodeGeneratorARM64::VisitStringBuilderAppend(HStringBuilderAppend* instruction) {
+  __ Mov(w0, instruction->GetFormat()->GetValue());
+  codegen_->InvokeRuntime(kQuickStringBuilderAppend, instruction, instruction->GetDexPc());
+}
+
 void LocationsBuilderARM64::VisitUnresolvedInstanceFieldGet(
     HUnresolvedInstanceFieldGet* instruction) {
   FieldAccessCallingConventionARM64 calling_convention;
diff --git a/compiler/optimizing/code_generator_arm64.h b/compiler/optimizing/code_generator_arm64.h
index ada5742..66f74ef 100644
--- a/compiler/optimizing/code_generator_arm64.h
+++ b/compiler/optimizing/code_generator_arm64.h
@@ -435,10 +435,14 @@ class CodeGeneratorARM64 : public CodeGenerator {
     return kArm64WordSize;
   }
 
-  size_t GetFloatingPointSpillSlotSize() const override {
+  size_t GetSlowPathFPWidth() const override {
     return GetGraph()->HasSIMD()
-        ? 2 * kArm64WordSize   // 16 bytes == 2 arm64 words for each spill
-        : 1 * kArm64WordSize;  //  8 bytes == 1 arm64 words for each spill
+        ? vixl::aarch64::kQRegSizeInBytes
+        : vixl::aarch64::kDRegSizeInBytes;
+  }
+
+  size_t GetCalleePreservedFPWidth() const override {
+    return vixl::aarch64::kDRegSizeInBytes;
   }
 
   uintptr_t GetAddressOf(HBasicBlock* block) override {
@@ -629,6 +633,9 @@ class CodeGeneratorARM64 : public CodeGenerator {
                                                dex::StringIndex string_index,
                                                vixl::aarch64::Label* adrp_label = nullptr);
 
+  // Emit the BL instruction for entrypoint thunk call and record the associated patch for AOT.
+  void EmitEntrypointThunkCall(ThreadOffset64 entrypoint_offset);
+
   // Emit the CBNZ instruction for baker read barrier and record
   // the associated patch for AOT or slow path for JIT.
   void EmitBakerReadBarrierCbnz(uint32_t custom_data);
@@ -887,10 +894,6 @@ class CodeGeneratorARM64 : public CodeGenerator {
   ParallelMoveResolverARM64 move_resolver_;
   Arm64Assembler assembler_;
 
-  // Deduplication map for 32-bit literals, used for non-patchable boot image addresses.
-  Uint32ToLiteralMap uint32_literals_;
-  // Deduplication map for 64-bit literals, used for non-patchable method address or method code.
-  Uint64ToLiteralMap uint64_literals_;
   // PC-relative method patch info for kBootImageLinkTimePcRelative/BootImageRelRo.
   // Also used for type/string patches for kBootImageRelRo (same linker patch as for methods).
   ArenaDeque<PcRelativePatchInfo> boot_image_method_patches_;
@@ -906,9 +909,15 @@ class CodeGeneratorARM64 : public CodeGenerator {
   ArenaDeque<PcRelativePatchInfo> string_bss_entry_patches_;
   // PC-relative patch info for IntrinsicObjects.
   ArenaDeque<PcRelativePatchInfo> boot_image_intrinsic_patches_;
+  // Patch info for calls to entrypoint dispatch thunks. Used for slow paths.
+  ArenaDeque<PatchInfo<vixl::aarch64::Label>> call_entrypoint_patches_;
   // Baker read barrier patch info.
   ArenaDeque<BakerReadBarrierPatchInfo> baker_read_barrier_patches_;
 
+  // Deduplication map for 32-bit literals, used for JIT for boot image addresses.
+  Uint32ToLiteralMap uint32_literals_;
+  // Deduplication map for 64-bit literals, used for JIT for method address or method code.
+  Uint64ToLiteralMap uint64_literals_;
   // Patches for string literals in JIT compiled code.
   StringToLiteralMap jit_string_patches_;
   // Patches for class literals in JIT compiled code.
diff --git a/compiler/optimizing/code_generator_arm_vixl.cc b/compiler/optimizing/code_generator_arm_vixl.cc
index 507c453..5640b01 100644
--- a/compiler/optimizing/code_generator_arm_vixl.cc
+++ b/compiler/optimizing/code_generator_arm_vixl.cc
@@ -47,7 +47,6 @@ namespace vixl32 = vixl::aarch32;
 using namespace vixl32;  // NOLINT(build/namespaces)
 
 using helpers::DRegisterFrom;
-using helpers::DWARFReg;
 using helpers::HighRegisterFrom;
 using helpers::InputDRegisterAt;
 using helpers::InputOperandAt;
@@ -1856,8 +1855,6 @@ CodeGeneratorARMVIXL::CodeGeneratorARMVIXL(HGraph* graph,
       instruction_visitor_(graph, this),
       move_resolver_(graph->GetAllocator(), this),
       assembler_(graph->GetAllocator()),
-      uint32_literals_(std::less<uint32_t>(),
-                       graph->GetAllocator()->Adapter(kArenaAllocCodeGenerator)),
       boot_image_method_patches_(graph->GetAllocator()->Adapter(kArenaAllocCodeGenerator)),
       method_bss_entry_patches_(graph->GetAllocator()->Adapter(kArenaAllocCodeGenerator)),
       boot_image_type_patches_(graph->GetAllocator()->Adapter(kArenaAllocCodeGenerator)),
@@ -1865,7 +1862,10 @@ CodeGeneratorARMVIXL::CodeGeneratorARMVIXL(HGraph* graph,
       boot_image_string_patches_(graph->GetAllocator()->Adapter(kArenaAllocCodeGenerator)),
       string_bss_entry_patches_(graph->GetAllocator()->Adapter(kArenaAllocCodeGenerator)),
       boot_image_intrinsic_patches_(graph->GetAllocator()->Adapter(kArenaAllocCodeGenerator)),
+      call_entrypoint_patches_(graph->GetAllocator()->Adapter(kArenaAllocCodeGenerator)),
       baker_read_barrier_patches_(graph->GetAllocator()->Adapter(kArenaAllocCodeGenerator)),
+      uint32_literals_(std::less<uint32_t>(),
+                       graph->GetAllocator()->Adapter(kArenaAllocCodeGenerator)),
       jit_string_patches_(StringReferenceValueComparator(),
                           graph->GetAllocator()->Adapter(kArenaAllocCodeGenerator)),
       jit_class_patches_(TypeReferenceValueComparator(),
@@ -2060,10 +2060,10 @@ InstructionCodeGeneratorARMVIXL::InstructionCodeGeneratorARMVIXL(HGraph* graph,
 
 void CodeGeneratorARMVIXL::ComputeSpillMask() {
   core_spill_mask_ = allocated_registers_.GetCoreRegisters() & core_callee_save_mask_;
-  DCHECK_NE(core_spill_mask_, 0u) << "At least the return address register must be saved";
-  // There is no easy instruction to restore just the PC on thumb2. We spill and
-  // restore another arbitrary register.
-  core_spill_mask_ |= (1 << kCoreAlwaysSpillRegister.GetCode());
+  DCHECK_NE(core_spill_mask_ & (1u << kLrCode), 0u)
+      << "At least the return address register must be saved";
+  // 16-bit PUSH/POP (T1) can save/restore just the LR/PC.
+  DCHECK(GetVIXLAssembler()->IsUsingT32());
   fpu_spill_mask_ = allocated_registers_.GetFloatingPointRegisters() & fpu_callee_save_mask_;
   // We use vpush and vpop for saving and restoring floating point registers, which take
   // a SRegister and the number of registers to save/restore after that SRegister. We
@@ -2125,32 +2125,66 @@ void CodeGeneratorARMVIXL::GenerateFrameEntry() {
     RecordPcInfo(nullptr, 0);
   }
 
-  __ Push(RegisterList(core_spill_mask_));
-  GetAssembler()->cfi().AdjustCFAOffset(kArmWordSize * POPCOUNT(core_spill_mask_));
-  GetAssembler()->cfi().RelOffsetForMany(DWARFReg(kMethodRegister),
-                                         0,
-                                         core_spill_mask_,
-                                         kArmWordSize);
-  if (fpu_spill_mask_ != 0) {
-    uint32_t first = LeastSignificantBit(fpu_spill_mask_);
-
-    // Check that list is contiguous.
-    DCHECK_EQ(fpu_spill_mask_ >> CTZ(fpu_spill_mask_), ~0u >> (32 - POPCOUNT(fpu_spill_mask_)));
-
-    __ Vpush(SRegisterList(vixl32::SRegister(first), POPCOUNT(fpu_spill_mask_)));
-    GetAssembler()->cfi().AdjustCFAOffset(kArmWordSize * POPCOUNT(fpu_spill_mask_));
-    GetAssembler()->cfi().RelOffsetForMany(DWARFReg(s0), 0, fpu_spill_mask_, kArmWordSize);
-  }
-
-  int adjust = GetFrameSize() - FrameEntrySpillSize();
-  __ Sub(sp, sp, adjust);
-  GetAssembler()->cfi().AdjustCFAOffset(adjust);
-
-  // Save the current method if we need it. Note that we do not
-  // do this in HCurrentMethod, as the instruction might have been removed
-  // in the SSA graph.
-  if (RequiresCurrentMethod()) {
-    GetAssembler()->StoreToOffset(kStoreWord, kMethodRegister, sp, 0);
+  uint32_t frame_size = GetFrameSize();
+  uint32_t core_spills_offset = frame_size - GetCoreSpillSize();
+  uint32_t fp_spills_offset = frame_size - FrameEntrySpillSize();
+  if ((fpu_spill_mask_ == 0u || IsPowerOfTwo(fpu_spill_mask_)) &&
+      core_spills_offset <= 3u * kArmWordSize) {
+    // Do a single PUSH for core registers including the method and up to two
+    // filler registers. Then store the single FP spill if any.
+    // (The worst case is when the method is not required and we actually
+    // store 3 extra registers but they are stored in the same properly
+    // aligned 16-byte chunk where we're already writing anyway.)
+    DCHECK_EQ(kMethodRegister.GetCode(), 0u);
+    uint32_t extra_regs = MaxInt<uint32_t>(core_spills_offset / kArmWordSize);
+    DCHECK_LT(MostSignificantBit(extra_regs), LeastSignificantBit(core_spill_mask_));
+    __ Push(RegisterList(core_spill_mask_ | extra_regs));
+    GetAssembler()->cfi().AdjustCFAOffset(frame_size);
+    GetAssembler()->cfi().RelOffsetForMany(DWARFReg(kMethodRegister),
+                                           core_spills_offset,
+                                           core_spill_mask_,
+                                           kArmWordSize);
+    if (fpu_spill_mask_ != 0u) {
+      DCHECK(IsPowerOfTwo(fpu_spill_mask_));
+      vixl::aarch32::SRegister sreg(LeastSignificantBit(fpu_spill_mask_));
+      GetAssembler()->StoreSToOffset(sreg, sp, fp_spills_offset);
+      GetAssembler()->cfi().RelOffset(DWARFReg(sreg), /*offset=*/ fp_spills_offset);
+    }
+  } else {
+    __ Push(RegisterList(core_spill_mask_));
+    GetAssembler()->cfi().AdjustCFAOffset(kArmWordSize * POPCOUNT(core_spill_mask_));
+    GetAssembler()->cfi().RelOffsetForMany(DWARFReg(kMethodRegister),
+                                           /*offset=*/ 0,
+                                           core_spill_mask_,
+                                           kArmWordSize);
+    if (fpu_spill_mask_ != 0) {
+      uint32_t first = LeastSignificantBit(fpu_spill_mask_);
+
+      // Check that list is contiguous.
+      DCHECK_EQ(fpu_spill_mask_ >> CTZ(fpu_spill_mask_), ~0u >> (32 - POPCOUNT(fpu_spill_mask_)));
+
+      __ Vpush(SRegisterList(vixl32::SRegister(first), POPCOUNT(fpu_spill_mask_)));
+      GetAssembler()->cfi().AdjustCFAOffset(kArmWordSize * POPCOUNT(fpu_spill_mask_));
+      GetAssembler()->cfi().RelOffsetForMany(DWARFReg(s0),
+                                             /*offset=*/ 0,
+                                             fpu_spill_mask_,
+                                             kArmWordSize);
+    }
+
+    // Adjust SP and save the current method if we need it. Note that we do
+    // not save the method in HCurrentMethod, as the instruction might have
+    // been removed in the SSA graph.
+    if (RequiresCurrentMethod() && fp_spills_offset <= 3 * kArmWordSize) {
+      DCHECK_EQ(kMethodRegister.GetCode(), 0u);
+      __ Push(RegisterList(MaxInt<uint32_t>(fp_spills_offset / kArmWordSize)));
+      GetAssembler()->cfi().AdjustCFAOffset(fp_spills_offset);
+    } else {
+      __ Sub(sp, sp, dchecked_integral_cast<int32_t>(fp_spills_offset));
+      GetAssembler()->cfi().AdjustCFAOffset(fp_spills_offset);
+      if (RequiresCurrentMethod()) {
+        GetAssembler()->StoreToOffset(kStoreWord, kMethodRegister, sp, 0);
+      }
+    }
   }
 
   if (GetGraph()->HasShouldDeoptimizeFlag()) {
@@ -2169,27 +2203,55 @@ void CodeGeneratorARMVIXL::GenerateFrameExit() {
     __ Bx(lr);
     return;
   }
-  GetAssembler()->cfi().RememberState();
-  int adjust = GetFrameSize() - FrameEntrySpillSize();
-  __ Add(sp, sp, adjust);
-  GetAssembler()->cfi().AdjustCFAOffset(-adjust);
-  if (fpu_spill_mask_ != 0) {
-    uint32_t first = LeastSignificantBit(fpu_spill_mask_);
 
-    // Check that list is contiguous.
-    DCHECK_EQ(fpu_spill_mask_ >> CTZ(fpu_spill_mask_), ~0u >> (32 - POPCOUNT(fpu_spill_mask_)));
-
-    __ Vpop(SRegisterList(vixl32::SRegister(first), POPCOUNT(fpu_spill_mask_)));
-    GetAssembler()->cfi().AdjustCFAOffset(
-        -static_cast<int>(kArmWordSize) * POPCOUNT(fpu_spill_mask_));
-    GetAssembler()->cfi().RestoreMany(DWARFReg(vixl32::SRegister(0)), fpu_spill_mask_);
-  }
   // Pop LR into PC to return.
   DCHECK_NE(core_spill_mask_ & (1 << kLrCode), 0U);
   uint32_t pop_mask = (core_spill_mask_ & (~(1 << kLrCode))) | 1 << kPcCode;
-  __ Pop(RegisterList(pop_mask));
-  GetAssembler()->cfi().RestoreState();
-  GetAssembler()->cfi().DefCFAOffset(GetFrameSize());
+
+  uint32_t frame_size = GetFrameSize();
+  uint32_t core_spills_offset = frame_size - GetCoreSpillSize();
+  uint32_t fp_spills_offset = frame_size - FrameEntrySpillSize();
+  if ((fpu_spill_mask_ == 0u || IsPowerOfTwo(fpu_spill_mask_)) &&
+      // r4 is blocked by TestCodeGeneratorARMVIXL used by some tests.
+      core_spills_offset <= (blocked_core_registers_[r4.GetCode()] ? 2u : 3u) * kArmWordSize) {
+    // Load the FP spill if any and then do a single POP including the method
+    // and up to two filler registers. If we have no FP spills, this also has
+    // the advantage that we do not need to emit CFI directives.
+    if (fpu_spill_mask_ != 0u) {
+      DCHECK(IsPowerOfTwo(fpu_spill_mask_));
+      vixl::aarch32::SRegister sreg(LeastSignificantBit(fpu_spill_mask_));
+      GetAssembler()->cfi().RememberState();
+      GetAssembler()->LoadSFromOffset(sreg, sp, fp_spills_offset);
+      GetAssembler()->cfi().Restore(DWARFReg(sreg));
+    }
+    // Clobber registers r2-r4 as they are caller-save in ART managed ABI and
+    // never hold the return value.
+    uint32_t extra_regs = MaxInt<uint32_t>(core_spills_offset / kArmWordSize) << r2.GetCode();
+    DCHECK_EQ(extra_regs & kCoreCalleeSaves.GetList(), 0u);
+    DCHECK_LT(MostSignificantBit(extra_regs), LeastSignificantBit(pop_mask));
+    __ Pop(RegisterList(pop_mask | extra_regs));
+    if (fpu_spill_mask_ != 0u) {
+      GetAssembler()->cfi().RestoreState();
+    }
+  } else {
+    GetAssembler()->cfi().RememberState();
+    __ Add(sp, sp, fp_spills_offset);
+    GetAssembler()->cfi().AdjustCFAOffset(-dchecked_integral_cast<int32_t>(fp_spills_offset));
+    if (fpu_spill_mask_ != 0) {
+      uint32_t first = LeastSignificantBit(fpu_spill_mask_);
+
+      // Check that list is contiguous.
+      DCHECK_EQ(fpu_spill_mask_ >> CTZ(fpu_spill_mask_), ~0u >> (32 - POPCOUNT(fpu_spill_mask_)));
+
+      __ Vpop(SRegisterList(vixl32::SRegister(first), POPCOUNT(fpu_spill_mask_)));
+      GetAssembler()->cfi().AdjustCFAOffset(
+          -static_cast<int>(kArmWordSize) * POPCOUNT(fpu_spill_mask_));
+      GetAssembler()->cfi().RestoreMany(DWARFReg(vixl32::SRegister(0)), fpu_spill_mask_);
+    }
+    __ Pop(RegisterList(pop_mask));
+    GetAssembler()->cfi().RestoreState();
+    GetAssembler()->cfi().DefCFAOffset(GetFrameSize());
+  }
 }
 
 void CodeGeneratorARMVIXL::Bind(HBasicBlock* block) {
@@ -2383,15 +2445,31 @@ void CodeGeneratorARMVIXL::InvokeRuntime(QuickEntrypointEnum entrypoint,
                                          uint32_t dex_pc,
                                          SlowPathCode* slow_path) {
   ValidateInvokeRuntime(entrypoint, instruction, slow_path);
-  __ Ldr(lr, MemOperand(tr, GetThreadOffset<kArmPointerSize>(entrypoint).Int32Value()));
-  // Ensure the pc position is recorded immediately after the `blx` instruction.
-  // blx in T32 has only 16bit encoding that's why a stricter check for the scope is used.
-  ExactAssemblyScope aas(GetVIXLAssembler(),
-                         vixl32::k16BitT32InstructionSizeInBytes,
-                         CodeBufferCheckScope::kExactSize);
-  __ blx(lr);
-  if (EntrypointRequiresStackMap(entrypoint)) {
-    RecordPcInfo(instruction, dex_pc, slow_path);
+
+  ThreadOffset32 entrypoint_offset = GetThreadOffset<kArmPointerSize>(entrypoint);
+  // Reduce code size for AOT by using shared trampolines for slow path runtime calls across the
+  // entire oat file. This adds an extra branch and we do not want to slow down the main path.
+  // For JIT, thunk sharing is per-method, so the gains would be smaller or even negative.
+  if (slow_path == nullptr || Runtime::Current()->UseJitCompilation()) {
+    __ Ldr(lr, MemOperand(tr, entrypoint_offset.Int32Value()));
+    // Ensure the pc position is recorded immediately after the `blx` instruction.
+    // blx in T32 has only 16bit encoding that's why a stricter check for the scope is used.
+    ExactAssemblyScope aas(GetVIXLAssembler(),
+                           vixl32::k16BitT32InstructionSizeInBytes,
+                           CodeBufferCheckScope::kExactSize);
+    __ blx(lr);
+    if (EntrypointRequiresStackMap(entrypoint)) {
+      RecordPcInfo(instruction, dex_pc, slow_path);
+    }
+  } else {
+    // Ensure the pc position is recorded immediately after the `bl` instruction.
+    ExactAssemblyScope aas(GetVIXLAssembler(),
+                           vixl32::k32BitT32InstructionSizeInBytes,
+                           CodeBufferCheckScope::kExactSize);
+    EmitEntrypointThunkCall(entrypoint_offset);
+    if (EntrypointRequiresStackMap(entrypoint)) {
+      RecordPcInfo(instruction, dex_pc, slow_path);
+    }
   }
 }
 
@@ -5722,6 +5800,15 @@ void InstructionCodeGeneratorARMVIXL::VisitStaticFieldSet(HStaticFieldSet* instr
   HandleFieldSet(instruction, instruction->GetFieldInfo(), instruction->GetValueCanBeNull());
 }
 
+void LocationsBuilderARMVIXL::VisitStringBuilderAppend(HStringBuilderAppend* instruction) {
+  codegen_->CreateStringBuilderAppendLocations(instruction, LocationFrom(r0));
+}
+
+void InstructionCodeGeneratorARMVIXL::VisitStringBuilderAppend(HStringBuilderAppend* instruction) {
+  __ Mov(r0, instruction->GetFormat()->GetValue());
+  codegen_->InvokeRuntime(kQuickStringBuilderAppend, instruction, instruction->GetDexPc());
+}
+
 void LocationsBuilderARMVIXL::VisitUnresolvedInstanceFieldGet(
     HUnresolvedInstanceFieldGet* instruction) {
   FieldAccessCallingConventionARMVIXL calling_convention;
@@ -8867,6 +8954,17 @@ CodeGeneratorARMVIXL::PcRelativePatchInfo* CodeGeneratorARMVIXL::NewPcRelativePa
   return &patches->back();
 }
 
+void CodeGeneratorARMVIXL::EmitEntrypointThunkCall(ThreadOffset32 entrypoint_offset) {
+  DCHECK(!__ AllowMacroInstructions());  // In ExactAssemblyScope.
+  DCHECK(!Runtime::Current()->UseJitCompilation());
+  call_entrypoint_patches_.emplace_back(/*dex_file*/ nullptr, entrypoint_offset.Uint32Value());
+  vixl::aarch32::Label* bl_label = &call_entrypoint_patches_.back().label;
+  __ bind(bl_label);
+  vixl32::Label placeholder_label;
+  __ bl(&placeholder_label);  // Placeholder, patched at link-time.
+  __ bind(&placeholder_label);
+}
+
 void CodeGeneratorARMVIXL::EmitBakerReadBarrierBne(uint32_t custom_data) {
   DCHECK(!__ AllowMacroInstructions());  // In ExactAssemblyScope.
   if (Runtime::Current()->UseJitCompilation()) {
@@ -8989,6 +9087,7 @@ void CodeGeneratorARMVIXL::EmitLinkerPatches(ArenaVector<linker::LinkerPatch>* l
       /* MOVW+MOVT for each entry */ 2u * boot_image_string_patches_.size() +
       /* MOVW+MOVT for each entry */ 2u * string_bss_entry_patches_.size() +
       /* MOVW+MOVT for each entry */ 2u * boot_image_intrinsic_patches_.size() +
+      call_entrypoint_patches_.size() +
       baker_read_barrier_patches_.size();
   linker_patches->reserve(size);
   if (GetCompilerOptions().IsBootImage()) {
@@ -9013,6 +9112,11 @@ void CodeGeneratorARMVIXL::EmitLinkerPatches(ArenaVector<linker::LinkerPatch>* l
       type_bss_entry_patches_, linker_patches);
   EmitPcRelativeLinkerPatches<linker::LinkerPatch::StringBssEntryPatch>(
       string_bss_entry_patches_, linker_patches);
+  for (const PatchInfo<vixl32::Label>& info : call_entrypoint_patches_) {
+    DCHECK(info.target_dex_file == nullptr);
+    linker_patches->push_back(linker::LinkerPatch::CallEntrypointPatch(
+        info.label.GetLocation(), info.offset_or_index));
+  }
   for (const BakerReadBarrierPatchInfo& info : baker_read_barrier_patches_) {
     linker_patches->push_back(linker::LinkerPatch::BakerReadBarrierBranchPatch(
         info.label.GetLocation(), info.custom_data));
@@ -9021,7 +9125,8 @@ void CodeGeneratorARMVIXL::EmitLinkerPatches(ArenaVector<linker::LinkerPatch>* l
 }
 
 bool CodeGeneratorARMVIXL::NeedsThunkCode(const linker::LinkerPatch& patch) const {
-  return patch.GetType() == linker::LinkerPatch::Type::kBakerReadBarrierBranch ||
+  return patch.GetType() == linker::LinkerPatch::Type::kCallEntrypoint ||
+         patch.GetType() == linker::LinkerPatch::Type::kBakerReadBarrierBranch ||
          patch.GetType() == linker::LinkerPatch::Type::kCallRelative;
 }
 
@@ -9030,23 +9135,30 @@ void CodeGeneratorARMVIXL::EmitThunkCode(const linker::LinkerPatch& patch,
                                          /*out*/ std::string* debug_name) {
   arm::ArmVIXLAssembler assembler(GetGraph()->GetAllocator());
   switch (patch.GetType()) {
-    case linker::LinkerPatch::Type::kCallRelative:
+    case linker::LinkerPatch::Type::kCallRelative: {
       // The thunk just uses the entry point in the ArtMethod. This works even for calls
       // to the generic JNI and interpreter trampolines.
-      assembler.LoadFromOffset(
-          arm::kLoadWord,
-          vixl32::pc,
-          vixl32::r0,
-          ArtMethod::EntryPointFromQuickCompiledCodeOffset(kArmPointerSize).Int32Value());
+      MemberOffset offset = ArtMethod::EntryPointFromQuickCompiledCodeOffset(kArmPointerSize);
+      assembler.LoadFromOffset(arm::kLoadWord, vixl32::pc, vixl32::r0, offset.Int32Value());
       assembler.GetVIXLAssembler()->Bkpt(0);
       if (GetCompilerOptions().GenerateAnyDebugInfo()) {
         *debug_name = "MethodCallThunk";
       }
       break;
-    case linker::LinkerPatch::Type::kBakerReadBarrierBranch:
+    }
+    case linker::LinkerPatch::Type::kCallEntrypoint: {
+      assembler.LoadFromOffset(arm::kLoadWord, vixl32::pc, tr, patch.EntrypointOffset());
+      assembler.GetVIXLAssembler()->Bkpt(0);
+      if (GetCompilerOptions().GenerateAnyDebugInfo()) {
+        *debug_name = "EntrypointCallThunk_" + std::to_string(patch.EntrypointOffset());
+      }
+      break;
+    }
+    case linker::LinkerPatch::Type::kBakerReadBarrierBranch: {
       DCHECK_EQ(patch.GetBakerCustomValue2(), 0u);
       CompileBakerReadBarrierThunk(assembler, patch.GetBakerCustomValue1(), debug_name);
       break;
+    }
     default:
       LOG(FATAL) << "Unexpected patch type " << patch.GetType();
       UNREACHABLE();
diff --git a/compiler/optimizing/code_generator_arm_vixl.h b/compiler/optimizing/code_generator_arm_vixl.h
index 5edca87..f2a8133 100644
--- a/compiler/optimizing/code_generator_arm_vixl.h
+++ b/compiler/optimizing/code_generator_arm_vixl.h
@@ -76,8 +76,6 @@ static const size_t kParameterFpuRegistersLengthVIXL = arraysize(kParameterFpuRe
 
 static const vixl::aarch32::Register kMethodRegister = vixl::aarch32::r0;
 
-static const vixl::aarch32::Register kCoreAlwaysSpillRegister = vixl::aarch32::r5;
-
 // Callee saves core registers r5, r6, r7, r8 (except when emitting Baker
 // read barriers, where it is used as Marking Register), r10, r11, and lr.
 static const vixl::aarch32::RegisterList kCoreCalleeSaves = vixl::aarch32::RegisterList::Union(
@@ -448,7 +446,9 @@ class CodeGeneratorARMVIXL : public CodeGenerator {
     return static_cast<size_t>(kArmPointerSize);
   }
 
-  size_t GetFloatingPointSpillSlotSize() const override { return vixl::aarch32::kRegSizeInBytes; }
+  size_t GetCalleePreservedFPWidth() const override {
+    return vixl::aarch32::kSRegSizeInBytes;
+  }
 
   HGraphVisitor* GetLocationBuilder() override { return &location_builder_; }
 
@@ -589,6 +589,9 @@ class CodeGeneratorARMVIXL : public CodeGenerator {
   PcRelativePatchInfo* NewStringBssEntryPatch(const DexFile& dex_file,
                                               dex::StringIndex string_index);
 
+  // Emit the BL instruction for entrypoint thunk call and record the associated patch for AOT.
+  void EmitEntrypointThunkCall(ThreadOffset32 entrypoint_offset);
+
   // Emit the BNE instruction for baker read barrier and record
   // the associated patch for AOT or slow path for JIT.
   void EmitBakerReadBarrierBne(uint32_t custom_data);
@@ -869,8 +872,6 @@ class CodeGeneratorARMVIXL : public CodeGenerator {
 
   ArmVIXLAssembler assembler_;
 
-  // Deduplication map for 32-bit literals, used for non-patchable boot image addresses.
-  Uint32ToLiteralMap uint32_literals_;
   // PC-relative method patch info for kBootImageLinkTimePcRelative/kBootImageRelRo.
   // Also used for type/string patches for kBootImageRelRo (same linker patch as for methods).
   ArenaDeque<PcRelativePatchInfo> boot_image_method_patches_;
@@ -886,9 +887,13 @@ class CodeGeneratorARMVIXL : public CodeGenerator {
   ArenaDeque<PcRelativePatchInfo> string_bss_entry_patches_;
   // PC-relative patch info for IntrinsicObjects.
   ArenaDeque<PcRelativePatchInfo> boot_image_intrinsic_patches_;
+  // Patch info for calls to entrypoint dispatch thunks. Used for slow paths.
+  ArenaDeque<PatchInfo<vixl::aarch32::Label>> call_entrypoint_patches_;
   // Baker read barrier patch info.
   ArenaDeque<BakerReadBarrierPatchInfo> baker_read_barrier_patches_;
 
+  // Deduplication map for 32-bit literals, used for JIT for boot image addresses.
+  Uint32ToLiteralMap uint32_literals_;
   // Patches for string literals in JIT compiled code.
   StringToLiteralMap jit_string_patches_;
   // Patches for class literals in JIT compiled code.
diff --git a/compiler/optimizing/code_generator_mips.cc b/compiler/optimizing/code_generator_mips.cc
index 72334af..d35e47e 100644
--- a/compiler/optimizing/code_generator_mips.cc
+++ b/compiler/optimizing/code_generator_mips.cc
@@ -1953,7 +1953,7 @@ size_t CodeGeneratorMIPS::SaveFloatingPointRegister(size_t stack_index, uint32_t
   } else {
     __ StoreDToOffset(FRegister(reg_id), SP, stack_index);
   }
-  return GetFloatingPointSpillSlotSize();
+  return GetSlowPathFPWidth();
 }
 
 size_t CodeGeneratorMIPS::RestoreFloatingPointRegister(size_t stack_index, uint32_t reg_id) {
@@ -1962,7 +1962,7 @@ size_t CodeGeneratorMIPS::RestoreFloatingPointRegister(size_t stack_index, uint3
   } else {
     __ LoadDFromOffset(FRegister(reg_id), SP, stack_index);
   }
-  return GetFloatingPointSpillSlotSize();
+  return GetSlowPathFPWidth();
 }
 
 void CodeGeneratorMIPS::DumpCoreRegister(std::ostream& stream, int reg) const {
@@ -9529,6 +9529,15 @@ void InstructionCodeGeneratorMIPS::VisitStaticFieldSet(HStaticFieldSet* instruct
                  instruction->GetValueCanBeNull());
 }
 
+void LocationsBuilderMIPS::VisitStringBuilderAppend(HStringBuilderAppend* instruction) {
+  codegen_->CreateStringBuilderAppendLocations(instruction, Location::RegisterLocation(V0));
+}
+
+void InstructionCodeGeneratorMIPS::VisitStringBuilderAppend(HStringBuilderAppend* instruction) {
+  __ LoadConst32(A0, instruction->GetFormat()->GetValue());
+  codegen_->InvokeRuntime(kQuickStringBuilderAppend, instruction, instruction->GetDexPc());
+}
+
 void LocationsBuilderMIPS::VisitUnresolvedInstanceFieldGet(
     HUnresolvedInstanceFieldGet* instruction) {
   FieldAccessCallingConventionMIPS calling_convention;
diff --git a/compiler/optimizing/code_generator_mips.h b/compiler/optimizing/code_generator_mips.h
index 5080731..b374d34 100644
--- a/compiler/optimizing/code_generator_mips.h
+++ b/compiler/optimizing/code_generator_mips.h
@@ -385,12 +385,16 @@ class CodeGeneratorMIPS : public CodeGenerator {
 
   size_t GetWordSize() const override { return kMipsWordSize; }
 
-  size_t GetFloatingPointSpillSlotSize() const override {
+  size_t GetSlowPathFPWidth() const override {
     return GetGraph()->HasSIMD()
         ? 2 * kMipsDoublewordSize   // 16 bytes for each spill.
         : 1 * kMipsDoublewordSize;  //  8 bytes for each spill.
   }
 
+  size_t GetCalleePreservedFPWidth() const override {
+    return 1 * kMipsDoublewordSize;
+  }
+
   uintptr_t GetAddressOf(HBasicBlock* block) override {
     return assembler_.GetLabelLocation(GetLabelOf(block));
   }
diff --git a/compiler/optimizing/code_generator_mips64.cc b/compiler/optimizing/code_generator_mips64.cc
index 0d3cb3b..2ab40c8 100644
--- a/compiler/optimizing/code_generator_mips64.cc
+++ b/compiler/optimizing/code_generator_mips64.cc
@@ -1814,7 +1814,7 @@ size_t CodeGeneratorMIPS64::SaveFloatingPointRegister(size_t stack_index, uint32
                       FpuRegister(reg_id),
                       SP,
                       stack_index);
-  return GetFloatingPointSpillSlotSize();
+  return GetSlowPathFPWidth();
 }
 
 size_t CodeGeneratorMIPS64::RestoreFloatingPointRegister(size_t stack_index, uint32_t reg_id) {
@@ -1822,7 +1822,7 @@ size_t CodeGeneratorMIPS64::RestoreFloatingPointRegister(size_t stack_index, uin
                        FpuRegister(reg_id),
                        SP,
                        stack_index);
-  return GetFloatingPointSpillSlotSize();
+  return GetSlowPathFPWidth();
 }
 
 void CodeGeneratorMIPS64::DumpCoreRegister(std::ostream& stream, int reg) const {
@@ -7148,6 +7148,15 @@ void InstructionCodeGeneratorMIPS64::VisitStaticFieldSet(HStaticFieldSet* instru
   HandleFieldSet(instruction, instruction->GetFieldInfo(), instruction->GetValueCanBeNull());
 }
 
+void LocationsBuilderMIPS64::VisitStringBuilderAppend(HStringBuilderAppend* instruction) {
+  codegen_->CreateStringBuilderAppendLocations(instruction, Location::RegisterLocation(V0));
+}
+
+void InstructionCodeGeneratorMIPS64::VisitStringBuilderAppend(HStringBuilderAppend* instruction) {
+  __ LoadConst32(A0, instruction->GetFormat()->GetValue());
+  codegen_->InvokeRuntime(kQuickStringBuilderAppend, instruction, instruction->GetDexPc());
+}
+
 void LocationsBuilderMIPS64::VisitUnresolvedInstanceFieldGet(
     HUnresolvedInstanceFieldGet* instruction) {
   FieldAccessCallingConventionMIPS64 calling_convention;
diff --git a/compiler/optimizing/code_generator_mips64.h b/compiler/optimizing/code_generator_mips64.h
index 52f3a62..e484626 100644
--- a/compiler/optimizing/code_generator_mips64.h
+++ b/compiler/optimizing/code_generator_mips64.h
@@ -363,12 +363,16 @@ class CodeGeneratorMIPS64 : public CodeGenerator {
 
   size_t GetWordSize() const override { return kMips64DoublewordSize; }
 
-  size_t GetFloatingPointSpillSlotSize() const override {
+  size_t GetSlowPathFPWidth() const override {
     return GetGraph()->HasSIMD()
         ? 2 * kMips64DoublewordSize   // 16 bytes for each spill.
         : 1 * kMips64DoublewordSize;  //  8 bytes for each spill.
   }
 
+  size_t GetCalleePreservedFPWidth() const override {
+    return 1* kMips64DoublewordSize;
+  }
+
   uintptr_t GetAddressOf(HBasicBlock* block) override {
     return assembler_.GetLabelLocation(GetLabelOf(block));
   }
diff --git a/compiler/optimizing/code_generator_x86.cc b/compiler/optimizing/code_generator_x86.cc
index 95118b0..04b840b 100644
--- a/compiler/optimizing/code_generator_x86.cc
+++ b/compiler/optimizing/code_generator_x86.cc
@@ -987,7 +987,7 @@ size_t CodeGeneratorX86::SaveFloatingPointRegister(size_t stack_index, uint32_t
   } else {
     __ movsd(Address(ESP, stack_index), XmmRegister(reg_id));
   }
-  return GetFloatingPointSpillSlotSize();
+  return GetSlowPathFPWidth();
 }
 
 size_t CodeGeneratorX86::RestoreFloatingPointRegister(size_t stack_index, uint32_t reg_id) {
@@ -996,7 +996,7 @@ size_t CodeGeneratorX86::RestoreFloatingPointRegister(size_t stack_index, uint32
   } else {
     __ movsd(XmmRegister(reg_id), Address(ESP, stack_index));
   }
-  return GetFloatingPointSpillSlotSize();
+  return GetSlowPathFPWidth();
 }
 
 void CodeGeneratorX86::InvokeRuntime(QuickEntrypointEnum entrypoint,
@@ -5510,6 +5510,15 @@ void InstructionCodeGeneratorX86::VisitInstanceFieldGet(HInstanceFieldGet* instr
   HandleFieldGet(instruction, instruction->GetFieldInfo());
 }
 
+void LocationsBuilderX86::VisitStringBuilderAppend(HStringBuilderAppend* instruction) {
+  codegen_->CreateStringBuilderAppendLocations(instruction, Location::RegisterLocation(EAX));
+}
+
+void InstructionCodeGeneratorX86::VisitStringBuilderAppend(HStringBuilderAppend* instruction) {
+  __ movl(EAX, Immediate(instruction->GetFormat()->GetValue()));
+  codegen_->InvokeRuntime(kQuickStringBuilderAppend, instruction, instruction->GetDexPc());
+}
+
 void LocationsBuilderX86::VisitUnresolvedInstanceFieldGet(
     HUnresolvedInstanceFieldGet* instruction) {
   FieldAccessCallingConventionX86 calling_convention;
diff --git a/compiler/optimizing/code_generator_x86.h b/compiler/optimizing/code_generator_x86.h
index deeef88..0eeae7d 100644
--- a/compiler/optimizing/code_generator_x86.h
+++ b/compiler/optimizing/code_generator_x86.h
@@ -353,12 +353,16 @@ class CodeGeneratorX86 : public CodeGenerator {
     return kX86WordSize;
   }
 
-  size_t GetFloatingPointSpillSlotSize() const override {
+  size_t GetSlowPathFPWidth() const override {
     return GetGraph()->HasSIMD()
         ? 4 * kX86WordSize   // 16 bytes == 4 words for each spill
         : 2 * kX86WordSize;  //  8 bytes == 2 words for each spill
   }
 
+  size_t GetCalleePreservedFPWidth() const override {
+    return 2 * kX86WordSize;
+  }
+
   HGraphVisitor* GetLocationBuilder() override {
     return &location_builder_;
   }
diff --git a/compiler/optimizing/code_generator_x86_64.cc b/compiler/optimizing/code_generator_x86_64.cc
index 7c293b8..9551334 100644
--- a/compiler/optimizing/code_generator_x86_64.cc
+++ b/compiler/optimizing/code_generator_x86_64.cc
@@ -1245,7 +1245,7 @@ size_t CodeGeneratorX86_64::SaveFloatingPointRegister(size_t stack_index, uint32
   } else {
     __ movsd(Address(CpuRegister(RSP), stack_index), XmmRegister(reg_id));
   }
-  return GetFloatingPointSpillSlotSize();
+  return GetSlowPathFPWidth();
 }
 
 size_t CodeGeneratorX86_64::RestoreFloatingPointRegister(size_t stack_index, uint32_t reg_id) {
@@ -1254,7 +1254,7 @@ size_t CodeGeneratorX86_64::RestoreFloatingPointRegister(size_t stack_index, uin
   } else {
     __ movsd(XmmRegister(reg_id), Address(CpuRegister(RSP), stack_index));
   }
-  return GetFloatingPointSpillSlotSize();
+  return GetSlowPathFPWidth();
 }
 
 void CodeGeneratorX86_64::InvokeRuntime(QuickEntrypointEnum entrypoint,
@@ -1373,7 +1373,7 @@ void CodeGeneratorX86_64::GenerateFrameEntry() {
   __ subq(CpuRegister(RSP), Immediate(adjust));
   __ cfi().AdjustCFAOffset(adjust);
   uint32_t xmm_spill_location = GetFpuSpillStart();
-  size_t xmm_spill_slot_size = GetFloatingPointSpillSlotSize();
+  size_t xmm_spill_slot_size = GetCalleePreservedFPWidth();
 
   for (int i = arraysize(kFpuCalleeSaves) - 1; i >= 0; --i) {
     if (allocated_registers_.ContainsFloatingPointRegister(kFpuCalleeSaves[i])) {
@@ -1401,7 +1401,7 @@ void CodeGeneratorX86_64::GenerateFrameExit() {
   __ cfi().RememberState();
   if (!HasEmptyFrame()) {
     uint32_t xmm_spill_location = GetFpuSpillStart();
-    size_t xmm_spill_slot_size = GetFloatingPointSpillSlotSize();
+    size_t xmm_spill_slot_size = GetCalleePreservedFPWidth();
     for (size_t i = 0; i < arraysize(kFpuCalleeSaves); ++i) {
       if (allocated_registers_.ContainsFloatingPointRegister(kFpuCalleeSaves[i])) {
         int offset = xmm_spill_location + (xmm_spill_slot_size * i);
@@ -4882,6 +4882,15 @@ void InstructionCodeGeneratorX86_64::VisitStaticFieldSet(HStaticFieldSet* instru
   HandleFieldSet(instruction, instruction->GetFieldInfo(), instruction->GetValueCanBeNull());
 }
 
+void LocationsBuilderX86_64::VisitStringBuilderAppend(HStringBuilderAppend* instruction) {
+  codegen_->CreateStringBuilderAppendLocations(instruction, Location::RegisterLocation(RAX));
+}
+
+void InstructionCodeGeneratorX86_64::VisitStringBuilderAppend(HStringBuilderAppend* instruction) {
+  __ movl(CpuRegister(RDI), Immediate(instruction->GetFormat()->GetValue()));
+  codegen_->InvokeRuntime(kQuickStringBuilderAppend, instruction, instruction->GetDexPc());
+}
+
 void LocationsBuilderX86_64::VisitUnresolvedInstanceFieldGet(
     HUnresolvedInstanceFieldGet* instruction) {
   FieldAccessCallingConventionX86_64 calling_convention;
diff --git a/compiler/optimizing/code_generator_x86_64.h b/compiler/optimizing/code_generator_x86_64.h
index f74e130..6f294c7 100644
--- a/compiler/optimizing/code_generator_x86_64.h
+++ b/compiler/optimizing/code_generator_x86_64.h
@@ -333,12 +333,16 @@ class CodeGeneratorX86_64 : public CodeGenerator {
     return kX86_64WordSize;
   }
 
-  size_t GetFloatingPointSpillSlotSize() const override {
+  size_t GetSlowPathFPWidth() const override {
     return GetGraph()->HasSIMD()
         ? 2 * kX86_64WordSize   // 16 bytes == 2 x86_64 words for each spill
         : 1 * kX86_64WordSize;  //  8 bytes == 1 x86_64 words for each spill
   }
 
+  size_t GetCalleePreservedFPWidth() const override {
+    return 1 * kX86_64WordSize;
+  }
+
   HGraphVisitor* GetLocationBuilder() override {
     return &location_builder_;
   }
diff --git a/compiler/optimizing/codegen_test.cc b/compiler/optimizing/codegen_test.cc
index b5a7c13..e562a8e 100644
--- a/compiler/optimizing/codegen_test.cc
+++ b/compiler/optimizing/codegen_test.cc
@@ -834,6 +834,7 @@ TEST_F(CodegenTest, ARM64IsaVIXLFeaturesA75) {
   EXPECT_TRUE(features->Has(vixl::CPUFeatures::kCRC32));
   EXPECT_TRUE(features->Has(vixl::CPUFeatures::kDotProduct));
   EXPECT_TRUE(features->Has(vixl::CPUFeatures::kFPHalf));
+  EXPECT_TRUE(features->Has(vixl::CPUFeatures::kNEONHalf));
   EXPECT_TRUE(features->Has(vixl::CPUFeatures::kAtomics));
 }
 
@@ -847,9 +848,53 @@ TEST_F(CodegenTest, ARM64IsaVIXLFeaturesA53) {
   EXPECT_TRUE(features->Has(vixl::CPUFeatures::kCRC32));
   EXPECT_FALSE(features->Has(vixl::CPUFeatures::kDotProduct));
   EXPECT_FALSE(features->Has(vixl::CPUFeatures::kFPHalf));
+  EXPECT_FALSE(features->Has(vixl::CPUFeatures::kNEONHalf));
   EXPECT_FALSE(features->Has(vixl::CPUFeatures::kAtomics));
 }
 
+constexpr static size_t kExpectedFPSpillSize = 8 * vixl::aarch64::kDRegSizeInBytes;
+
+// The following two tests check that for both SIMD and non-SIMD graphs exactly 64-bit is
+// allocated on stack per callee-saved FP register to be preserved in the frame entry as
+// ABI states.
+TEST_F(CodegenTest, ARM64FrameSizeSIMD) {
+  OverrideInstructionSetFeatures(InstructionSet::kArm64, "default");
+  HGraph* graph = CreateGraph();
+  arm64::CodeGeneratorARM64 codegen(graph, *compiler_options_);
+
+  codegen.Initialize();
+  graph->SetHasSIMD(true);
+
+  DCHECK_EQ(arm64::callee_saved_fp_registers.GetCount(), 8);
+  vixl::aarch64::CPURegList reg_list = arm64::callee_saved_fp_registers;
+  while (!reg_list.IsEmpty()) {
+    uint32_t reg_code = reg_list.PopLowestIndex().GetCode();
+    codegen.AddAllocatedRegister(Location::FpuRegisterLocation(reg_code));
+  }
+  codegen.ComputeSpillMask();
+
+  EXPECT_EQ(codegen.GetFpuSpillSize(), kExpectedFPSpillSize);
+}
+
+TEST_F(CodegenTest, ARM64FrameSizeNoSIMD) {
+  OverrideInstructionSetFeatures(InstructionSet::kArm64, "default");
+  HGraph* graph = CreateGraph();
+  arm64::CodeGeneratorARM64 codegen(graph, *compiler_options_);
+
+  codegen.Initialize();
+  graph->SetHasSIMD(false);
+
+  DCHECK_EQ(arm64::callee_saved_fp_registers.GetCount(), 8);
+  vixl::aarch64::CPURegList reg_list = arm64::callee_saved_fp_registers;
+  while (!reg_list.IsEmpty()) {
+    uint32_t reg_code = reg_list.PopLowestIndex().GetCode();
+    codegen.AddAllocatedRegister(Location::FpuRegisterLocation(reg_code));
+  }
+  codegen.ComputeSpillMask();
+
+  EXPECT_EQ(codegen.GetFpuSpillSize(), kExpectedFPSpillSize);
+}
+
 #endif
 
 #ifdef ART_ENABLE_CODEGEN_mips
diff --git a/compiler/optimizing/common_arm.h b/compiler/optimizing/common_arm.h
index 7d3af95..320915e 100644
--- a/compiler/optimizing/common_arm.h
+++ b/compiler/optimizing/common_arm.h
@@ -17,7 +17,6 @@
 #ifndef ART_COMPILER_OPTIMIZING_COMMON_ARM_H_
 #define ART_COMPILER_OPTIMIZING_COMMON_ARM_H_
 
-#include "dwarf/register.h"
 #include "instruction_simplifier_shared.h"
 #include "locations.h"
 #include "nodes.h"
@@ -38,14 +37,6 @@ namespace helpers {
 
 static_assert(vixl::aarch32::kSpCode == SP, "vixl::aarch32::kSpCode must equal ART's SP");
 
-inline dwarf::Reg DWARFReg(vixl::aarch32::Register reg) {
-  return dwarf::Reg::ArmCore(static_cast<int>(reg.GetCode()));
-}
-
-inline dwarf::Reg DWARFReg(vixl::aarch32::SRegister reg) {
-  return dwarf::Reg::ArmFp(static_cast<int>(reg.GetCode()));
-}
-
 inline vixl::aarch32::Register HighRegisterFrom(Location location) {
   DCHECK(location.IsRegisterPair()) << location;
   return vixl::aarch32::Register(location.AsRegisterPairHigh<vixl::aarch32::Register>());
diff --git a/compiler/optimizing/common_arm64.h b/compiler/optimizing/common_arm64.h
index 5556f16..20ec4e6 100644
--- a/compiler/optimizing/common_arm64.h
+++ b/compiler/optimizing/common_arm64.h
@@ -107,6 +107,11 @@ inline vixl::aarch64::FPRegister SRegisterFrom(Location location) {
   return vixl::aarch64::FPRegister::GetSRegFromCode(location.reg());
 }
 
+inline vixl::aarch64::FPRegister HRegisterFrom(Location location) {
+  DCHECK(location.IsFpuRegister()) << location;
+  return vixl::aarch64::FPRegister::GetHRegFromCode(location.reg());
+}
+
 inline vixl::aarch64::FPRegister FPRegisterFrom(Location location, DataType::Type type) {
   DCHECK(DataType::IsFloatingPointType(type)) << type;
   return type == DataType::Type::kFloat64 ? DRegisterFrom(location) : SRegisterFrom(location);
diff --git a/compiler/optimizing/constant_folding.cc b/compiler/optimizing/constant_folding.cc
index 09e7cab..2031707 100644
--- a/compiler/optimizing/constant_folding.cc
+++ b/compiler/optimizing/constant_folding.cc
@@ -217,6 +217,7 @@ void InstructionWithAbsorbingInputSimplifier::VisitBelowOrEqual(HBelowOrEqual* i
 }
 
 void InstructionWithAbsorbingInputSimplifier::VisitAnd(HAnd* instruction) {
+  DataType::Type type = instruction->GetType();
   HConstant* input_cst = instruction->GetConstantRight();
   if ((input_cst != nullptr) && input_cst->IsZeroBitPattern()) {
     // Replace code looking like
@@ -226,6 +227,25 @@ void InstructionWithAbsorbingInputSimplifier::VisitAnd(HAnd* instruction) {
     instruction->ReplaceWith(input_cst);
     instruction->GetBlock()->RemoveInstruction(instruction);
   }
+
+  HInstruction* left = instruction->GetLeft();
+  HInstruction* right = instruction->GetRight();
+
+  if (left->IsNot() ^ right->IsNot()) {
+    // Replace code looking like
+    //    NOT notsrc, src
+    //    AND dst, notsrc, src
+    // with
+    //    CONSTANT 0
+    HInstruction* hnot = (left->IsNot() ? left : right);
+    HInstruction* hother = (left->IsNot() ? right : left);
+    HInstruction* src = hnot->AsNot()->GetInput();
+
+    if (src == hother) {
+      instruction->ReplaceWith(GetGraph()->GetConstant(type, 0));
+      instruction->GetBlock()->RemoveInstruction(instruction);
+    }
+  }
 }
 
 void InstructionWithAbsorbingInputSimplifier::VisitCompare(HCompare* instruction) {
diff --git a/compiler/optimizing/instruction_simplifier.cc b/compiler/optimizing/instruction_simplifier.cc
index ce62495..63c5965 100644
--- a/compiler/optimizing/instruction_simplifier.cc
+++ b/compiler/optimizing/instruction_simplifier.cc
@@ -25,6 +25,7 @@
 #include "mirror/class-inl.h"
 #include "scoped_thread_state_change-inl.h"
 #include "sharpening.h"
+#include "string_builder_append.h"
 
 namespace art {
 
@@ -2467,6 +2468,186 @@ static bool NoEscapeForStringBufferReference(HInstruction* reference, HInstructi
   return false;
 }
 
+static bool TryReplaceStringBuilderAppend(HInvoke* invoke) {
+  DCHECK_EQ(invoke->GetIntrinsic(), Intrinsics::kStringBuilderToString);
+  if (invoke->CanThrowIntoCatchBlock()) {
+    return false;
+  }
+
+  HBasicBlock* block = invoke->GetBlock();
+  HInstruction* sb = invoke->InputAt(0);
+
+  // We support only a new StringBuilder, otherwise we cannot ensure that
+  // the StringBuilder data does not need to be populated for other users.
+  if (!sb->IsNewInstance()) {
+    return false;
+  }
+
+  // For now, we support only single-block recognition.
+  // (Ternary operators feeding the append could be implemented.)
+  for (const HUseListNode<HInstruction*>& use : sb->GetUses()) {
+    if (use.GetUser()->GetBlock() != block) {
+      return false;
+    }
+  }
+
+  // Collect args and check for unexpected uses.
+  // We expect one call to a constructor with no arguments, one constructor fence (unless
+  // eliminated), some number of append calls and one call to StringBuilder.toString().
+  bool seen_constructor = false;
+  bool seen_constructor_fence = false;
+  bool seen_to_string = false;
+  uint32_t format = 0u;
+  uint32_t num_args = 0u;
+  HInstruction* args[StringBuilderAppend::kMaxArgs];  // Added in reverse order.
+  for (const HUseListNode<HInstruction*>& use : sb->GetUses()) {
+    // The append pattern uses the StringBuilder only as the first argument.
+    if (use.GetIndex() != 0u) {
+      return false;
+    }
+    // We see the uses in reverse order because they are inserted at the front
+    // of the singly-linked list, so the StringBuilder.append() must come first.
+    HInstruction* user = use.GetUser();
+    if (!seen_to_string) {
+      if (user->IsInvokeVirtual() &&
+          user->AsInvokeVirtual()->GetIntrinsic() == Intrinsics::kStringBuilderToString) {
+        seen_to_string = true;
+        continue;
+      } else {
+        return false;
+      }
+    }
+    // Then we should see the arguments.
+    if (user->IsInvokeVirtual()) {
+      HInvokeVirtual* as_invoke_virtual = user->AsInvokeVirtual();
+      DCHECK(!seen_constructor);
+      DCHECK(!seen_constructor_fence);
+      StringBuilderAppend::Argument arg;
+      switch (as_invoke_virtual->GetIntrinsic()) {
+        case Intrinsics::kStringBuilderAppendObject:
+          // TODO: Unimplemented, needs to call String.valueOf().
+          return false;
+        case Intrinsics::kStringBuilderAppendString:
+          arg = StringBuilderAppend::Argument::kString;
+          break;
+        case Intrinsics::kStringBuilderAppendCharArray:
+          // TODO: Unimplemented, StringBuilder.append(char[]) can throw NPE and we would
+          // not have the correct stack trace for it.
+          return false;
+        case Intrinsics::kStringBuilderAppendBoolean:
+          arg = StringBuilderAppend::Argument::kBoolean;
+          break;
+        case Intrinsics::kStringBuilderAppendChar:
+          arg = StringBuilderAppend::Argument::kChar;
+          break;
+        case Intrinsics::kStringBuilderAppendInt:
+          arg = StringBuilderAppend::Argument::kInt;
+          break;
+        case Intrinsics::kStringBuilderAppendLong:
+          arg = StringBuilderAppend::Argument::kLong;
+          break;
+        case Intrinsics::kStringBuilderAppendCharSequence: {
+          ReferenceTypeInfo rti = user->AsInvokeVirtual()->InputAt(1)->GetReferenceTypeInfo();
+          if (!rti.IsValid()) {
+            return false;
+          }
+          ScopedObjectAccess soa(Thread::Current());
+          Handle<mirror::Class> input_type = rti.GetTypeHandle();
+          DCHECK(input_type != nullptr);
+          if (input_type.Get() == GetClassRoot<mirror::String>()) {
+            arg = StringBuilderAppend::Argument::kString;
+          } else {
+            // TODO: Check and implement for StringBuilder. We could find the StringBuilder's
+            // internal char[] inconsistent with the length, or the string compression
+            // of the result could be compromised with a concurrent modification, and
+            // we would need to throw appropriate exceptions.
+            return false;
+          }
+          break;
+        }
+        case Intrinsics::kStringBuilderAppendFloat:
+        case Intrinsics::kStringBuilderAppendDouble:
+          // TODO: Unimplemented, needs to call FloatingDecimal.getBinaryToASCIIConverter().
+          return false;
+        default: {
+          return false;
+        }
+      }
+      // Uses of the append return value should have been replaced with the first input.
+      DCHECK(!as_invoke_virtual->HasUses());
+      DCHECK(!as_invoke_virtual->HasEnvironmentUses());
+      if (num_args == StringBuilderAppend::kMaxArgs) {
+        return false;
+      }
+      format = (format << StringBuilderAppend::kBitsPerArg) | static_cast<uint32_t>(arg);
+      args[num_args] = as_invoke_virtual->InputAt(1u);
+      ++num_args;
+    } else if (user->IsInvokeStaticOrDirect() &&
+               user->AsInvokeStaticOrDirect()->GetResolvedMethod() != nullptr &&
+               user->AsInvokeStaticOrDirect()->GetResolvedMethod()->IsConstructor() &&
+               user->AsInvokeStaticOrDirect()->GetNumberOfArguments() == 1u) {
+      // After arguments, we should see the constructor.
+      // We accept only the constructor with no extra arguments.
+      DCHECK(!seen_constructor);
+      DCHECK(!seen_constructor_fence);
+      seen_constructor = true;
+    } else if (user->IsConstructorFence()) {
+      // The last use we see is the constructor fence.
+      DCHECK(seen_constructor);
+      DCHECK(!seen_constructor_fence);
+      seen_constructor_fence = true;
+    } else {
+      return false;
+    }
+  }
+
+  if (num_args == 0u) {
+    return false;
+  }
+
+  // Check environment uses.
+  for (const HUseListNode<HEnvironment*>& use : sb->GetEnvUses()) {
+    HInstruction* holder = use.GetUser()->GetHolder();
+    if (holder->GetBlock() != block) {
+      return false;
+    }
+    // Accept only calls on the StringBuilder (which shall all be removed).
+    // TODO: Carve-out for const-string? Or rely on environment pruning (to be implemented)?
+    if (holder->InputCount() == 0 || holder->InputAt(0) != sb) {
+      return false;
+    }
+  }
+
+  // Create replacement instruction.
+  HIntConstant* fmt = block->GetGraph()->GetIntConstant(static_cast<int32_t>(format));
+  ArenaAllocator* allocator = block->GetGraph()->GetAllocator();
+  HStringBuilderAppend* append =
+      new (allocator) HStringBuilderAppend(fmt, num_args, allocator, invoke->GetDexPc());
+  append->SetReferenceTypeInfo(invoke->GetReferenceTypeInfo());
+  for (size_t i = 0; i != num_args; ++i) {
+    append->SetArgumentAt(i, args[num_args - 1u - i]);
+  }
+  block->InsertInstructionBefore(append, invoke);
+  invoke->ReplaceWith(append);
+  // Copy environment, except for the StringBuilder uses.
+  for (size_t i = 0, size = invoke->GetEnvironment()->Size(); i != size; ++i) {
+    if (invoke->GetEnvironment()->GetInstructionAt(i) == sb) {
+      invoke->GetEnvironment()->RemoveAsUserOfInput(i);
+      invoke->GetEnvironment()->SetRawEnvAt(i, nullptr);
+    }
+  }
+  append->CopyEnvironmentFrom(invoke->GetEnvironment());
+  // Remove the old instruction.
+  block->RemoveInstruction(invoke);
+  // Remove the StringBuilder's uses and StringBuilder.
+  while (sb->HasNonEnvironmentUses()) {
+    block->RemoveInstruction(sb->GetUses().front().GetUser());
+  }
+  DCHECK(!sb->HasEnvironmentUses());
+  block->RemoveInstruction(sb);
+  return true;
+}
+
 // Certain allocation intrinsics are not removed by dead code elimination
 // because of potentially throwing an OOM exception or other side effects.
 // This method removes such intrinsics when special circumstances allow.
@@ -2481,6 +2662,9 @@ void InstructionSimplifierVisitor::SimplifyAllocationIntrinsic(HInvoke* invoke)
       invoke->GetBlock()->RemoveInstruction(invoke);
       RecordSimplification();
     }
+  } else if (invoke->GetIntrinsic() == Intrinsics::kStringBuilderToString &&
+             TryReplaceStringBuilderAppend(invoke)) {
+    RecordSimplification();
   }
 }
 
@@ -2569,7 +2753,16 @@ void InstructionSimplifierVisitor::VisitInvoke(HInvoke* instruction) {
       SimplifyNPEOnArgN(instruction, 1);  // 0th has own NullCheck
       break;
     case Intrinsics::kStringBufferAppend:
-    case Intrinsics::kStringBuilderAppend:
+    case Intrinsics::kStringBuilderAppendObject:
+    case Intrinsics::kStringBuilderAppendString:
+    case Intrinsics::kStringBuilderAppendCharSequence:
+    case Intrinsics::kStringBuilderAppendCharArray:
+    case Intrinsics::kStringBuilderAppendBoolean:
+    case Intrinsics::kStringBuilderAppendChar:
+    case Intrinsics::kStringBuilderAppendInt:
+    case Intrinsics::kStringBuilderAppendLong:
+    case Intrinsics::kStringBuilderAppendFloat:
+    case Intrinsics::kStringBuilderAppendDouble:
       SimplifyReturnThis(instruction);
       break;
     case Intrinsics::kStringBufferToString:
diff --git a/compiler/optimizing/intrinsics_arm64.cc b/compiler/optimizing/intrinsics_arm64.cc
index ec5d17a..f84c66d 100644
--- a/compiler/optimizing/intrinsics_arm64.cc
+++ b/compiler/optimizing/intrinsics_arm64.cc
@@ -54,6 +54,7 @@ using helpers::RegisterFrom;
 using helpers::SRegisterFrom;
 using helpers::WRegisterFrom;
 using helpers::XRegisterFrom;
+using helpers::HRegisterFrom;
 using helpers::InputRegisterAt;
 using helpers::OutputRegister;
 
@@ -299,6 +300,14 @@ static void CreateIntToIntLocations(ArenaAllocator* allocator, HInvoke* invoke)
   locations->SetOut(Location::RequiresRegister(), Location::kNoOutputOverlap);
 }
 
+static void CreateIntIntToIntLocations(ArenaAllocator* allocator, HInvoke* invoke) {
+  LocationSummary* locations =
+      new (allocator) LocationSummary(invoke, LocationSummary::kNoCall, kIntrinsified);
+  locations->SetInAt(0, Location::RequiresRegister());
+  locations->SetInAt(1, Location::RequiresRegister());
+  locations->SetOut(Location::RequiresRegister(), Location::kNoOutputOverlap);
+}
+
 static void GenReverseBytes(LocationSummary* locations,
                             DataType::Type type,
                             MacroAssembler* masm) {
@@ -1960,7 +1969,8 @@ void IntrinsicCodeGeneratorARM64::VisitStringGetCharsNoCheck(HInvoke* invoke) {
   Register tmp2 = temps.AcquireX();
 
   vixl::aarch64::Label done;
-  vixl::aarch64::Label compressed_string_loop;
+  vixl::aarch64::Label compressed_string_vector_loop;
+  vixl::aarch64::Label compressed_string_remainder;
   __ Sub(num_chr, srcEnd, srcBegin);
   // Early out for valid zero-length retrievals.
   __ Cbz(num_chr, &done);
@@ -2013,16 +2023,39 @@ void IntrinsicCodeGeneratorARM64::VisitStringGetCharsNoCheck(HInvoke* invoke) {
   __ B(&done);
 
   if (mirror::kUseStringCompression) {
+    // For compressed strings, acquire a SIMD temporary register.
+    FPRegister vtmp1 = temps.AcquireVRegisterOfSize(kQRegSize);
     const size_t c_char_size = DataType::Size(DataType::Type::kInt8);
     DCHECK_EQ(c_char_size, 1u);
     __ Bind(&compressed_string_preloop);
     __ Add(src_ptr, src_ptr, Operand(srcBegin));
-    // Copy loop for compressed src, copying 1 character (8-bit) to (16-bit) at a time.
-    __ Bind(&compressed_string_loop);
+
+    // Save repairing the value of num_chr on the < 8 character path.
+    __ Subs(tmp1, num_chr, 8);
+    __ B(lt, &compressed_string_remainder);
+
+    // Keep the result of the earlier subs, we are going to fetch at least 8 characters.
+    __ Mov(num_chr, tmp1);
+
+    // Main loop for compressed src, copying 8 characters (8-bit) to (16-bit) at a time.
+    // Uses SIMD instructions.
+    __ Bind(&compressed_string_vector_loop);
+    __ Ld1(vtmp1.V8B(), MemOperand(src_ptr, c_char_size * 8, PostIndex));
+    __ Subs(num_chr, num_chr, 8);
+    __ Uxtl(vtmp1.V8H(), vtmp1.V8B());
+    __ St1(vtmp1.V8H(), MemOperand(dst_ptr, char_size * 8, PostIndex));
+    __ B(ge, &compressed_string_vector_loop);
+
+    __ Adds(num_chr, num_chr, 8);
+    __ B(eq, &done);
+
+    // Loop for < 8 character case and remainder handling with a compressed src.
+    // Copies 1 character (8-bit) to (16-bit) at a time.
+    __ Bind(&compressed_string_remainder);
     __ Ldrb(tmp1, MemOperand(src_ptr, c_char_size, PostIndex));
     __ Strh(tmp1, MemOperand(dst_ptr, char_size, PostIndex));
     __ Subs(num_chr, num_chr, Operand(1));
-    __ B(gt, &compressed_string_loop);
+    __ B(gt, &compressed_string_remainder);
   }
 
   __ Bind(&done);
@@ -2796,22 +2829,25 @@ static void GenIsInfinite(LocationSummary* locations,
                           bool is64bit,
                           MacroAssembler* masm) {
   Operand infinity;
+  Operand tst_mask;
   Register out;
 
   if (is64bit) {
     infinity = kPositiveInfinityDouble;
+    tst_mask = MaskLeastSignificant<uint64_t>(63);
     out = XRegisterFrom(locations->Out());
   } else {
     infinity = kPositiveInfinityFloat;
+    tst_mask = MaskLeastSignificant<uint32_t>(31);
     out = WRegisterFrom(locations->Out());
   }
 
-  const Register zero = vixl::aarch64::Assembler::AppropriateZeroRegFor(out);
-
   MoveFPToInt(locations, is64bit, masm);
+  // Checks whether exponent bits are all 1 and fraction bits are all 0.
   __ Eor(out, out, infinity);
-  // We don't care about the sign bit, so shift left.
-  __ Cmp(zero, Operand(out, LSL, 1));
+  // TST bitmask is used to mask out the sign bit: either 0x7fffffff or 0x7fffffffffffffff
+  // depending on is64bit.
+  __ Tst(out, tst_mask);
   __ Cset(out, eq);
 }
 
@@ -3169,6 +3205,203 @@ void IntrinsicCodeGeneratorARM64::VisitCRC32UpdateByteBuffer(HInvoke* invoke) {
   GenerateCodeForCalculationCRC32ValueOfBytes(masm, crc, ptr, length, out);
 }
 
+void IntrinsicLocationsBuilderARM64::VisitFP16ToFloat(HInvoke* invoke) {
+  if (!codegen_->GetInstructionSetFeatures().HasFP16()) {
+    return;
+  }
+
+  LocationSummary* locations = new (allocator_) LocationSummary(invoke,
+                                                                LocationSummary::kNoCall,
+                                                                kIntrinsified);
+  locations->SetInAt(0, Location::RequiresRegister());
+  locations->SetOut(Location::RequiresFpuRegister());
+}
+
+void IntrinsicCodeGeneratorARM64::VisitFP16ToFloat(HInvoke* invoke) {
+  DCHECK(codegen_->GetInstructionSetFeatures().HasFP16());
+  MacroAssembler* masm = GetVIXLAssembler();
+  UseScratchRegisterScope scratch_scope(masm);
+  Register bits = InputRegisterAt(invoke, 0);
+  FPRegister out = SRegisterFrom(invoke->GetLocations()->Out());
+  FPRegister half = scratch_scope.AcquireH();
+  __ Fmov(half, bits);  // ARMv8.2
+  __ Fcvt(out, half);
+}
+
+void IntrinsicLocationsBuilderARM64::VisitFP16ToHalf(HInvoke* invoke) {
+  if (!codegen_->GetInstructionSetFeatures().HasFP16()) {
+    return;
+  }
+
+  LocationSummary* locations = new (allocator_) LocationSummary(invoke,
+                                                                LocationSummary::kNoCall,
+                                                                kIntrinsified);
+  locations->SetInAt(0, Location::RequiresFpuRegister());
+  locations->SetOut(Location::RequiresRegister());
+}
+
+void IntrinsicCodeGeneratorARM64::VisitFP16ToHalf(HInvoke* invoke) {
+  DCHECK(codegen_->GetInstructionSetFeatures().HasFP16());
+  MacroAssembler* masm = GetVIXLAssembler();
+  UseScratchRegisterScope scratch_scope(masm);
+  FPRegister in = SRegisterFrom(invoke->GetLocations()->InAt(0));
+  FPRegister half = scratch_scope.AcquireH();
+  Register out = WRegisterFrom(invoke->GetLocations()->Out());
+  __ Fcvt(half, in);
+  __ Fmov(out, half);
+  __ Sxth(out, out);  // sign extend due to returning a short type.
+}
+
+template<typename OP>
+void GenerateFP16Round(HInvoke* invoke,
+                       CodeGeneratorARM64* const codegen_,
+                       MacroAssembler* masm,
+                       const OP roundOp) {
+  DCHECK(codegen_->GetInstructionSetFeatures().HasFP16());
+  LocationSummary* locations = invoke->GetLocations();
+  UseScratchRegisterScope scratch_scope(masm);
+  Register out = WRegisterFrom(locations->Out());
+  VRegister half = scratch_scope.AcquireH();
+  __ Fmov(half, WRegisterFrom(locations->InAt(0)));
+  roundOp(half, half);
+  __ Fmov(out, half);
+  __ Sxth(out, out);
+}
+
+void IntrinsicLocationsBuilderARM64::VisitFP16Floor(HInvoke* invoke) {
+  if (!codegen_->GetInstructionSetFeatures().HasFP16()) {
+    return;
+  }
+
+  CreateIntToIntLocations(allocator_, invoke);
+}
+
+void IntrinsicCodeGeneratorARM64::VisitFP16Floor(HInvoke* invoke) {
+  MacroAssembler* masm = GetVIXLAssembler();
+  auto roundOp = [masm](const VRegister& out, const VRegister& in) {
+    __ Frintm(out, in);  // Round towards Minus infinity
+  };
+  GenerateFP16Round(invoke, codegen_, masm, roundOp);
+}
+
+void IntrinsicLocationsBuilderARM64::VisitFP16Ceil(HInvoke* invoke) {
+  if (!codegen_->GetInstructionSetFeatures().HasFP16()) {
+    return;
+  }
+
+  CreateIntToIntLocations(allocator_, invoke);
+}
+
+void IntrinsicCodeGeneratorARM64::VisitFP16Ceil(HInvoke* invoke) {
+  MacroAssembler* masm = GetVIXLAssembler();
+  auto roundOp = [masm](const FPRegister& out, const FPRegister& in) {
+    __ Frintp(out, in);  // Round towards Plus infinity
+  };
+  GenerateFP16Round(invoke, codegen_, masm, roundOp);
+}
+
+void IntrinsicLocationsBuilderARM64::VisitFP16Rint(HInvoke* invoke) {
+  if (!codegen_->GetInstructionSetFeatures().HasFP16()) {
+    return;
+  }
+
+  CreateIntToIntLocations(allocator_, invoke);
+}
+
+void IntrinsicCodeGeneratorARM64::VisitFP16Rint(HInvoke* invoke) {
+  MacroAssembler* masm = GetVIXLAssembler();
+  auto roundOp = [masm](const FPRegister& out, const FPRegister& in) {
+    __ Frintn(out, in);  // Round to nearest, with ties to even
+  };
+  GenerateFP16Round(invoke, codegen_, masm, roundOp);
+}
+
+template<typename OP>
+void GenerateFP16Compare(HInvoke* invoke,
+                         CodeGeneratorARM64* codegen,
+                         MacroAssembler* masm,
+                         const OP compareOp) {
+  DCHECK(codegen->GetInstructionSetFeatures().HasFP16());
+  LocationSummary* locations = invoke->GetLocations();
+  Register out = WRegisterFrom(locations->Out());
+  VRegister half0 = HRegisterFrom(locations->GetTemp(0));
+  VRegister half1 = HRegisterFrom(locations->GetTemp(1));
+  __ Fmov(half0, WRegisterFrom(locations->InAt(0)));
+  __ Fmov(half1, WRegisterFrom(locations->InAt(1)));
+  compareOp(out, half0, half1);
+}
+
+static inline void GenerateFP16Compare(HInvoke* invoke,
+                                       CodeGeneratorARM64* codegen,
+                                       MacroAssembler* masm,
+                                       vixl::aarch64::Condition cond) {
+  auto compareOp = [masm, cond](const Register out, const VRegister& in0, const VRegister& in1) {
+    __ Fcmp(in0, in1);
+    __ Cset(out, cond);
+  };
+  GenerateFP16Compare(invoke, codegen, masm, compareOp);
+}
+
+void IntrinsicLocationsBuilderARM64::VisitFP16Greater(HInvoke* invoke) {
+  if (!codegen_->GetInstructionSetFeatures().HasFP16()) {
+    return;
+  }
+
+  CreateIntIntToIntLocations(allocator_, invoke);
+  invoke->GetLocations()->AddTemp(Location::RequiresFpuRegister());
+  invoke->GetLocations()->AddTemp(Location::RequiresFpuRegister());
+}
+
+void IntrinsicCodeGeneratorARM64::VisitFP16Greater(HInvoke* invoke) {
+  MacroAssembler* masm = GetVIXLAssembler();
+  GenerateFP16Compare(invoke, codegen_, masm, gt);
+}
+
+void IntrinsicLocationsBuilderARM64::VisitFP16GreaterEquals(HInvoke* invoke) {
+  if (!codegen_->GetInstructionSetFeatures().HasFP16()) {
+    return;
+  }
+
+  CreateIntIntToIntLocations(allocator_, invoke);
+  invoke->GetLocations()->AddTemp(Location::RequiresFpuRegister());
+  invoke->GetLocations()->AddTemp(Location::RequiresFpuRegister());
+}
+
+void IntrinsicCodeGeneratorARM64::VisitFP16GreaterEquals(HInvoke* invoke) {
+  MacroAssembler* masm = GetVIXLAssembler();
+  GenerateFP16Compare(invoke, codegen_, masm, ge);
+}
+
+void IntrinsicLocationsBuilderARM64::VisitFP16Less(HInvoke* invoke) {
+  if (!codegen_->GetInstructionSetFeatures().HasFP16()) {
+    return;
+  }
+
+  CreateIntIntToIntLocations(allocator_, invoke);
+  invoke->GetLocations()->AddTemp(Location::RequiresFpuRegister());
+  invoke->GetLocations()->AddTemp(Location::RequiresFpuRegister());
+}
+
+void IntrinsicCodeGeneratorARM64::VisitFP16Less(HInvoke* invoke) {
+  MacroAssembler* masm = GetVIXLAssembler();
+  GenerateFP16Compare(invoke, codegen_, masm, mi);
+}
+
+void IntrinsicLocationsBuilderARM64::VisitFP16LessEquals(HInvoke* invoke) {
+  if (!codegen_->GetInstructionSetFeatures().HasFP16()) {
+    return;
+  }
+
+  CreateIntIntToIntLocations(allocator_, invoke);
+  invoke->GetLocations()->AddTemp(Location::RequiresFpuRegister());
+  invoke->GetLocations()->AddTemp(Location::RequiresFpuRegister());
+}
+
+void IntrinsicCodeGeneratorARM64::VisitFP16LessEquals(HInvoke* invoke) {
+  MacroAssembler* masm = GetVIXLAssembler();
+  GenerateFP16Compare(invoke, codegen_, masm, ls);
+}
+
 UNIMPLEMENTED_INTRINSIC(ARM64, ReferenceGetReferent)
 
 UNIMPLEMENTED_INTRINSIC(ARM64, StringStringIndexOf);
@@ -3176,7 +3409,16 @@ UNIMPLEMENTED_INTRINSIC(ARM64, StringStringIndexOfAfter);
 UNIMPLEMENTED_INTRINSIC(ARM64, StringBufferAppend);
 UNIMPLEMENTED_INTRINSIC(ARM64, StringBufferLength);
 UNIMPLEMENTED_INTRINSIC(ARM64, StringBufferToString);
-UNIMPLEMENTED_INTRINSIC(ARM64, StringBuilderAppend);
+UNIMPLEMENTED_INTRINSIC(ARM64, StringBuilderAppendObject);
+UNIMPLEMENTED_INTRINSIC(ARM64, StringBuilderAppendString);
+UNIMPLEMENTED_INTRINSIC(ARM64, StringBuilderAppendCharSequence);
+UNIMPLEMENTED_INTRINSIC(ARM64, StringBuilderAppendCharArray);
+UNIMPLEMENTED_INTRINSIC(ARM64, StringBuilderAppendBoolean);
+UNIMPLEMENTED_INTRINSIC(ARM64, StringBuilderAppendChar);
+UNIMPLEMENTED_INTRINSIC(ARM64, StringBuilderAppendInt);
+UNIMPLEMENTED_INTRINSIC(ARM64, StringBuilderAppendLong);
+UNIMPLEMENTED_INTRINSIC(ARM64, StringBuilderAppendFloat);
+UNIMPLEMENTED_INTRINSIC(ARM64, StringBuilderAppendDouble);
 UNIMPLEMENTED_INTRINSIC(ARM64, StringBuilderLength);
 UNIMPLEMENTED_INTRINSIC(ARM64, StringBuilderToString);
 
diff --git a/compiler/optimizing/intrinsics_arm_vixl.cc b/compiler/optimizing/intrinsics_arm_vixl.cc
index f0aa92e..89e5203 100644
--- a/compiler/optimizing/intrinsics_arm_vixl.cc
+++ b/compiler/optimizing/intrinsics_arm_vixl.cc
@@ -3070,13 +3070,31 @@ UNIMPLEMENTED_INTRINSIC(ARMVIXL, ReferenceGetReferent)
 UNIMPLEMENTED_INTRINSIC(ARMVIXL, CRC32Update)
 UNIMPLEMENTED_INTRINSIC(ARMVIXL, CRC32UpdateBytes)
 UNIMPLEMENTED_INTRINSIC(ARMVIXL, CRC32UpdateByteBuffer)
+UNIMPLEMENTED_INTRINSIC(ARMVIXL, FP16ToFloat)
+UNIMPLEMENTED_INTRINSIC(ARMVIXL, FP16ToHalf)
+UNIMPLEMENTED_INTRINSIC(ARMVIXL, FP16Floor)
+UNIMPLEMENTED_INTRINSIC(ARMVIXL, FP16Ceil)
+UNIMPLEMENTED_INTRINSIC(ARMVIXL, FP16Rint)
+UNIMPLEMENTED_INTRINSIC(ARMVIXL, FP16Greater)
+UNIMPLEMENTED_INTRINSIC(ARMVIXL, FP16GreaterEquals)
+UNIMPLEMENTED_INTRINSIC(ARMVIXL, FP16Less)
+UNIMPLEMENTED_INTRINSIC(ARMVIXL, FP16LessEquals)
 
 UNIMPLEMENTED_INTRINSIC(ARMVIXL, StringStringIndexOf);
 UNIMPLEMENTED_INTRINSIC(ARMVIXL, StringStringIndexOfAfter);
 UNIMPLEMENTED_INTRINSIC(ARMVIXL, StringBufferAppend);
 UNIMPLEMENTED_INTRINSIC(ARMVIXL, StringBufferLength);
 UNIMPLEMENTED_INTRINSIC(ARMVIXL, StringBufferToString);
-UNIMPLEMENTED_INTRINSIC(ARMVIXL, StringBuilderAppend);
+UNIMPLEMENTED_INTRINSIC(ARMVIXL, StringBuilderAppendObject);
+UNIMPLEMENTED_INTRINSIC(ARMVIXL, StringBuilderAppendString);
+UNIMPLEMENTED_INTRINSIC(ARMVIXL, StringBuilderAppendCharSequence);
+UNIMPLEMENTED_INTRINSIC(ARMVIXL, StringBuilderAppendCharArray);
+UNIMPLEMENTED_INTRINSIC(ARMVIXL, StringBuilderAppendBoolean);
+UNIMPLEMENTED_INTRINSIC(ARMVIXL, StringBuilderAppendChar);
+UNIMPLEMENTED_INTRINSIC(ARMVIXL, StringBuilderAppendInt);
+UNIMPLEMENTED_INTRINSIC(ARMVIXL, StringBuilderAppendLong);
+UNIMPLEMENTED_INTRINSIC(ARMVIXL, StringBuilderAppendFloat);
+UNIMPLEMENTED_INTRINSIC(ARMVIXL, StringBuilderAppendDouble);
 UNIMPLEMENTED_INTRINSIC(ARMVIXL, StringBuilderLength);
 UNIMPLEMENTED_INTRINSIC(ARMVIXL, StringBuilderToString);
 
diff --git a/compiler/optimizing/intrinsics_mips.cc b/compiler/optimizing/intrinsics_mips.cc
index 3da0e57..537255f 100644
--- a/compiler/optimizing/intrinsics_mips.cc
+++ b/compiler/optimizing/intrinsics_mips.cc
@@ -2707,13 +2707,31 @@ UNIMPLEMENTED_INTRINSIC(MIPS, SystemArrayCopy)
 UNIMPLEMENTED_INTRINSIC(MIPS, CRC32Update)
 UNIMPLEMENTED_INTRINSIC(MIPS, CRC32UpdateBytes)
 UNIMPLEMENTED_INTRINSIC(MIPS, CRC32UpdateByteBuffer)
+UNIMPLEMENTED_INTRINSIC(MIPS, FP16ToFloat)
+UNIMPLEMENTED_INTRINSIC(MIPS, FP16ToHalf)
+UNIMPLEMENTED_INTRINSIC(MIPS, FP16Floor)
+UNIMPLEMENTED_INTRINSIC(MIPS, FP16Ceil)
+UNIMPLEMENTED_INTRINSIC(MIPS, FP16Rint)
+UNIMPLEMENTED_INTRINSIC(MIPS, FP16Greater)
+UNIMPLEMENTED_INTRINSIC(MIPS, FP16GreaterEquals)
+UNIMPLEMENTED_INTRINSIC(MIPS, FP16Less)
+UNIMPLEMENTED_INTRINSIC(MIPS, FP16LessEquals)
 
 UNIMPLEMENTED_INTRINSIC(MIPS, StringStringIndexOf);
 UNIMPLEMENTED_INTRINSIC(MIPS, StringStringIndexOfAfter);
 UNIMPLEMENTED_INTRINSIC(MIPS, StringBufferAppend);
 UNIMPLEMENTED_INTRINSIC(MIPS, StringBufferLength);
 UNIMPLEMENTED_INTRINSIC(MIPS, StringBufferToString);
-UNIMPLEMENTED_INTRINSIC(MIPS, StringBuilderAppend);
+UNIMPLEMENTED_INTRINSIC(MIPS, StringBuilderAppendObject);
+UNIMPLEMENTED_INTRINSIC(MIPS, StringBuilderAppendString);
+UNIMPLEMENTED_INTRINSIC(MIPS, StringBuilderAppendCharSequence);
+UNIMPLEMENTED_INTRINSIC(MIPS, StringBuilderAppendCharArray);
+UNIMPLEMENTED_INTRINSIC(MIPS, StringBuilderAppendBoolean);
+UNIMPLEMENTED_INTRINSIC(MIPS, StringBuilderAppendChar);
+UNIMPLEMENTED_INTRINSIC(MIPS, StringBuilderAppendInt);
+UNIMPLEMENTED_INTRINSIC(MIPS, StringBuilderAppendLong);
+UNIMPLEMENTED_INTRINSIC(MIPS, StringBuilderAppendFloat);
+UNIMPLEMENTED_INTRINSIC(MIPS, StringBuilderAppendDouble);
 UNIMPLEMENTED_INTRINSIC(MIPS, StringBuilderLength);
 UNIMPLEMENTED_INTRINSIC(MIPS, StringBuilderToString);
 
diff --git a/compiler/optimizing/intrinsics_mips64.cc b/compiler/optimizing/intrinsics_mips64.cc
index 3e68765..5920394 100644
--- a/compiler/optimizing/intrinsics_mips64.cc
+++ b/compiler/optimizing/intrinsics_mips64.cc
@@ -2357,13 +2357,31 @@ UNIMPLEMENTED_INTRINSIC(MIPS64, SystemArrayCopy)
 UNIMPLEMENTED_INTRINSIC(MIPS64, CRC32Update)
 UNIMPLEMENTED_INTRINSIC(MIPS64, CRC32UpdateBytes)
 UNIMPLEMENTED_INTRINSIC(MIPS64, CRC32UpdateByteBuffer)
+UNIMPLEMENTED_INTRINSIC(MIPS64, FP16ToFloat)
+UNIMPLEMENTED_INTRINSIC(MIPS64, FP16ToHalf)
+UNIMPLEMENTED_INTRINSIC(MIPS64, FP16Floor)
+UNIMPLEMENTED_INTRINSIC(MIPS64, FP16Ceil)
+UNIMPLEMENTED_INTRINSIC(MIPS64, FP16Rint)
+UNIMPLEMENTED_INTRINSIC(MIPS64, FP16Greater)
+UNIMPLEMENTED_INTRINSIC(MIPS64, FP16GreaterEquals)
+UNIMPLEMENTED_INTRINSIC(MIPS64, FP16Less)
+UNIMPLEMENTED_INTRINSIC(MIPS64, FP16LessEquals)
 
 UNIMPLEMENTED_INTRINSIC(MIPS64, StringStringIndexOf);
 UNIMPLEMENTED_INTRINSIC(MIPS64, StringStringIndexOfAfter);
 UNIMPLEMENTED_INTRINSIC(MIPS64, StringBufferAppend);
 UNIMPLEMENTED_INTRINSIC(MIPS64, StringBufferLength);
 UNIMPLEMENTED_INTRINSIC(MIPS64, StringBufferToString);
-UNIMPLEMENTED_INTRINSIC(MIPS64, StringBuilderAppend);
+UNIMPLEMENTED_INTRINSIC(MIPS64, StringBuilderAppendObject);
+UNIMPLEMENTED_INTRINSIC(MIPS64, StringBuilderAppendString);
+UNIMPLEMENTED_INTRINSIC(MIPS64, StringBuilderAppendCharSequence);
+UNIMPLEMENTED_INTRINSIC(MIPS64, StringBuilderAppendCharArray);
+UNIMPLEMENTED_INTRINSIC(MIPS64, StringBuilderAppendBoolean);
+UNIMPLEMENTED_INTRINSIC(MIPS64, StringBuilderAppendChar);
+UNIMPLEMENTED_INTRINSIC(MIPS64, StringBuilderAppendInt);
+UNIMPLEMENTED_INTRINSIC(MIPS64, StringBuilderAppendLong);
+UNIMPLEMENTED_INTRINSIC(MIPS64, StringBuilderAppendFloat);
+UNIMPLEMENTED_INTRINSIC(MIPS64, StringBuilderAppendDouble);
 UNIMPLEMENTED_INTRINSIC(MIPS64, StringBuilderLength);
 UNIMPLEMENTED_INTRINSIC(MIPS64, StringBuilderToString);
 
diff --git a/compiler/optimizing/intrinsics_x86.cc b/compiler/optimizing/intrinsics_x86.cc
index de697f0..6d7462e 100644
--- a/compiler/optimizing/intrinsics_x86.cc
+++ b/compiler/optimizing/intrinsics_x86.cc
@@ -3081,13 +3081,31 @@ UNIMPLEMENTED_INTRINSIC(X86, LongHighestOneBit)
 UNIMPLEMENTED_INTRINSIC(X86, CRC32Update)
 UNIMPLEMENTED_INTRINSIC(X86, CRC32UpdateBytes)
 UNIMPLEMENTED_INTRINSIC(X86, CRC32UpdateByteBuffer)
+UNIMPLEMENTED_INTRINSIC(X86, FP16ToFloat)
+UNIMPLEMENTED_INTRINSIC(X86, FP16ToHalf)
+UNIMPLEMENTED_INTRINSIC(X86, FP16Floor)
+UNIMPLEMENTED_INTRINSIC(X86, FP16Ceil)
+UNIMPLEMENTED_INTRINSIC(X86, FP16Rint)
+UNIMPLEMENTED_INTRINSIC(X86, FP16Greater)
+UNIMPLEMENTED_INTRINSIC(X86, FP16GreaterEquals)
+UNIMPLEMENTED_INTRINSIC(X86, FP16Less)
+UNIMPLEMENTED_INTRINSIC(X86, FP16LessEquals)
 
 UNIMPLEMENTED_INTRINSIC(X86, StringStringIndexOf);
 UNIMPLEMENTED_INTRINSIC(X86, StringStringIndexOfAfter);
 UNIMPLEMENTED_INTRINSIC(X86, StringBufferAppend);
 UNIMPLEMENTED_INTRINSIC(X86, StringBufferLength);
 UNIMPLEMENTED_INTRINSIC(X86, StringBufferToString);
-UNIMPLEMENTED_INTRINSIC(X86, StringBuilderAppend);
+UNIMPLEMENTED_INTRINSIC(X86, StringBuilderAppendObject);
+UNIMPLEMENTED_INTRINSIC(X86, StringBuilderAppendString);
+UNIMPLEMENTED_INTRINSIC(X86, StringBuilderAppendCharSequence);
+UNIMPLEMENTED_INTRINSIC(X86, StringBuilderAppendCharArray);
+UNIMPLEMENTED_INTRINSIC(X86, StringBuilderAppendBoolean);
+UNIMPLEMENTED_INTRINSIC(X86, StringBuilderAppendChar);
+UNIMPLEMENTED_INTRINSIC(X86, StringBuilderAppendInt);
+UNIMPLEMENTED_INTRINSIC(X86, StringBuilderAppendLong);
+UNIMPLEMENTED_INTRINSIC(X86, StringBuilderAppendFloat);
+UNIMPLEMENTED_INTRINSIC(X86, StringBuilderAppendDouble);
 UNIMPLEMENTED_INTRINSIC(X86, StringBuilderLength);
 UNIMPLEMENTED_INTRINSIC(X86, StringBuilderToString);
 
diff --git a/compiler/optimizing/intrinsics_x86_64.cc b/compiler/optimizing/intrinsics_x86_64.cc
index e79c0c9..0f6b006 100644
--- a/compiler/optimizing/intrinsics_x86_64.cc
+++ b/compiler/optimizing/intrinsics_x86_64.cc
@@ -2748,13 +2748,31 @@ UNIMPLEMENTED_INTRINSIC(X86_64, DoubleIsInfinite)
 UNIMPLEMENTED_INTRINSIC(X86_64, CRC32Update)
 UNIMPLEMENTED_INTRINSIC(X86_64, CRC32UpdateBytes)
 UNIMPLEMENTED_INTRINSIC(X86_64, CRC32UpdateByteBuffer)
+UNIMPLEMENTED_INTRINSIC(X86_64, FP16ToFloat)
+UNIMPLEMENTED_INTRINSIC(X86_64, FP16ToHalf)
+UNIMPLEMENTED_INTRINSIC(X86_64, FP16Floor)
+UNIMPLEMENTED_INTRINSIC(X86_64, FP16Ceil)
+UNIMPLEMENTED_INTRINSIC(X86_64, FP16Rint)
+UNIMPLEMENTED_INTRINSIC(X86_64, FP16Greater)
+UNIMPLEMENTED_INTRINSIC(X86_64, FP16GreaterEquals)
+UNIMPLEMENTED_INTRINSIC(X86_64, FP16Less)
+UNIMPLEMENTED_INTRINSIC(X86_64, FP16LessEquals)
 
 UNIMPLEMENTED_INTRINSIC(X86_64, StringStringIndexOf);
 UNIMPLEMENTED_INTRINSIC(X86_64, StringStringIndexOfAfter);
 UNIMPLEMENTED_INTRINSIC(X86_64, StringBufferAppend);
 UNIMPLEMENTED_INTRINSIC(X86_64, StringBufferLength);
 UNIMPLEMENTED_INTRINSIC(X86_64, StringBufferToString);
-UNIMPLEMENTED_INTRINSIC(X86_64, StringBuilderAppend);
+UNIMPLEMENTED_INTRINSIC(X86_64, StringBuilderAppendObject);
+UNIMPLEMENTED_INTRINSIC(X86_64, StringBuilderAppendString);
+UNIMPLEMENTED_INTRINSIC(X86_64, StringBuilderAppendCharSequence);
+UNIMPLEMENTED_INTRINSIC(X86_64, StringBuilderAppendCharArray);
+UNIMPLEMENTED_INTRINSIC(X86_64, StringBuilderAppendBoolean);
+UNIMPLEMENTED_INTRINSIC(X86_64, StringBuilderAppendChar);
+UNIMPLEMENTED_INTRINSIC(X86_64, StringBuilderAppendInt);
+UNIMPLEMENTED_INTRINSIC(X86_64, StringBuilderAppendLong);
+UNIMPLEMENTED_INTRINSIC(X86_64, StringBuilderAppendFloat);
+UNIMPLEMENTED_INTRINSIC(X86_64, StringBuilderAppendDouble);
 UNIMPLEMENTED_INTRINSIC(X86_64, StringBuilderLength);
 UNIMPLEMENTED_INTRINSIC(X86_64, StringBuilderToString);
 
diff --git a/compiler/optimizing/load_store_elimination.cc b/compiler/optimizing/load_store_elimination.cc
index b33d0f4..4c150da 100644
--- a/compiler/optimizing/load_store_elimination.cc
+++ b/compiler/optimizing/load_store_elimination.cc
@@ -23,8 +23,6 @@
 #include "load_store_analysis.h"
 #include "side_effects_analysis.h"
 
-#include <iostream>
-
 /**
  * The general algorithm of load-store elimination (LSE).
  * Load-store analysis in the previous pass collects a list of heap locations
@@ -64,8 +62,9 @@
  *   all the heap values, depending on the instruction's side effects.
  * - Finalizable objects are considered as persisting at method
  *   return/deoptimization.
- * - Currently this LSE algorithm doesn't handle SIMD graph, e.g. with VecLoad
- *   and VecStore instructions.
+ * - SIMD graphs (with VecLoad and VecStore instructions) are also handled. Any
+ *   partial overlap access among ArrayGet/ArraySet/VecLoad/Store is seen as
+ *   alias and no load/store is eliminated in such case.
  * - Currently this LSE algorithm doesn't handle graph with try-catch, due to
  *   the special block merging structure.
  */
@@ -172,9 +171,7 @@ class LSEVisitor : public HGraphDelegateVisitor {
         DCHECK(substitute2->IsTypeConversion());
         continue;
       }
-      DCHECK(load2->IsInstanceFieldGet() ||
-             load2->IsStaticFieldGet() ||
-             load2->IsArrayGet());
+      DCHECK(IsLoad(load2));
       DCHECK(substitute2 != nullptr);
       if (substitute2 == substitute &&
           load2->GetType() == load->GetType() &&
@@ -204,9 +201,7 @@ class LSEVisitor : public HGraphDelegateVisitor {
         DCHECK(substitute_instructions_for_loads_[i]->IsTypeConversion());
         continue;
       }
-      DCHECK(load->IsInstanceFieldGet() ||
-             load->IsStaticFieldGet() ||
-             load->IsArrayGet());
+      DCHECK(IsLoad(load));
       HInstruction* substitute = substitute_instructions_for_loads_[i];
       DCHECK(substitute != nullptr);
       // We proactively retrieve the substitute for a removed load, so
@@ -224,7 +219,7 @@ class LSEVisitor : public HGraphDelegateVisitor {
       // We guarantee that type A stored as type B and then fetched out as
       // type C is the same as casting from type A to type C directly, since
       // type B and type C will have the same size which is guarenteed in
-      // HInstanceFieldGet/HStaticFieldGet/HArrayGet's SetType().
+      // HInstanceFieldGet/HStaticFieldGet/HArrayGet/HVecLoad's SetType().
       // So we only need one type conversion from type A to type C.
       HTypeConversion* type_conversion = AddTypeConversionIfNecessary(
           load, substitute, load->GetType());
@@ -240,7 +235,7 @@ class LSEVisitor : public HGraphDelegateVisitor {
 
     // At this point, stores in possibly_removed_stores_ can be safely removed.
     for (HInstruction* store : possibly_removed_stores_) {
-      DCHECK(store->IsInstanceFieldSet() || store->IsStaticFieldSet() || store->IsArraySet());
+      DCHECK(IsStore(store));
       store->GetBlock()->RemoveInstruction(store);
     }
 
@@ -261,26 +256,37 @@ class LSEVisitor : public HGraphDelegateVisitor {
   }
 
  private:
-  static bool IsLoad(HInstruction* instruction) {
+  static bool IsLoad(const HInstruction* instruction) {
     if (instruction == kUnknownHeapValue || instruction == kDefaultHeapValue) {
       return false;
     }
     // Unresolved load is not treated as a load.
     return instruction->IsInstanceFieldGet() ||
         instruction->IsStaticFieldGet() ||
+        instruction->IsVecLoad() ||
         instruction->IsArrayGet();
   }
 
-  static bool IsStore(HInstruction* instruction) {
+  static bool IsStore(const HInstruction* instruction) {
     if (instruction == kUnknownHeapValue || instruction == kDefaultHeapValue) {
       return false;
     }
     // Unresolved store is not treated as a store.
     return instruction->IsInstanceFieldSet() ||
         instruction->IsArraySet() ||
+        instruction->IsVecStore() ||
         instruction->IsStaticFieldSet();
   }
 
+  // Check if it is allowed to use default values for the specified load.
+  static bool IsDefaultAllowedForLoad(const HInstruction* load) {
+    DCHECK(IsLoad(load));
+    // Using defaults for VecLoads requires to create additional vector operations.
+    // As there are some issues with scheduling vector operations it is better to avoid creating
+    // them.
+    return !load->IsVecOperation();
+  }
+
   // Returns the real heap value by finding its substitute or by "peeling"
   // a store instruction.
   HInstruction* GetRealHeapValue(HInstruction* heap_value) {
@@ -298,6 +304,8 @@ class LSEVisitor : public HGraphDelegateVisitor {
       heap_value = heap_value->AsInstanceFieldSet()->GetValue();
     } else if (heap_value->IsStaticFieldSet()) {
       heap_value = heap_value->AsStaticFieldSet()->GetValue();
+    } else if (heap_value->IsVecStore()) {
+      heap_value = heap_value->AsVecStore()->GetValue();
     } else {
       DCHECK(heap_value->IsArraySet());
       heap_value = heap_value->AsArraySet()->GetValue();
@@ -553,10 +561,15 @@ class LSEVisitor : public HGraphDelegateVisitor {
         heap_values_for_[instruction->GetBlock()->GetBlockId()];
     HInstruction* heap_value = heap_values[idx];
     if (heap_value == kDefaultHeapValue) {
-      HInstruction* constant = GetDefaultValue(instruction->GetType());
-      AddRemovedLoad(instruction, constant);
-      heap_values[idx] = constant;
-      return;
+      if (IsDefaultAllowedForLoad(instruction)) {
+        HInstruction* constant = GetDefaultValue(instruction->GetType());
+        AddRemovedLoad(instruction, constant);
+        heap_values[idx] = constant;
+        return;
+      } else {
+        heap_values[idx] = kUnknownHeapValue;
+        heap_value = kUnknownHeapValue;
+      }
     }
     heap_value = GetRealHeapValue(heap_value);
     if (heap_value == kUnknownHeapValue) {
@@ -590,6 +603,35 @@ class LSEVisitor : public HGraphDelegateVisitor {
     return false;
   }
 
+  bool CanValueBeKeptIfSameAsNew(HInstruction* value,
+                                 HInstruction* new_value,
+                                 HInstruction* new_value_set_instr) {
+    // For field/array set location operations, if the value is the same as the new_value
+    // it can be kept even if aliasing happens. All aliased operations will access the same memory
+    // range.
+    // For vector values, this is not true. For example:
+    //  packed_data = [0xA, 0xB, 0xC, 0xD];            <-- Different values in each lane.
+    //  VecStore array[i  ,i+1,i+2,i+3] = packed_data;
+    //  VecStore array[i+1,i+2,i+3,i+4] = packed_data; <-- We are here (partial overlap).
+    //  VecLoad  vx = array[i,i+1,i+2,i+3];            <-- Cannot be eliminated because the value
+    //                                                     here is not packed_data anymore.
+    //
+    // TODO: to allow such 'same value' optimization on vector data,
+    // LSA needs to report more fine-grain MAY alias information:
+    // (1) May alias due to two vector data partial overlap.
+    //     e.g. a[i..i+3] and a[i+1,..,i+4].
+    // (2) May alias due to two vector data may complete overlap each other.
+    //     e.g. a[i..i+3] and b[i..i+3].
+    // (3) May alias but the exact relationship between two locations is unknown.
+    //     e.g. a[i..i+3] and b[j..j+3], where values of a,b,i,j are all unknown.
+    // This 'same value' optimization can apply only on case (2).
+    if (new_value_set_instr->IsVecOperation()) {
+      return false;
+    }
+
+    return Equal(value, new_value);
+  }
+
   void VisitSetLocation(HInstruction* instruction, size_t idx, HInstruction* value) {
     DCHECK_NE(idx, HeapLocationCollector::kHeapLocationNotFound);
     DCHECK(!IsStore(value)) << value->DebugName();
@@ -636,23 +678,16 @@ class LSEVisitor : public HGraphDelegateVisitor {
 
     // This store may kill values in other heap locations due to aliasing.
     for (size_t i = 0; i < heap_values.size(); i++) {
-      if (i == idx) {
-        continue;
-      }
-      if (Equal(heap_values[i], value)) {
-        // Same value should be kept even if aliasing happens.
+      if (i == idx ||
+          heap_values[i] == kUnknownHeapValue ||
+          CanValueBeKeptIfSameAsNew(heap_values[i], value, instruction) ||
+          !heap_location_collector_.MayAlias(i, idx)) {
         continue;
       }
-      if (heap_values[i] == kUnknownHeapValue) {
-        // Value is already unknown, no need for aliasing check.
-        continue;
-      }
-      if (heap_location_collector_.MayAlias(i, idx)) {
-        // Kill heap locations that may alias and as a result if the heap value
-        // is a store, the store needs to be kept.
-        KeepIfIsStore(heap_values[i]);
-        heap_values[i] = kUnknownHeapValue;
-      }
+      // Kill heap locations that may alias and as a result if the heap value
+      // is a store, the store needs to be kept.
+      KeepIfIsStore(heap_values[i]);
+      heap_values[i] = kUnknownHeapValue;
     }
   }
 
@@ -689,7 +724,16 @@ class LSEVisitor : public HGraphDelegateVisitor {
 
   void VisitArraySet(HArraySet* instruction) override {
     size_t idx = heap_location_collector_.GetArrayHeapLocation(instruction);
-    VisitSetLocation(instruction, idx, instruction->InputAt(2));
+    VisitSetLocation(instruction, idx, instruction->GetValue());
+  }
+
+  void VisitVecLoad(HVecLoad* instruction) override {
+    VisitGetLocation(instruction, heap_location_collector_.GetArrayHeapLocation(instruction));
+  }
+
+  void VisitVecStore(HVecStore* instruction) override {
+    size_t idx = heap_location_collector_.GetArrayHeapLocation(instruction);
+    VisitSetLocation(instruction, idx, instruction->GetValue());
   }
 
   void VisitDeoptimize(HDeoptimize* instruction) override {
@@ -892,11 +936,6 @@ bool LoadStoreElimination::Run() {
     return false;
   }
 
-  // TODO: analyze VecLoad/VecStore better.
-  if (graph_->HasSIMD()) {
-    return false;
-  }
-
   LSEVisitor lse_visitor(graph_, heap_location_collector, side_effects_, stats_);
   for (HBasicBlock* block : graph_->GetReversePostOrder()) {
     lse_visitor.VisitBasicBlock(block);
diff --git a/compiler/optimizing/nodes.h b/compiler/optimizing/nodes.h
index fedad0c..cb53ae3 100644
--- a/compiler/optimizing/nodes.h
+++ b/compiler/optimizing/nodes.h
@@ -1438,6 +1438,7 @@ class HLoopInformationOutwardIterator : public ValueObject {
   M(Shr, BinaryOperation)                                               \
   M(StaticFieldGet, Instruction)                                        \
   M(StaticFieldSet, Instruction)                                        \
+  M(StringBuilderAppend, Instruction)                                   \
   M(UnresolvedInstanceFieldGet, Instruction)                            \
   M(UnresolvedInstanceFieldSet, Instruction)                            \
   M(UnresolvedStaticFieldGet, Instruction)                              \
@@ -4775,7 +4776,16 @@ class HInvokeVirtual final : public HInvoke {
       case Intrinsics::kThreadCurrentThread:
       case Intrinsics::kStringBufferAppend:
       case Intrinsics::kStringBufferToString:
-      case Intrinsics::kStringBuilderAppend:
+      case Intrinsics::kStringBuilderAppendObject:
+      case Intrinsics::kStringBuilderAppendString:
+      case Intrinsics::kStringBuilderAppendCharSequence:
+      case Intrinsics::kStringBuilderAppendCharArray:
+      case Intrinsics::kStringBuilderAppendBoolean:
+      case Intrinsics::kStringBuilderAppendChar:
+      case Intrinsics::kStringBuilderAppendInt:
+      case Intrinsics::kStringBuilderAppendLong:
+      case Intrinsics::kStringBuilderAppendFloat:
+      case Intrinsics::kStringBuilderAppendDouble:
       case Intrinsics::kStringBuilderToString:
         return false;
       default:
@@ -6880,6 +6890,55 @@ class HStaticFieldSet final : public HExpression<2> {
   const FieldInfo field_info_;
 };
 
+class HStringBuilderAppend final : public HVariableInputSizeInstruction {
+ public:
+  HStringBuilderAppend(HIntConstant* format,
+                       uint32_t number_of_arguments,
+                       ArenaAllocator* allocator,
+                       uint32_t dex_pc)
+      : HVariableInputSizeInstruction(
+            kStringBuilderAppend,
+            DataType::Type::kReference,
+            // The runtime call may read memory from inputs. It never writes outside
+            // of the newly allocated result object (or newly allocated helper objects).
+            SideEffects::AllReads().Union(SideEffects::CanTriggerGC()),
+            dex_pc,
+            allocator,
+            number_of_arguments + /* format */ 1u,
+            kArenaAllocInvokeInputs) {
+    DCHECK_GE(number_of_arguments, 1u);  // There must be something to append.
+    SetRawInputAt(FormatIndex(), format);
+  }
+
+  void SetArgumentAt(size_t index, HInstruction* argument) {
+    DCHECK_LE(index, GetNumberOfArguments());
+    SetRawInputAt(index, argument);
+  }
+
+  // Return the number of arguments, excluding the format.
+  size_t GetNumberOfArguments() const {
+    DCHECK_GE(InputCount(), 1u);
+    return InputCount() - 1u;
+  }
+
+  size_t FormatIndex() const {
+    return GetNumberOfArguments();
+  }
+
+  HIntConstant* GetFormat() {
+    return InputAt(FormatIndex())->AsIntConstant();
+  }
+
+  bool NeedsEnvironment() const override { return true; }
+
+  bool CanThrow() const override { return true; }
+
+  DECLARE_INSTRUCTION(StringBuilderAppend);
+
+ protected:
+  DEFAULT_COPY_CONSTRUCTOR(StringBuilderAppend);
+};
+
 class HUnresolvedInstanceFieldGet final : public HExpression<1> {
  public:
   HUnresolvedInstanceFieldGet(HInstruction* obj,
diff --git a/compiler/optimizing/nodes_vector.h b/compiler/optimizing/nodes_vector.h
index efe4d6b..e817048 100644
--- a/compiler/optimizing/nodes_vector.h
+++ b/compiler/optimizing/nodes_vector.h
@@ -1155,6 +1155,8 @@ class HVecStore final : public HVecMemoryOperation {
   // A store needs to stay in place.
   bool CanBeMoved() const override { return false; }
 
+  HInstruction* GetValue() const { return InputAt(2); }
+
   DECLARE_INSTRUCTION(VecStore);
 
  protected:
diff --git a/compiler/optimizing/register_allocation_resolver.cc b/compiler/optimizing/register_allocation_resolver.cc
index b1f0a1a..1786048 100644
--- a/compiler/optimizing/register_allocation_resolver.cc
+++ b/compiler/optimizing/register_allocation_resolver.cc
@@ -274,7 +274,7 @@ void RegisterAllocationResolver::UpdateSafepointLiveRegisters() {
 size_t RegisterAllocationResolver::CalculateMaximumSafepointSpillSize(
     ArrayRef<HInstruction* const> safepoints) {
   size_t core_register_spill_size = codegen_->GetWordSize();
-  size_t fp_register_spill_size = codegen_->GetFloatingPointSpillSlotSize();
+  size_t fp_register_spill_size = codegen_->GetSlowPathFPWidth();
   size_t maximum_safepoint_spill_size = 0u;
   for (HInstruction* instruction : safepoints) {
     LocationSummary* locations = instruction->GetLocations();
diff --git a/compiler/optimizing/scheduler.cc b/compiler/optimizing/scheduler.cc
index fdef45e..f722cf9 100644
--- a/compiler/optimizing/scheduler.cc
+++ b/compiler/optimizing/scheduler.cc
@@ -43,34 +43,37 @@ void SchedulingGraph::AddDependency(SchedulingNode* node,
   }
 
   if (is_data_dependency) {
-    if (!HasImmediateDataDependency(node, dependency)) {
-      node->AddDataPredecessor(dependency);
-    }
-  } else if (!HasImmediateOtherDependency(node, dependency)) {
+    node->AddDataPredecessor(dependency);
+  } else {
     node->AddOtherPredecessor(dependency);
   }
 }
 
-static bool MayHaveReorderingDependency(SideEffects node, SideEffects other) {
+bool SideEffectDependencyAnalysis::HasReorderingDependency(const HInstruction* instr1,
+                                                           const HInstruction* instr2) {
+  SideEffects instr1_side_effects = instr1->GetSideEffects();
+  SideEffects instr2_side_effects = instr2->GetSideEffects();
+
   // Read after write.
-  if (node.MayDependOn(other)) {
+  if (instr1_side_effects.MayDependOn(instr2_side_effects)) {
     return true;
   }
 
   // Write after read.
-  if (other.MayDependOn(node)) {
+  if (instr2_side_effects.MayDependOn(instr1_side_effects)) {
     return true;
   }
 
   // Memory write after write.
-  if (node.DoesAnyWrite() && other.DoesAnyWrite()) {
+  if (instr1_side_effects.DoesAnyWrite() && instr2_side_effects.DoesAnyWrite()) {
     return true;
   }
 
   return false;
 }
 
-size_t SchedulingGraph::ArrayAccessHeapLocation(HInstruction* instruction) const {
+size_t SideEffectDependencyAnalysis::MemoryDependencyAnalysis::ArrayAccessHeapLocation(
+    HInstruction* instruction) const {
   DCHECK(heap_location_collector_ != nullptr);
   size_t heap_loc = heap_location_collector_->GetArrayHeapLocation(instruction);
   // This array access should be analyzed and added to HeapLocationCollector before.
@@ -78,19 +81,19 @@ size_t SchedulingGraph::ArrayAccessHeapLocation(HInstruction* instruction) const
   return heap_loc;
 }
 
-bool SchedulingGraph::ArrayAccessMayAlias(HInstruction* node,
-                                          HInstruction* other) const {
+bool SideEffectDependencyAnalysis::MemoryDependencyAnalysis::ArrayAccessMayAlias(
+    HInstruction* instr1, HInstruction* instr2) const {
   DCHECK(heap_location_collector_ != nullptr);
-  size_t node_heap_loc = ArrayAccessHeapLocation(node);
-  size_t other_heap_loc = ArrayAccessHeapLocation(other);
+  size_t instr1_heap_loc = ArrayAccessHeapLocation(instr1);
+  size_t instr2_heap_loc = ArrayAccessHeapLocation(instr2);
 
   // For example: arr[0] and arr[0]
-  if (node_heap_loc == other_heap_loc) {
+  if (instr1_heap_loc == instr2_heap_loc) {
     return true;
   }
 
   // For example: arr[0] and arr[i]
-  if (heap_location_collector_->MayAlias(node_heap_loc, other_heap_loc)) {
+  if (heap_location_collector_->MayAlias(instr1_heap_loc, instr2_heap_loc)) {
     return true;
   }
 
@@ -148,55 +151,55 @@ static const FieldInfo* GetFieldInfo(const HInstruction* instruction) {
   }
 }
 
-size_t SchedulingGraph::FieldAccessHeapLocation(HInstruction* obj, const FieldInfo* field) const {
-  DCHECK(obj != nullptr);
-  DCHECK(field != nullptr);
+size_t SideEffectDependencyAnalysis::MemoryDependencyAnalysis::FieldAccessHeapLocation(
+    const HInstruction* instr) const {
+  DCHECK(instr != nullptr);
+  DCHECK(GetFieldInfo(instr) != nullptr);
   DCHECK(heap_location_collector_ != nullptr);
 
-  size_t heap_loc = heap_location_collector_->GetFieldHeapLocation(obj, field);
+  size_t heap_loc = heap_location_collector_->GetFieldHeapLocation(instr->InputAt(0),
+                                                                   GetFieldInfo(instr));
   // This field access should be analyzed and added to HeapLocationCollector before.
   DCHECK(heap_loc != HeapLocationCollector::kHeapLocationNotFound);
 
   return heap_loc;
 }
 
-bool SchedulingGraph::FieldAccessMayAlias(const HInstruction* node,
-                                          const HInstruction* other) const {
+bool SideEffectDependencyAnalysis::MemoryDependencyAnalysis::FieldAccessMayAlias(
+    const HInstruction* instr1, const HInstruction* instr2) const {
   DCHECK(heap_location_collector_ != nullptr);
 
   // Static and instance field accesses should not alias.
-  if ((IsInstanceFieldAccess(node) && IsStaticFieldAccess(other)) ||
-      (IsStaticFieldAccess(node) && IsInstanceFieldAccess(other))) {
+  if ((IsInstanceFieldAccess(instr1) && IsStaticFieldAccess(instr2)) ||
+      (IsStaticFieldAccess(instr1) && IsInstanceFieldAccess(instr2))) {
     return false;
   }
 
   // If either of the field accesses is unresolved.
-  if (IsUnresolvedFieldAccess(node) || IsUnresolvedFieldAccess(other)) {
+  if (IsUnresolvedFieldAccess(instr1) || IsUnresolvedFieldAccess(instr2)) {
     // Conservatively treat these two accesses may alias.
     return true;
   }
 
   // If both fields accesses are resolved.
-  const FieldInfo* node_field = GetFieldInfo(node);
-  const FieldInfo* other_field = GetFieldInfo(other);
-
-  size_t node_loc = FieldAccessHeapLocation(node->InputAt(0), node_field);
-  size_t other_loc = FieldAccessHeapLocation(other->InputAt(0), other_field);
+  size_t instr1_field_access_heap_loc = FieldAccessHeapLocation(instr1);
+  size_t instr2_field_access_heap_loc = FieldAccessHeapLocation(instr2);
 
-  if (node_loc == other_loc) {
+  if (instr1_field_access_heap_loc == instr2_field_access_heap_loc) {
     return true;
   }
 
-  if (!heap_location_collector_->MayAlias(node_loc, other_loc)) {
+  if (!heap_location_collector_->MayAlias(instr1_field_access_heap_loc,
+                                          instr2_field_access_heap_loc)) {
     return false;
   }
 
   return true;
 }
 
-bool SchedulingGraph::HasMemoryDependency(HInstruction* node,
-                                          HInstruction* other) const {
-  if (!MayHaveReorderingDependency(node->GetSideEffects(), other->GetSideEffects())) {
+bool SideEffectDependencyAnalysis::MemoryDependencyAnalysis::HasMemoryDependency(
+    HInstruction* instr1, HInstruction* instr2) const {
+  if (!HasReorderingDependency(instr1, instr2)) {
     return false;
   }
 
@@ -208,35 +211,35 @@ bool SchedulingGraph::HasMemoryDependency(HInstruction* node,
     return true;
   }
 
-  if (IsArrayAccess(node) && IsArrayAccess(other)) {
-    return ArrayAccessMayAlias(node, other);
+  if (IsArrayAccess(instr1) && IsArrayAccess(instr2)) {
+    return ArrayAccessMayAlias(instr1, instr2);
   }
-  if (IsFieldAccess(node) && IsFieldAccess(other)) {
-    return FieldAccessMayAlias(node, other);
+  if (IsFieldAccess(instr1) && IsFieldAccess(instr2)) {
+    return FieldAccessMayAlias(instr1, instr2);
   }
 
   // TODO(xueliang): LSA to support alias analysis among HVecLoad, HVecStore and ArrayAccess
-  if (node->IsVecMemoryOperation() && other->IsVecMemoryOperation()) {
+  if (instr1->IsVecMemoryOperation() && instr2->IsVecMemoryOperation()) {
     return true;
   }
-  if (node->IsVecMemoryOperation() && IsArrayAccess(other)) {
+  if (instr1->IsVecMemoryOperation() && IsArrayAccess(instr2)) {
     return true;
   }
-  if (IsArrayAccess(node) && other->IsVecMemoryOperation()) {
+  if (IsArrayAccess(instr1) && instr2->IsVecMemoryOperation()) {
     return true;
   }
 
   // Heap accesses of different kinds should not alias.
-  if (IsArrayAccess(node) && IsFieldAccess(other)) {
+  if (IsArrayAccess(instr1) && IsFieldAccess(instr2)) {
     return false;
   }
-  if (IsFieldAccess(node) && IsArrayAccess(other)) {
+  if (IsFieldAccess(instr1) && IsArrayAccess(instr2)) {
     return false;
   }
-  if (node->IsVecMemoryOperation() && IsFieldAccess(other)) {
+  if (instr1->IsVecMemoryOperation() && IsFieldAccess(instr2)) {
     return false;
   }
-  if (IsFieldAccess(node) && other->IsVecMemoryOperation()) {
+  if (IsFieldAccess(instr1) && instr2->IsVecMemoryOperation()) {
     return false;
   }
 
@@ -245,15 +248,15 @@ bool SchedulingGraph::HasMemoryDependency(HInstruction* node,
   return true;
 }
 
-bool SchedulingGraph::HasExceptionDependency(const HInstruction* node,
-                                             const HInstruction* other) const {
-  if (other->CanThrow() && node->GetSideEffects().DoesAnyWrite()) {
+bool SideEffectDependencyAnalysis::HasExceptionDependency(const HInstruction* instr1,
+                                                          const HInstruction* instr2) {
+  if (instr2->CanThrow() && instr1->GetSideEffects().DoesAnyWrite()) {
     return true;
   }
-  if (other->GetSideEffects().DoesAnyWrite() && node->CanThrow()) {
+  if (instr2->GetSideEffects().DoesAnyWrite() && instr1->CanThrow()) {
     return true;
   }
-  if (other->CanThrow() && node->CanThrow()) {
+  if (instr2->CanThrow() && instr1->CanThrow()) {
     return true;
   }
 
@@ -262,24 +265,6 @@ bool SchedulingGraph::HasExceptionDependency(const HInstruction* node,
   return false;
 }
 
-// Check whether `node` depends on `other`, taking into account `SideEffect`
-// information and `CanThrow` information.
-bool SchedulingGraph::HasSideEffectDependency(HInstruction* node,
-                                              HInstruction* other) const {
-  if (HasMemoryDependency(node, other)) {
-    return true;
-  }
-
-  // Even if above memory dependency check has passed, it is still necessary to
-  // check dependencies between instructions that can throw and instructions
-  // that write to memory.
-  if (HasExceptionDependency(node, other)) {
-    return true;
-  }
-
-  return false;
-}
-
 // Check if the specified instruction is a better candidate which more likely will
 // have other instructions depending on it.
 static bool IsBetterCandidateWithMoreLikelyDependencies(HInstruction* new_candidate,
@@ -297,8 +282,39 @@ static bool IsBetterCandidateWithMoreLikelyDependencies(HInstruction* new_candid
   }
 }
 
-void SchedulingGraph::AddDependencies(HInstruction* instruction, bool is_scheduling_barrier) {
-  SchedulingNode* instruction_node = GetNode(instruction);
+void SchedulingGraph::AddCrossIterationDependencies(SchedulingNode* node) {
+  for (HInstruction* instruction : node->GetInstruction()->GetInputs()) {
+    // Having a phi-function from a loop header as an input means the current node of the
+    // scheduling graph has a cross-iteration dependency because such phi-functions bring values
+    // from the previous iteration to the current iteration.
+    if (!instruction->IsLoopHeaderPhi()) {
+      continue;
+    }
+    for (HInstruction* phi_input : instruction->GetInputs()) {
+      // As a scheduling graph of the current basic block is built by
+      // processing instructions bottom-up, nullptr returned by GetNode means
+      // an instruction defining a value for the phi is either before the
+      // instruction represented by node or it is in a different basic block.
+      SchedulingNode* def_node = GetNode(phi_input);
+
+      // We don't create a dependency if there are uses besides the use in phi.
+      // In such cases a register to hold phi_input is usually allocated and
+      // a MOV instruction is generated. In cases with multiple uses and no MOV
+      // instruction, reordering creating a MOV instruction can improve
+      // performance more than an attempt to avoid a MOV instruction.
+      if (def_node != nullptr && def_node != node && phi_input->GetUses().HasExactlyOneElement()) {
+        // We have an implicit data dependency between node and def_node.
+        // AddAddDataDependency cannot be used because it is for explicit data dependencies.
+        // So AddOtherDependency is used.
+        AddOtherDependency(def_node, node);
+      }
+    }
+  }
+}
+
+void SchedulingGraph::AddDependencies(SchedulingNode* instruction_node,
+                                      bool is_scheduling_barrier) {
+  HInstruction* instruction = instruction_node->GetInstruction();
 
   // Define-use dependencies.
   for (const HUseListNode<HInstruction*>& use : instruction->GetUses()) {
@@ -354,12 +370,16 @@ void SchedulingGraph::AddDependencies(HInstruction* instruction, bool is_schedul
       if (other_node->IsSchedulingBarrier()) {
         // We have reached a scheduling barrier so we can stop further
         // processing.
-        DCHECK(HasImmediateOtherDependency(other_node, instruction_node));
+        //
+        // As a "other" dependency is not set up if a data dependency exists, we need to check that
+        // one of them must exist.
+        DCHECK(other_node->HasOtherDependency(instruction_node)
+               || other_node->HasDataDependency(instruction_node));
         break;
       }
-      if (HasSideEffectDependency(other, instruction)) {
+      if (side_effect_dependency_analysis_.HasSideEffectDependency(other, instruction)) {
         if (dep_chain_candidate != nullptr &&
-            HasSideEffectDependency(other, dep_chain_candidate)) {
+            side_effect_dependency_analysis_.HasSideEffectDependency(other, dep_chain_candidate)) {
           // Skip an explicit dependency to reduce memory usage, rely on the transitive dependency.
         } else {
           AddOtherDependency(other_node, instruction_node);
@@ -386,44 +406,8 @@ void SchedulingGraph::AddDependencies(HInstruction* instruction, bool is_schedul
       AddOtherDependency(GetNode(use.GetUser()->GetHolder()), instruction_node);
     }
   }
-}
-
-bool SchedulingGraph::HasImmediateDataDependency(const SchedulingNode* node,
-                                                 const SchedulingNode* other) const {
-  return ContainsElement(node->GetDataPredecessors(), other);
-}
 
-bool SchedulingGraph::HasImmediateDataDependency(const HInstruction* instruction,
-                                                 const HInstruction* other_instruction) const {
-  const SchedulingNode* node = GetNode(instruction);
-  const SchedulingNode* other = GetNode(other_instruction);
-  if (node == nullptr || other == nullptr) {
-    // Both instructions must be in current basic block, i.e. the SchedulingGraph can see their
-    // corresponding SchedulingNode in the graph, and tell whether there is a dependency.
-    // Otherwise there is no dependency from SchedulingGraph's perspective, for example,
-    // instruction and other_instruction are in different basic blocks.
-    return false;
-  }
-  return HasImmediateDataDependency(node, other);
-}
-
-bool SchedulingGraph::HasImmediateOtherDependency(const SchedulingNode* node,
-                                                  const SchedulingNode* other) const {
-  return ContainsElement(node->GetOtherPredecessors(), other);
-}
-
-bool SchedulingGraph::HasImmediateOtherDependency(const HInstruction* instruction,
-                                                  const HInstruction* other_instruction) const {
-  const SchedulingNode* node = GetNode(instruction);
-  const SchedulingNode* other = GetNode(other_instruction);
-  if (node == nullptr || other == nullptr) {
-    // Both instructions must be in current basic block, i.e. the SchedulingGraph can see their
-    // corresponding SchedulingNode in the graph, and tell whether there is a dependency.
-    // Otherwise there is no dependency from SchedulingGraph's perspective, for example,
-    // instruction and other_instruction are in different basic blocks.
-    return false;
-  }
-  return HasImmediateOtherDependency(node, other);
+  AddCrossIterationDependencies(instruction_node);
 }
 
 static const std::string InstructionTypeId(const HInstruction* instruction) {
@@ -594,7 +578,7 @@ void HScheduler::Schedule(HBasicBlock* block,
   ScopedArenaVector<SchedulingNode*> scheduling_nodes(allocator.Adapter(kArenaAllocScheduler));
 
   // Build the scheduling graph.
-  SchedulingGraph scheduling_graph(this, &allocator, heap_location_collector);
+  SchedulingGraph scheduling_graph(&allocator, heap_location_collector);
   for (HBackwardInstructionIterator it(block->GetInstructions()); !it.Done(); it.Advance()) {
     HInstruction* instruction = it.Current();
     CHECK_EQ(instruction->GetBlock(), block)
diff --git a/compiler/optimizing/scheduler.h b/compiler/optimizing/scheduler.h
index d2dbeca..f7180a0 100644
--- a/compiler/optimizing/scheduler.h
+++ b/compiler/optimizing/scheduler.h
@@ -21,6 +21,7 @@
 
 #include "base/scoped_arena_allocator.h"
 #include "base/scoped_arena_containers.h"
+#include "base/stl_util.h"
 #include "base/time_utils.h"
 #include "code_generator.h"
 #include "load_store_analysis.h"
@@ -168,6 +169,10 @@ class SchedulingNode : public DeletableArenaObject<kArenaAllocScheduler> {
   }
 
   void AddDataPredecessor(SchedulingNode* predecessor) {
+    // Check whether the predecessor has been added earlier.
+    if (HasDataDependency(predecessor)) {
+      return;
+    }
     data_predecessors_.push_back(predecessor);
     predecessor->num_unscheduled_successors_++;
   }
@@ -177,6 +182,12 @@ class SchedulingNode : public DeletableArenaObject<kArenaAllocScheduler> {
   }
 
   void AddOtherPredecessor(SchedulingNode* predecessor) {
+    // Check whether the predecessor has been added earlier.
+    // As an optimization of the scheduling graph, we don't need to create another dependency if
+    // there is a data dependency between scheduling nodes.
+    if (HasOtherDependency(predecessor) || HasDataDependency(predecessor)) {
+      return;
+    }
     other_predecessors_.push_back(predecessor);
     predecessor->num_unscheduled_successors_++;
   }
@@ -205,6 +216,14 @@ class SchedulingNode : public DeletableArenaObject<kArenaAllocScheduler> {
   uint32_t GetCriticalPath() const { return critical_path_; }
   bool IsSchedulingBarrier() const { return is_scheduling_barrier_; }
 
+  bool HasDataDependency(const SchedulingNode* node) const {
+    return ContainsElement(data_predecessors_, node);
+  }
+
+  bool HasOtherDependency(const SchedulingNode* node) const {
+    return ContainsElement(other_predecessors_, node);
+  }
+
  private:
   // The latency of this node. It represents the latency between the moment the
   // last instruction for this node has executed to the moment the result
@@ -245,19 +264,68 @@ class SchedulingNode : public DeletableArenaObject<kArenaAllocScheduler> {
   static constexpr size_t kPreallocatedPredecessors = 4;
 };
 
+/*
+ * Provide analysis of instruction dependencies (side effects) which are not in a form of explicit
+ * def-use data dependencies.
+ */
+class SideEffectDependencyAnalysis {
+ public:
+  explicit SideEffectDependencyAnalysis(const HeapLocationCollector* heap_location_collector)
+      : memory_dependency_analysis_(heap_location_collector) {}
+
+  bool HasSideEffectDependency(HInstruction* instr1, HInstruction* instr2) const {
+    if (memory_dependency_analysis_.HasMemoryDependency(instr1, instr2)) {
+      return true;
+    }
+
+    // Even if above memory dependency check has passed, it is still necessary to
+    // check dependencies between instructions that can throw and instructions
+    // that write to memory.
+    if (HasExceptionDependency(instr1, instr2)) {
+      return true;
+    }
+
+    return false;
+  }
+
+ private:
+  static bool HasExceptionDependency(const HInstruction* instr1, const HInstruction* instr2);
+  static bool HasReorderingDependency(const HInstruction* instr1, const HInstruction* instr2);
+
+  /*
+   * Memory dependency analysis of instructions based on their memory side effects
+   * and heap location information from the LCA pass if it is provided.
+   */
+  class MemoryDependencyAnalysis {
+   public:
+    explicit MemoryDependencyAnalysis(const HeapLocationCollector* heap_location_collector)
+        : heap_location_collector_(heap_location_collector) {}
+
+    bool HasMemoryDependency(HInstruction* instr1, HInstruction* instr2) const;
+
+   private:
+    bool ArrayAccessMayAlias(HInstruction* instr1, HInstruction* instr2) const;
+    bool FieldAccessMayAlias(const HInstruction* instr1, const HInstruction* instr2) const;
+    size_t ArrayAccessHeapLocation(HInstruction* instruction) const;
+    size_t FieldAccessHeapLocation(const HInstruction* instruction) const;
+
+    const HeapLocationCollector* const heap_location_collector_;
+  };
+
+  MemoryDependencyAnalysis memory_dependency_analysis_;
+};
+
 /*
  * Directed acyclic graph for scheduling.
  */
 class SchedulingGraph : public ValueObject {
  public:
-  SchedulingGraph(const HScheduler* scheduler,
-                  ScopedArenaAllocator* allocator,
+  SchedulingGraph(ScopedArenaAllocator* allocator,
                   const HeapLocationCollector* heap_location_collector)
-      : scheduler_(scheduler),
-        allocator_(allocator),
+      : allocator_(allocator),
         contains_scheduling_barrier_(false),
         nodes_map_(allocator_->Adapter(kArenaAllocScheduler)),
-        heap_location_collector_(heap_location_collector) {}
+        side_effect_dependency_analysis_(heap_location_collector) {}
 
   SchedulingNode* AddNode(HInstruction* instr, bool is_scheduling_barrier = false) {
     std::unique_ptr<SchedulingNode> node(
@@ -265,7 +333,7 @@ class SchedulingGraph : public ValueObject {
     SchedulingNode* result = node.get();
     nodes_map_.insert(std::make_pair(instr, std::move(node)));
     contains_scheduling_barrier_ |= is_scheduling_barrier;
-    AddDependencies(instr, is_scheduling_barrier);
+    AddDependencies(result, is_scheduling_barrier);
     return result;
   }
 
@@ -278,13 +346,6 @@ class SchedulingGraph : public ValueObject {
     }
   }
 
-  bool IsSchedulingBarrier(const HInstruction* instruction) const;
-
-  bool HasImmediateDataDependency(const SchedulingNode* node, const SchedulingNode* other) const;
-  bool HasImmediateDataDependency(const HInstruction* node, const HInstruction* other) const;
-  bool HasImmediateOtherDependency(const SchedulingNode* node, const SchedulingNode* other) const;
-  bool HasImmediateOtherDependency(const HInstruction* node, const HInstruction* other) const;
-
   size_t Size() const {
     return nodes_map_.size();
   }
@@ -302,26 +363,33 @@ class SchedulingGraph : public ValueObject {
   void AddOtherDependency(SchedulingNode* node, SchedulingNode* dependency) {
     AddDependency(node, dependency, /*is_data_dependency*/false);
   }
-  bool HasMemoryDependency(HInstruction* node, HInstruction* other) const;
-  bool HasExceptionDependency(const HInstruction* node, const HInstruction* other) const;
-  bool HasSideEffectDependency(HInstruction* node, HInstruction* other) const;
-  bool ArrayAccessMayAlias(HInstruction* node, HInstruction* other) const;
-  bool FieldAccessMayAlias(const HInstruction* node, const HInstruction* other) const;
-  size_t ArrayAccessHeapLocation(HInstruction* instruction) const;
-  size_t FieldAccessHeapLocation(HInstruction* obj, const FieldInfo* field) const;
 
-  // Add dependencies nodes for the given `HInstruction`: inputs, environments, and side-effects.
-  void AddDependencies(HInstruction* instruction, bool is_scheduling_barrier = false);
+  // Analyze whether the scheduling node has cross-iteration dependencies which mean it uses
+  // values defined on the previous iteration.
+  //
+  // Supported cases:
+  //
+  //   L:
+  //     v2 = loop_head_phi(v1)
+  //     instr1(v2)
+  //     v1 = instr2
+  //     goto L
+  //
+  // In such cases moving instr2 before instr1 creates intersecting live ranges
+  // of v1 and v2. As a result a separate register is needed to keep the value
+  // defined by instr2 which is only used on the next iteration.
+  // If instr2 is not moved, no additional register is needed. The register
+  // used by instr1 is reused.
+  // To prevent such a situation a "other" dependency between instr1 and instr2 must be set.
+  void AddCrossIterationDependencies(SchedulingNode* node);
 
-  const HScheduler* const scheduler_;
+  // Add dependencies nodes for the given `SchedulingNode`: inputs, environments, and side-effects.
+  void AddDependencies(SchedulingNode* node, bool is_scheduling_barrier = false);
 
   ScopedArenaAllocator* const allocator_;
-
   bool contains_scheduling_barrier_;
-
   ScopedArenaHashMap<const HInstruction*, std::unique_ptr<SchedulingNode>> nodes_map_;
-
-  const HeapLocationCollector* const heap_location_collector_;
+  SideEffectDependencyAnalysis side_effect_dependency_analysis_;
 };
 
 /*
@@ -477,10 +545,6 @@ class HScheduler {
   DISALLOW_COPY_AND_ASSIGN(HScheduler);
 };
 
-inline bool SchedulingGraph::IsSchedulingBarrier(const HInstruction* instruction) const {
-  return scheduler_->IsSchedulingBarrier(instruction);
-}
-
 class HInstructionScheduling : public HOptimization {
  public:
   HInstructionScheduling(HGraph* graph,
diff --git a/compiler/optimizing/scheduler_test.cc b/compiler/optimizing/scheduler_test.cc
index e0e265a..4c47f2e 100644
--- a/compiler/optimizing/scheduler_test.cc
+++ b/compiler/optimizing/scheduler_test.cc
@@ -146,9 +146,7 @@ class SchedulerTest : public OptimizingUnitTest {
     environment->SetRawEnvAt(1, mul);
     mul->AddEnvUseAt(div_check->GetEnvironment(), 1);
 
-    SchedulingGraph scheduling_graph(scheduler,
-                                     GetScopedAllocator(),
-                                     /* heap_location_collector= */ nullptr);
+    TestSchedulingGraph scheduling_graph(GetScopedAllocator());
     // Instructions must be inserted in reverse order into the scheduling graph.
     for (HInstruction* instr : ReverseRange(block_instructions)) {
       scheduling_graph.AddNode(instr);
@@ -283,7 +281,7 @@ class SchedulerTest : public OptimizingUnitTest {
     HeapLocationCollector heap_location_collector(graph_);
     heap_location_collector.VisitBasicBlock(entry);
     heap_location_collector.BuildAliasingMatrix();
-    SchedulingGraph scheduling_graph(scheduler, GetScopedAllocator(), &heap_location_collector);
+    TestSchedulingGraph scheduling_graph(GetScopedAllocator(), &heap_location_collector);
 
     for (HInstruction* instr : ReverseRange(block_instructions)) {
       // Build scheduling graph with memory access aliasing information
@@ -357,6 +355,41 @@ class SchedulerTest : public OptimizingUnitTest {
     scheduler->Schedule(graph_);
   }
 
+  class TestSchedulingGraph : public SchedulingGraph {
+   public:
+    explicit TestSchedulingGraph(ScopedArenaAllocator* allocator,
+                                 const HeapLocationCollector *heap_location_collector = nullptr)
+        : SchedulingGraph(allocator, heap_location_collector) {}
+
+    bool HasImmediateDataDependency(const HInstruction* instruction,
+                                    const HInstruction* other_instruction) const {
+      const SchedulingNode* node = GetNode(instruction);
+      const SchedulingNode* other = GetNode(other_instruction);
+      if (node == nullptr || other == nullptr) {
+        // Both instructions must be in current basic block, i.e. the SchedulingGraph can see their
+        // corresponding SchedulingNode in the graph, and tell whether there is a dependency.
+        // Otherwise there is no dependency from SchedulingGraph's perspective, for example,
+        // instruction and other_instruction are in different basic blocks.
+        return false;
+      }
+      return node->HasDataDependency(other);
+    }
+
+    bool HasImmediateOtherDependency(const HInstruction* instruction,
+                                     const HInstruction* other_instruction) const {
+      const SchedulingNode* node = GetNode(instruction);
+      const SchedulingNode* other = GetNode(other_instruction);
+      if (node == nullptr || other == nullptr) {
+        // Both instructions must be in current basic block, i.e. the SchedulingGraph can see their
+        // corresponding SchedulingNode in the graph, and tell whether there is a dependency.
+        // Otherwise there is no dependency from SchedulingGraph's perspective, for example,
+        // instruction and other_instruction are in different basic blocks.
+        return false;
+      }
+      return node->HasOtherDependency(other);
+    }
+  };
+
   HGraph* graph_;
 };
 
diff --git a/compiler/optimizing/stack_map_stream.cc b/compiler/optimizing/stack_map_stream.cc
index 60ca61c..e21e21c 100644
--- a/compiler/optimizing/stack_map_stream.cc
+++ b/compiler/optimizing/stack_map_stream.cc
@@ -52,6 +52,15 @@ void StackMapStream::BeginMethod(size_t frame_size_in_bytes,
   core_spill_mask_ = core_spill_mask;
   fp_spill_mask_ = fp_spill_mask;
   num_dex_registers_ = num_dex_registers;
+
+  if (kVerifyStackMaps) {
+    dchecks_.emplace_back([=](const CodeInfo& code_info) {
+      DCHECK_EQ(code_info.packed_frame_size_, frame_size_in_bytes / kStackAlignment);
+      DCHECK_EQ(code_info.core_spill_mask_, core_spill_mask);
+      DCHECK_EQ(code_info.fp_spill_mask_, fp_spill_mask);
+      DCHECK_EQ(code_info.number_of_dex_registers_, num_dex_registers);
+    });
+  }
 }
 
 void StackMapStream::EndMethod() {
@@ -175,6 +184,7 @@ void StackMapStream::BeginInlineInfoEntry(ArtMethod* method,
   in_inline_info_ = true;
   DCHECK_EQ(expected_num_dex_registers_, current_dex_registers_.size());
 
+  flags_ |= CodeInfo::kHasInlineInfo;
   expected_num_dex_registers_ += num_dex_registers;
 
   BitTableBuilder<InlineInfo>::Entry entry;
@@ -296,6 +306,7 @@ ScopedArenaVector<uint8_t> StackMapStream::Encode() {
 
   ScopedArenaVector<uint8_t> buffer(allocator_->Adapter(kArenaAllocStackMapStream));
   BitMemoryWriter<ScopedArenaVector<uint8_t>> out(&buffer);
+  out.WriteVarint(flags_);
   out.WriteVarint(packed_frame_size_);
   out.WriteVarint(core_spill_mask_);
   out.WriteVarint(fp_spill_mask_);
diff --git a/compiler/optimizing/stack_map_stream.h b/compiler/optimizing/stack_map_stream.h
index 01c6bf9..20dd32e 100644
--- a/compiler/optimizing/stack_map_stream.h
+++ b/compiler/optimizing/stack_map_stream.h
@@ -99,6 +99,7 @@ class StackMapStream : public DeletableArenaObject<kArenaAllocStackMapStream> {
 
   ScopedArenaAllocator* allocator_;
   const InstructionSet instruction_set_;
+  uint32_t flags_ = 0;
   uint32_t packed_frame_size_ = 0;
   uint32_t core_spill_mask_ = 0;
   uint32_t fp_spill_mask_ = 0;
diff --git a/compiler/utils/arm/assembler_arm_vixl.h b/compiler/utils/arm/assembler_arm_vixl.h
index 98c0191..59d7edd 100644
--- a/compiler/utils/arm/assembler_arm_vixl.h
+++ b/compiler/utils/arm/assembler_arm_vixl.h
@@ -22,6 +22,7 @@
 #include "base/arena_containers.h"
 #include "base/macros.h"
 #include "constants_arm.h"
+#include "dwarf/register.h"
 #include "offsets.h"
 #include "utils/arm/assembler_arm_shared.h"
 #include "utils/arm/managed_register_arm.h"
@@ -39,6 +40,14 @@ namespace vixl32 = vixl::aarch32;
 namespace art {
 namespace arm {
 
+inline dwarf::Reg DWARFReg(vixl32::Register reg) {
+  return dwarf::Reg::ArmCore(static_cast<int>(reg.GetCode()));
+}
+
+inline dwarf::Reg DWARFReg(vixl32::SRegister reg) {
+  return dwarf::Reg::ArmFp(static_cast<int>(reg.GetCode()));
+}
+
 class ArmVIXLMacroAssembler final : public vixl32::MacroAssembler {
  public:
   // Most methods fit in a 1KB code buffer, which results in more optimal alloc/realloc and
diff --git a/compiler/utils/arm/jni_macro_assembler_arm_vixl.cc b/compiler/utils/arm/jni_macro_assembler_arm_vixl.cc
index c6c764e..47a067b 100644
--- a/compiler/utils/arm/jni_macro_assembler_arm_vixl.cc
+++ b/compiler/utils/arm/jni_macro_assembler_arm_vixl.cc
@@ -68,14 +68,6 @@ void ArmVIXLJNIMacroAssembler::FinalizeCode() {
   asm_.FinalizeCode();
 }
 
-static dwarf::Reg DWARFReg(vixl32::Register reg) {
-  return dwarf::Reg::ArmCore(static_cast<int>(reg.GetCode()));
-}
-
-static dwarf::Reg DWARFReg(vixl32::SRegister reg) {
-  return dwarf::Reg::ArmFp(static_cast<int>(reg.GetCode()));
-}
-
 static constexpr size_t kFramePointerSize = static_cast<size_t>(kArmPointerSize);
 
 void ArmVIXLJNIMacroAssembler::BuildFrame(size_t frame_size,
diff --git a/compiler/utils/arm64/assembler_arm64.cc b/compiler/utils/arm64/assembler_arm64.cc
index d7ade05..d722e00 100644
--- a/compiler/utils/arm64/assembler_arm64.cc
+++ b/compiler/utils/arm64/assembler_arm64.cc
@@ -49,6 +49,7 @@ static void SetVIXLCPUFeaturesFromART(vixl::aarch64::MacroAssembler* vixl_masm_,
   }
   if (art_features->HasFP16()) {
     features->Combine(vixl::CPUFeatures::kFPHalf);
+    features->Combine(vixl::CPUFeatures::kNEONHalf);
   }
   if (art_features->HasLSE()) {
     features->Combine(vixl::CPUFeatures::kAtomics);
@@ -103,15 +104,6 @@ void Arm64Assembler::JumpTo(ManagedRegister m_base, Offset offs, ManagedRegister
   ___ Br(reg_x(scratch.AsXRegister()));
 }
 
-static inline dwarf::Reg DWARFReg(CPURegister reg) {
-  if (reg.IsFPRegister()) {
-    return dwarf::Reg::Arm64Fp(reg.GetCode());
-  } else {
-    DCHECK_LT(reg.GetCode(), 31u);  // X0 - X30.
-    return dwarf::Reg::Arm64Core(reg.GetCode());
-  }
-}
-
 void Arm64Assembler::SpillRegisters(CPURegList registers, int offset) {
   int size = registers.GetRegisterSizeInBytes();
   const Register sp = vixl_masm_.StackPointer();
diff --git a/compiler/utils/arm64/assembler_arm64.h b/compiler/utils/arm64/assembler_arm64.h
index 9e01a70..594c6b4 100644
--- a/compiler/utils/arm64/assembler_arm64.h
+++ b/compiler/utils/arm64/assembler_arm64.h
@@ -25,6 +25,7 @@
 
 #include "base/arena_containers.h"
 #include "base/macros.h"
+#include "dwarf/register.h"
 #include "offsets.h"
 #include "utils/arm64/managed_register_arm64.h"
 #include "utils/assembler.h"
@@ -42,6 +43,15 @@ class Arm64InstructionSetFeatures;
 
 namespace arm64 {
 
+static inline dwarf::Reg DWARFReg(vixl::aarch64::CPURegister reg) {
+  if (reg.IsFPRegister()) {
+    return dwarf::Reg::Arm64Fp(reg.GetCode());
+  } else {
+    DCHECK_LT(reg.GetCode(), 31u);  // X0 - X30.
+    return dwarf::Reg::Arm64Core(reg.GetCode());
+  }
+}
+
 #define MEM_OP(...)      vixl::aarch64::MemOperand(__VA_ARGS__)
 
 enum LoadOperandType {
diff --git a/dex2oat/linker/arm/relative_patcher_arm_base.cc b/dex2oat/linker/arm/relative_patcher_arm_base.cc
index 828dc5d..35e799a 100644
--- a/dex2oat/linker/arm/relative_patcher_arm_base.cc
+++ b/dex2oat/linker/arm/relative_patcher_arm_base.cc
@@ -386,6 +386,12 @@ ArmBaseRelativePatcher::ThunkKey ArmBaseRelativePatcher::GetMethodCallKey() {
   return ThunkKey(ThunkType::kMethodCall);
 }
 
+ArmBaseRelativePatcher::ThunkKey ArmBaseRelativePatcher::GetEntrypointCallKey(
+    const LinkerPatch& patch) {
+  DCHECK_EQ(patch.GetType(), LinkerPatch::Type::kCallEntrypoint);
+  return ThunkKey(ThunkType::kEntrypointCall, patch.EntrypointOffset());
+}
+
 ArmBaseRelativePatcher::ThunkKey ArmBaseRelativePatcher::GetBakerThunkKey(
     const LinkerPatch& patch) {
   DCHECK_EQ(patch.GetType(), LinkerPatch::Type::kBakerReadBarrierBranch);
@@ -399,6 +405,7 @@ void ArmBaseRelativePatcher::ProcessPatches(const CompiledMethod* compiled_metho
   for (const LinkerPatch& patch : compiled_method->GetPatches()) {
     uint32_t patch_offset = code_offset + patch.LiteralOffset();
     ThunkKey key(static_cast<ThunkType>(-1));
+    bool simple_thunk_patch = false;
     ThunkData* old_data = nullptr;
     if (patch.GetType() == LinkerPatch::Type::kCallRelative) {
       key = GetMethodCallKey();
@@ -411,8 +418,14 @@ void ArmBaseRelativePatcher::ProcessPatches(const CompiledMethod* compiled_metho
       } else {
         old_data = method_call_thunk_;
       }
+    } else if (patch.GetType() == LinkerPatch::Type::kCallEntrypoint) {
+      key = GetEntrypointCallKey(patch);
+      simple_thunk_patch = true;
     } else if (patch.GetType() == LinkerPatch::Type::kBakerReadBarrierBranch) {
       key = GetBakerThunkKey(patch);
+      simple_thunk_patch = true;
+    }
+    if (simple_thunk_patch) {
       auto lb = thunks_.lower_bound(key);
       if (lb == thunks_.end() || thunks_.key_comp()(key, lb->first)) {
         uint32_t max_next_offset = CalculateMaxNextOffset(patch_offset, key);
diff --git a/dex2oat/linker/arm/relative_patcher_arm_base.h b/dex2oat/linker/arm/relative_patcher_arm_base.h
index 0eb4417..bf3e81f 100644
--- a/dex2oat/linker/arm/relative_patcher_arm_base.h
+++ b/dex2oat/linker/arm/relative_patcher_arm_base.h
@@ -44,6 +44,7 @@ class ArmBaseRelativePatcher : public RelativePatcher {
 
   enum class ThunkType {
     kMethodCall,              // Method call thunk.
+    kEntrypointCall,          // Entrypoint call.
     kBakerReadBarrier,        // Baker read barrier.
   };
 
@@ -84,6 +85,7 @@ class ArmBaseRelativePatcher : public RelativePatcher {
   };
 
   static ThunkKey GetMethodCallKey();
+  static ThunkKey GetEntrypointCallKey(const LinkerPatch& patch);
   static ThunkKey GetBakerThunkKey(const LinkerPatch& patch);
 
   uint32_t ReserveSpaceInternal(uint32_t offset,
diff --git a/dex2oat/linker/arm/relative_patcher_thumb2.cc b/dex2oat/linker/arm/relative_patcher_thumb2.cc
index 697fb09..72b93ec 100644
--- a/dex2oat/linker/arm/relative_patcher_thumb2.cc
+++ b/dex2oat/linker/arm/relative_patcher_thumb2.cc
@@ -58,28 +58,10 @@ void Thumb2RelativePatcher::PatchCall(std::vector<uint8_t>* code,
                                       uint32_t literal_offset,
                                       uint32_t patch_offset,
                                       uint32_t target_offset) {
-  DCHECK_LE(literal_offset + 4u, code->size());
-  DCHECK_EQ(literal_offset & 1u, 0u);
-  DCHECK_EQ(patch_offset & 1u, 0u);
+  DCHECK_ALIGNED(patch_offset, 2u);
   DCHECK_EQ(target_offset & 1u, 1u);  // Thumb2 mode bit.
   uint32_t displacement = CalculateMethodCallDisplacement(patch_offset, target_offset & ~1u);
-  displacement -= kPcDisplacement;  // The base PC is at the end of the 4-byte patch.
-  DCHECK_EQ(displacement & 1u, 0u);
-  DCHECK((displacement >> 24) == 0u || (displacement >> 24) == 255u);  // 25-bit signed.
-  uint32_t signbit = (displacement >> 31) & 0x1;
-  uint32_t i1 = (displacement >> 23) & 0x1;
-  uint32_t i2 = (displacement >> 22) & 0x1;
-  uint32_t imm10 = (displacement >> 12) & 0x03ff;
-  uint32_t imm11 = (displacement >> 1) & 0x07ff;
-  uint32_t j1 = i1 ^ (signbit ^ 1);
-  uint32_t j2 = i2 ^ (signbit ^ 1);
-  uint32_t value = (signbit << 26) | (j1 << 13) | (j2 << 11) | (imm10 << 16) | imm11;
-  value |= 0xf000d000;  // BL
-
-  // Check that we're just overwriting an existing BL.
-  DCHECK_EQ(GetInsn32(code, literal_offset) & 0xf800d000, 0xf000d000);
-  // Write the new BL.
-  SetInsn32(code, literal_offset, value);
+  PatchBl(code, literal_offset, displacement);
 }
 
 void Thumb2RelativePatcher::PatchPcRelativeReference(std::vector<uint8_t>* code,
@@ -102,6 +84,17 @@ void Thumb2RelativePatcher::PatchPcRelativeReference(std::vector<uint8_t>* code,
   SetInsn32(code, literal_offset, insn);
 }
 
+void Thumb2RelativePatcher::PatchEntrypointCall(std::vector<uint8_t>* code,
+                                                const LinkerPatch& patch,
+                                                uint32_t patch_offset) {
+  DCHECK_ALIGNED(patch_offset, 2u);
+  ThunkKey key = GetEntrypointCallKey(patch);
+  uint32_t target_offset = GetThunkTargetOffset(key, patch_offset);
+  DCHECK_ALIGNED(target_offset, 4u);
+  uint32_t displacement = target_offset - patch_offset;
+  PatchBl(code, patch.LiteralOffset(), displacement);
+}
+
 void Thumb2RelativePatcher::PatchBakerReadBarrierBranch(std::vector<uint8_t>* code,
                                                         const LinkerPatch& patch,
                                                         uint32_t patch_offset) {
@@ -127,6 +120,7 @@ void Thumb2RelativePatcher::PatchBakerReadBarrierBranch(std::vector<uint8_t>* co
 uint32_t Thumb2RelativePatcher::MaxPositiveDisplacement(const ThunkKey& key) {
   switch (key.GetType()) {
     case ThunkType::kMethodCall:
+    case ThunkType::kEntrypointCall:
       return kMaxMethodCallPositiveDisplacement;
     case ThunkType::kBakerReadBarrier:
       return kMaxBcondPositiveDisplacement;
@@ -136,12 +130,35 @@ uint32_t Thumb2RelativePatcher::MaxPositiveDisplacement(const ThunkKey& key) {
 uint32_t Thumb2RelativePatcher::MaxNegativeDisplacement(const ThunkKey& key) {
   switch (key.GetType()) {
     case ThunkType::kMethodCall:
+    case ThunkType::kEntrypointCall:
       return kMaxMethodCallNegativeDisplacement;
     case ThunkType::kBakerReadBarrier:
       return kMaxBcondNegativeDisplacement;
   }
 }
 
+void Thumb2RelativePatcher::PatchBl(std::vector<uint8_t>* code,
+                                    uint32_t literal_offset,
+                                    uint32_t displacement) {
+  displacement -= kPcDisplacement;  // The base PC is at the end of the 4-byte patch.
+  DCHECK_EQ(displacement & 1u, 0u);
+  DCHECK((displacement >> 24) == 0u || (displacement >> 24) == 255u);  // 25-bit signed.
+  uint32_t signbit = (displacement >> 31) & 0x1;
+  uint32_t i1 = (displacement >> 23) & 0x1;
+  uint32_t i2 = (displacement >> 22) & 0x1;
+  uint32_t imm10 = (displacement >> 12) & 0x03ff;
+  uint32_t imm11 = (displacement >> 1) & 0x07ff;
+  uint32_t j1 = i1 ^ (signbit ^ 1);
+  uint32_t j2 = i2 ^ (signbit ^ 1);
+  uint32_t value = (signbit << 26) | (j1 << 13) | (j2 << 11) | (imm10 << 16) | imm11;
+  value |= 0xf000d000;  // BL
+
+  // Check that we're just overwriting an existing BL.
+  DCHECK_EQ(GetInsn32(code, literal_offset) & 0xf800d000, 0xf000d000);
+  // Write the new BL.
+  SetInsn32(code, literal_offset, value);
+}
+
 void Thumb2RelativePatcher::SetInsn32(std::vector<uint8_t>* code, uint32_t offset, uint32_t value) {
   DCHECK_LE(offset + 4u, code->size());
   DCHECK_ALIGNED(offset, 2u);
diff --git a/dex2oat/linker/arm/relative_patcher_thumb2.h b/dex2oat/linker/arm/relative_patcher_thumb2.h
index dbf64a1..d360482 100644
--- a/dex2oat/linker/arm/relative_patcher_thumb2.h
+++ b/dex2oat/linker/arm/relative_patcher_thumb2.h
@@ -42,6 +42,9 @@ class Thumb2RelativePatcher final : public ArmBaseRelativePatcher {
                                 const LinkerPatch& patch,
                                 uint32_t patch_offset,
                                 uint32_t target_offset) override;
+  void PatchEntrypointCall(std::vector<uint8_t>* code,
+                           const LinkerPatch& patch,
+                           uint32_t patch_offset) override;
   void PatchBakerReadBarrierBranch(std::vector<uint8_t>* code,
                                    const LinkerPatch& patch,
                                    uint32_t patch_offset) override;
@@ -51,7 +54,9 @@ class Thumb2RelativePatcher final : public ArmBaseRelativePatcher {
   uint32_t MaxNegativeDisplacement(const ThunkKey& key) override;
 
  private:
-  void SetInsn32(std::vector<uint8_t>* code, uint32_t offset, uint32_t value);
+  static void PatchBl(std::vector<uint8_t>* code, uint32_t literal_offset, uint32_t displacement);
+
+  static void SetInsn32(std::vector<uint8_t>* code, uint32_t offset, uint32_t value);
   static uint32_t GetInsn32(ArrayRef<const uint8_t> code, uint32_t offset);
 
   template <typename Vector>
diff --git a/dex2oat/linker/arm/relative_patcher_thumb2_test.cc b/dex2oat/linker/arm/relative_patcher_thumb2_test.cc
index 04a897e..296bf61 100644
--- a/dex2oat/linker/arm/relative_patcher_thumb2_test.cc
+++ b/dex2oat/linker/arm/relative_patcher_thumb2_test.cc
@@ -225,7 +225,8 @@ class Thumb2RelativePatcherTest : public RelativePatcherTest {
 
     // Make sure the ThunkProvider has all the necessary thunks.
     for (const LinkerPatch& patch : patches) {
-      if (patch.GetType() == LinkerPatch::Type::kBakerReadBarrierBranch ||
+      if (patch.GetType() == LinkerPatch::Type::kCallEntrypoint ||
+          patch.GetType() == LinkerPatch::Type::kBakerReadBarrierBranch ||
           patch.GetType() == LinkerPatch::Type::kCallRelative) {
         std::string debug_name;
         std::vector<uint8_t> thunk_code = CompileThunk(patch, &debug_name);
@@ -662,6 +663,35 @@ TEST_F(Thumb2RelativePatcherTest, StringReference4) {
   ASSERT_LT(GetMethodOffset(1u), 0xfcu);
 }
 
+TEST_F(Thumb2RelativePatcherTest, EntrypointCall) {
+  constexpr uint32_t kEntrypointOffset = 512;
+  const LinkerPatch patches[] = {
+      LinkerPatch::CallEntrypointPatch(0u, kEntrypointOffset),
+  };
+  AddCompiledMethod(MethodRef(1u), kCallCode, ArrayRef<const LinkerPatch>(patches));
+  Link();
+
+  uint32_t method_offset = GetMethodOffset(1u);
+  uint32_t thunk_offset = CompiledCode::AlignCode(method_offset + kCallCode.size(),
+                                                  InstructionSet::kThumb2);
+  uint32_t diff = thunk_offset - method_offset - kPcAdjustment;
+  ASSERT_TRUE(IsAligned<2u>(diff));
+  ASSERT_LT(diff >> 1, 1u << 8);  // Simple encoding, (diff >> 1) fits into 8 bits.
+  auto expected_code = GenNopsAndBl(0u, kBlPlus0 | ((diff >> 1) & 0xffu));
+  EXPECT_TRUE(CheckLinkedMethod(MethodRef(1u), ArrayRef<const uint8_t>(expected_code)));
+
+  // Verify the thunk.
+  uint32_t ldr_pc_tr_offset =
+      0xf8d00000 |                        // LDR Rt, [Rn, #<imm12>]
+      (/* tr */ 9 << 16) |                // Rn = TR
+      (/* pc */ 15 << 12) |               // Rt = PC
+      kEntrypointOffset;                  // imm12
+  uint16_t bkpt = 0xbe00;
+  ASSERT_LE(6u, output_.size() - thunk_offset);
+  EXPECT_EQ(ldr_pc_tr_offset, GetOutputInsn32(thunk_offset));
+  EXPECT_EQ(bkpt, GetOutputInsn16(thunk_offset + 4u));
+}
+
 const uint32_t kBakerValidRegs[] = {
     0,  1,  2,  3,  4,  5,  6,  7,
     9, 10, 11,                      // r8 (rMR), IP, SP, LR and PC are reserved.
diff --git a/dex2oat/linker/arm64/relative_patcher_arm64.cc b/dex2oat/linker/arm64/relative_patcher_arm64.cc
index ee8d4d1..2260f66 100644
--- a/dex2oat/linker/arm64/relative_patcher_arm64.cc
+++ b/dex2oat/linker/arm64/relative_patcher_arm64.cc
@@ -58,6 +58,7 @@ constexpr uint32_t kAdrpThunkSize = 8u;
 inline bool IsAdrpPatch(const LinkerPatch& patch) {
   switch (patch.GetType()) {
     case LinkerPatch::Type::kCallRelative:
+    case LinkerPatch::Type::kCallEntrypoint:
     case LinkerPatch::Type::kBakerReadBarrierBranch:
       return false;
     case LinkerPatch::Type::kIntrinsicReference:
@@ -189,30 +190,21 @@ uint32_t Arm64RelativePatcher::WriteThunks(OutputStream* out, uint32_t offset) {
 
 void Arm64RelativePatcher::PatchCall(std::vector<uint8_t>* code,
                                      uint32_t literal_offset,
-                                     uint32_t patch_offset, uint32_t
-                                     target_offset) {
-  DCHECK_LE(literal_offset + 4u, code->size());
-  DCHECK_EQ(literal_offset & 3u, 0u);
-  DCHECK_EQ(patch_offset & 3u, 0u);
-  DCHECK_EQ(target_offset & 3u, 0u);
+                                     uint32_t patch_offset,
+                                     uint32_t target_offset) {
+  DCHECK_ALIGNED(literal_offset, 4u);
+  DCHECK_ALIGNED(patch_offset, 4u);
+  DCHECK_ALIGNED(target_offset, 4u);
   uint32_t displacement = CalculateMethodCallDisplacement(patch_offset, target_offset & ~1u);
-  DCHECK_EQ(displacement & 3u, 0u);
-  DCHECK((displacement >> 27) == 0u || (displacement >> 27) == 31u);  // 28-bit signed.
-  uint32_t insn = (displacement & 0x0fffffffu) >> 2;
-  insn |= 0x94000000;  // BL
-
-  // Check that we're just overwriting an existing BL.
-  DCHECK_EQ(GetInsn(code, literal_offset) & 0xfc000000u, 0x94000000u);
-  // Write the new BL.
-  SetInsn(code, literal_offset, insn);
+  PatchBl(code, literal_offset, displacement);
 }
 
 void Arm64RelativePatcher::PatchPcRelativeReference(std::vector<uint8_t>* code,
                                                     const LinkerPatch& patch,
                                                     uint32_t patch_offset,
                                                     uint32_t target_offset) {
-  DCHECK_EQ(patch_offset & 3u, 0u);
-  DCHECK_EQ(target_offset & 3u, 0u);
+  DCHECK_ALIGNED(patch_offset, 4u);
+  DCHECK_ALIGNED(target_offset, 4u);
   uint32_t literal_offset = patch.LiteralOffset();
   uint32_t insn = GetInsn(code, literal_offset);
   uint32_t pc_insn_offset = patch.PcInsnOffset();
@@ -307,13 +299,21 @@ void Arm64RelativePatcher::PatchPcRelativeReference(std::vector<uint8_t>* code,
   }
 }
 
+void Arm64RelativePatcher::PatchEntrypointCall(std::vector<uint8_t>* code,
+                                               const LinkerPatch& patch,
+                                               uint32_t patch_offset) {
+  DCHECK_ALIGNED(patch_offset, 4u);
+  ThunkKey key = GetEntrypointCallKey(patch);
+  uint32_t target_offset = GetThunkTargetOffset(key, patch_offset);
+  uint32_t displacement = target_offset - patch_offset;
+  PatchBl(code, patch.LiteralOffset(), displacement);
+}
+
 void Arm64RelativePatcher::PatchBakerReadBarrierBranch(std::vector<uint8_t>* code,
                                                        const LinkerPatch& patch,
                                                        uint32_t patch_offset) {
   DCHECK_ALIGNED(patch_offset, 4u);
   uint32_t literal_offset = patch.LiteralOffset();
-  DCHECK_ALIGNED(literal_offset, 4u);
-  DCHECK_LT(literal_offset, code->size());
   uint32_t insn = GetInsn(code, literal_offset);
   DCHECK_EQ(insn & 0xffffffe0u, 0xb5000000);  // CBNZ Xt, +0 (unpatched)
   ThunkKey key = GetBakerThunkKey(patch);
@@ -328,6 +328,7 @@ void Arm64RelativePatcher::PatchBakerReadBarrierBranch(std::vector<uint8_t>* cod
 uint32_t Arm64RelativePatcher::MaxPositiveDisplacement(const ThunkKey& key) {
   switch (key.GetType()) {
     case ThunkType::kMethodCall:
+    case ThunkType::kEntrypointCall:
       return kMaxMethodCallPositiveDisplacement;
     case ThunkType::kBakerReadBarrier:
       return kMaxBcondPositiveDisplacement;
@@ -337,6 +338,7 @@ uint32_t Arm64RelativePatcher::MaxPositiveDisplacement(const ThunkKey& key) {
 uint32_t Arm64RelativePatcher::MaxNegativeDisplacement(const ThunkKey& key) {
   switch (key.GetType()) {
     case ThunkType::kMethodCall:
+    case ThunkType::kEntrypointCall:
       return kMaxMethodCallNegativeDisplacement;
     case ThunkType::kBakerReadBarrier:
       return kMaxBcondNegativeDisplacement;
@@ -357,6 +359,20 @@ uint32_t Arm64RelativePatcher::PatchAdrp(uint32_t adrp, uint32_t disp) {
       ((disp & 0x80000000u) >> (31 - 23));
 }
 
+void Arm64RelativePatcher::PatchBl(std::vector<uint8_t>* code,
+                                   uint32_t literal_offset,
+                                   uint32_t displacement) {
+  DCHECK_ALIGNED(displacement, 4u);
+  DCHECK((displacement >> 27) == 0u || (displacement >> 27) == 31u);  // 28-bit signed.
+  uint32_t insn = (displacement & 0x0fffffffu) >> 2;
+  insn |= 0x94000000;  // BL
+
+  // Check that we're just overwriting an existing BL.
+  DCHECK_EQ(GetInsn(code, literal_offset) & 0xfc000000u, 0x94000000u);
+  // Write the new BL.
+  SetInsn(code, literal_offset, insn);
+}
+
 bool Arm64RelativePatcher::NeedsErratum843419Thunk(ArrayRef<const uint8_t> code,
                                                    uint32_t literal_offset,
                                                    uint32_t patch_offset) {
@@ -409,7 +425,7 @@ bool Arm64RelativePatcher::NeedsErratum843419Thunk(ArrayRef<const uint8_t> code,
 
 void Arm64RelativePatcher::SetInsn(std::vector<uint8_t>* code, uint32_t offset, uint32_t value) {
   DCHECK_LE(offset + 4u, code->size());
-  DCHECK_EQ(offset & 3u, 0u);
+  DCHECK_ALIGNED(offset, 4u);
   uint8_t* addr = &(*code)[offset];
   addr[0] = (value >> 0) & 0xff;
   addr[1] = (value >> 8) & 0xff;
@@ -419,7 +435,7 @@ void Arm64RelativePatcher::SetInsn(std::vector<uint8_t>* code, uint32_t offset,
 
 uint32_t Arm64RelativePatcher::GetInsn(ArrayRef<const uint8_t> code, uint32_t offset) {
   DCHECK_LE(offset + 4u, code.size());
-  DCHECK_EQ(offset & 3u, 0u);
+  DCHECK_ALIGNED(offset, 4u);
   const uint8_t* addr = &code[offset];
   return
       (static_cast<uint32_t>(addr[0]) << 0) +
diff --git a/dex2oat/linker/arm64/relative_patcher_arm64.h b/dex2oat/linker/arm64/relative_patcher_arm64.h
index e95d0fe..9ad2c96 100644
--- a/dex2oat/linker/arm64/relative_patcher_arm64.h
+++ b/dex2oat/linker/arm64/relative_patcher_arm64.h
@@ -47,6 +47,9 @@ class Arm64RelativePatcher final : public ArmBaseRelativePatcher {
                                 const LinkerPatch& patch,
                                 uint32_t patch_offset,
                                 uint32_t target_offset) override;
+  void PatchEntrypointCall(std::vector<uint8_t>* code,
+                           const LinkerPatch& patch,
+                           uint32_t patch_offset) override;
   void PatchBakerReadBarrierBranch(std::vector<uint8_t>* code,
                                    const LinkerPatch& patch,
                                    uint32_t patch_offset) override;
@@ -57,10 +60,11 @@ class Arm64RelativePatcher final : public ArmBaseRelativePatcher {
 
  private:
   static uint32_t PatchAdrp(uint32_t adrp, uint32_t disp);
+  static void PatchBl(std::vector<uint8_t>* code, uint32_t literal_offset, uint32_t displacement);
 
   static bool NeedsErratum843419Thunk(ArrayRef<const uint8_t> code, uint32_t literal_offset,
                                       uint32_t patch_offset);
-  void SetInsn(std::vector<uint8_t>* code, uint32_t offset, uint32_t value);
+  static void SetInsn(std::vector<uint8_t>* code, uint32_t offset, uint32_t value);
   static uint32_t GetInsn(ArrayRef<const uint8_t> code, uint32_t offset);
 
   template <typename Alloc>
diff --git a/dex2oat/linker/arm64/relative_patcher_arm64_test.cc b/dex2oat/linker/arm64/relative_patcher_arm64_test.cc
index 9e54bbf..8bae5d4 100644
--- a/dex2oat/linker/arm64/relative_patcher_arm64_test.cc
+++ b/dex2oat/linker/arm64/relative_patcher_arm64_test.cc
@@ -198,7 +198,8 @@ class Arm64RelativePatcherTest : public RelativePatcherTest {
 
     // Make sure the ThunkProvider has all the necessary thunks.
     for (const LinkerPatch& patch : patches) {
-      if (patch.GetType() == LinkerPatch::Type::kBakerReadBarrierBranch ||
+      if (patch.GetType() == LinkerPatch::Type::kCallEntrypoint ||
+          patch.GetType() == LinkerPatch::Type::kBakerReadBarrierBranch ||
           patch.GetType() == LinkerPatch::Type::kCallRelative) {
         std::string debug_name;
         std::vector<uint8_t> thunk_code = CompileThunk(patch, &debug_name);
@@ -1005,6 +1006,36 @@ TEST_F(Arm64RelativePatcherTestDefault, StringReferenceXSpRel) {
       { 0u, 8u });
 }
 
+TEST_F(Arm64RelativePatcherTestDefault, EntrypointCall) {
+  constexpr uint32_t kEntrypointOffset = 512;
+  const LinkerPatch patches[] = {
+      LinkerPatch::CallEntrypointPatch(0u, kEntrypointOffset),
+  };
+  AddCompiledMethod(MethodRef(1u), kCallCode, ArrayRef<const LinkerPatch>(patches));
+  Link();
+
+  uint32_t method_offset = GetMethodOffset(1u);
+  uint32_t thunk_offset = CompiledCode::AlignCode(method_offset + kCallCode.size(),
+                                                  InstructionSet::kArm64);
+  uint32_t diff = thunk_offset - method_offset;
+  ASSERT_TRUE(IsAligned<4u>(diff));
+  ASSERT_LT(diff, 128 * MB);
+  auto expected_code = RawCode({kBlPlus0 | (diff >> 2)});
+  EXPECT_TRUE(CheckLinkedMethod(MethodRef(1u), ArrayRef<const uint8_t>(expected_code)));
+
+  // Verify the thunk.
+  uint32_t ldr_ip0_tr_offset =
+      0xf9400000 |                        // LDR Xt, [Xn, #<simm>]
+      ((kEntrypointOffset >> 3) << 10) |  // imm12 = (simm >> scale), scale = 3
+      (/* tr */ 19 << 5) |                // Xn = TR
+      /* ip0 */ 16;                       // Xt = ip0
+  uint32_t br_ip0 = 0xd61f0000 | (/* ip0 */ 16 << 5);
+  auto expected_thunk = RawCode({ ldr_ip0_tr_offset, br_ip0 });
+  ASSERT_LE(8u, output_.size() - thunk_offset);
+  EXPECT_EQ(ldr_ip0_tr_offset, GetOutputInsn(thunk_offset));
+  EXPECT_EQ(br_ip0, GetOutputInsn(thunk_offset + 4u));
+}
+
 void Arm64RelativePatcherTest::TestBakerField(uint32_t offset, uint32_t ref_reg) {
   uint32_t valid_regs[] = {
       0,  1,  2,  3,  4,  5,  6,  7,  8,  9,
diff --git a/dex2oat/linker/mips/relative_patcher_mips.cc b/dex2oat/linker/mips/relative_patcher_mips.cc
index 69e0846..4f4dc48 100644
--- a/dex2oat/linker/mips/relative_patcher_mips.cc
+++ b/dex2oat/linker/mips/relative_patcher_mips.cc
@@ -86,6 +86,12 @@ void MipsRelativePatcher::PatchPcRelativeReference(std::vector<uint8_t>* code,
   }
 }
 
+void MipsRelativePatcher::PatchEntrypointCall(std::vector<uint8_t>* code ATTRIBUTE_UNUSED,
+                                              const LinkerPatch& patch ATTRIBUTE_UNUSED,
+                                              uint32_t patch_offset ATTRIBUTE_UNUSED) {
+  LOG(FATAL) << "UNIMPLEMENTED";
+}
+
 void MipsRelativePatcher::PatchBakerReadBarrierBranch(std::vector<uint8_t>* code ATTRIBUTE_UNUSED,
                                                       const LinkerPatch& patch ATTRIBUTE_UNUSED,
                                                       uint32_t patch_offset ATTRIBUTE_UNUSED) {
diff --git a/dex2oat/linker/mips/relative_patcher_mips.h b/dex2oat/linker/mips/relative_patcher_mips.h
index 4c385a3..7cdac45 100644
--- a/dex2oat/linker/mips/relative_patcher_mips.h
+++ b/dex2oat/linker/mips/relative_patcher_mips.h
@@ -41,6 +41,9 @@ class MipsRelativePatcher final : public RelativePatcher {
                                 const LinkerPatch& patch,
                                 uint32_t patch_offset,
                                 uint32_t target_offset) override;
+  void PatchEntrypointCall(std::vector<uint8_t>* code,
+                           const LinkerPatch& patch,
+                           uint32_t patch_offset) override;
   void PatchBakerReadBarrierBranch(std::vector<uint8_t>* code,
                                    const LinkerPatch& patch,
                                    uint32_t patch_offset) override;
diff --git a/dex2oat/linker/mips64/relative_patcher_mips64.cc b/dex2oat/linker/mips64/relative_patcher_mips64.cc
index aae5746..2992487 100644
--- a/dex2oat/linker/mips64/relative_patcher_mips64.cc
+++ b/dex2oat/linker/mips64/relative_patcher_mips64.cc
@@ -84,6 +84,12 @@ void Mips64RelativePatcher::PatchPcRelativeReference(std::vector<uint8_t>* code,
   }
 }
 
+void Mips64RelativePatcher::PatchEntrypointCall(std::vector<uint8_t>* code ATTRIBUTE_UNUSED,
+                                                const LinkerPatch& patch ATTRIBUTE_UNUSED,
+                                                uint32_t patch_offset ATTRIBUTE_UNUSED) {
+  LOG(FATAL) << "UNIMPLEMENTED";
+}
+
 void Mips64RelativePatcher::PatchBakerReadBarrierBranch(std::vector<uint8_t>* code ATTRIBUTE_UNUSED,
                                                         const LinkerPatch& patch ATTRIBUTE_UNUSED,
                                                         uint32_t patch_offset ATTRIBUTE_UNUSED) {
diff --git a/dex2oat/linker/mips64/relative_patcher_mips64.h b/dex2oat/linker/mips64/relative_patcher_mips64.h
index 7b7c2cc..9d27b87 100644
--- a/dex2oat/linker/mips64/relative_patcher_mips64.h
+++ b/dex2oat/linker/mips64/relative_patcher_mips64.h
@@ -39,6 +39,9 @@ class Mips64RelativePatcher final : public RelativePatcher {
                                 const LinkerPatch& patch,
                                 uint32_t patch_offset,
                                 uint32_t target_offset) override;
+  void PatchEntrypointCall(std::vector<uint8_t>* code,
+                           const LinkerPatch& patch,
+                           uint32_t patch_offset) override;
   void PatchBakerReadBarrierBranch(std::vector<uint8_t>* code,
                                    const LinkerPatch& patch,
                                    uint32_t patch_offset) override;
diff --git a/dex2oat/linker/multi_oat_relative_patcher.h b/dex2oat/linker/multi_oat_relative_patcher.h
index 9b47a0d..2daada4 100644
--- a/dex2oat/linker/multi_oat_relative_patcher.h
+++ b/dex2oat/linker/multi_oat_relative_patcher.h
@@ -114,6 +114,13 @@ class MultiOatRelativePatcher final {
     relative_patcher_->PatchPcRelativeReference(code, patch, patch_offset, target_offset);
   }
 
+  void PatchEntrypointCall(std::vector<uint8_t>* code,
+                           const LinkerPatch& patch,
+                           uint32_t patch_offset) {
+    patch_offset += adjustment_;
+    relative_patcher_->PatchEntrypointCall(code, patch, patch_offset);
+  }
+
   void PatchBakerReadBarrierBranch(std::vector<uint8_t>* code,
                                    const LinkerPatch& patch,
                                    uint32_t patch_offset) {
diff --git a/dex2oat/linker/multi_oat_relative_patcher_test.cc b/dex2oat/linker/multi_oat_relative_patcher_test.cc
index 274084f..2a05816 100644
--- a/dex2oat/linker/multi_oat_relative_patcher_test.cc
+++ b/dex2oat/linker/multi_oat_relative_patcher_test.cc
@@ -94,6 +94,12 @@ class MultiOatRelativePatcherTest : public testing::Test {
       last_target_offset_ = target_offset;
     }
 
+    void PatchEntrypointCall(std::vector<uint8_t>* code ATTRIBUTE_UNUSED,
+                             const LinkerPatch& patch ATTRIBUTE_UNUSED,
+                             uint32_t patch_offset ATTRIBUTE_UNUSED) override {
+      LOG(FATAL) << "UNIMPLEMENTED";
+    }
+
     void PatchBakerReadBarrierBranch(std::vector<uint8_t>* code ATTRIBUTE_UNUSED,
                                      const LinkerPatch& patch ATTRIBUTE_UNUSED,
                                      uint32_t patch_offset ATTRIBUTE_UNUSED) override {
diff --git a/dex2oat/linker/oat_writer.cc b/dex2oat/linker/oat_writer.cc
index 6b23883..eec4940 100644
--- a/dex2oat/linker/oat_writer.cc
+++ b/dex2oat/linker/oat_writer.cc
@@ -1812,6 +1812,12 @@ class OatWriter::WriteCodeMethodVisitor : public OrderedMethodVisitor {
                                                                    target_offset);
               break;
             }
+            case LinkerPatch::Type::kCallEntrypoint: {
+              writer_->relative_patcher_->PatchEntrypointCall(&patched_code_,
+                                                              patch,
+                                                              offset_ + literal_offset);
+              break;
+            }
             case LinkerPatch::Type::kBakerReadBarrierBranch: {
               writer_->relative_patcher_->PatchBakerReadBarrierBranch(&patched_code_,
                                                                       patch,
diff --git a/dex2oat/linker/oat_writer_test.cc b/dex2oat/linker/oat_writer_test.cc
index c46aa18..2142727 100644
--- a/dex2oat/linker/oat_writer_test.cc
+++ b/dex2oat/linker/oat_writer_test.cc
@@ -469,7 +469,7 @@ TEST_F(OatTest, OatHeaderSizeCheck) {
   EXPECT_EQ(56U, sizeof(OatHeader));
   EXPECT_EQ(4U, sizeof(OatMethodOffsets));
   EXPECT_EQ(8U, sizeof(OatQuickMethodHeader));
-  EXPECT_EQ(166 * static_cast<size_t>(GetInstructionSetPointerSize(kRuntimeISA)),
+  EXPECT_EQ(167 * static_cast<size_t>(GetInstructionSetPointerSize(kRuntimeISA)),
             sizeof(QuickEntryPoints));
 }
 
diff --git a/dex2oat/linker/relative_patcher.cc b/dex2oat/linker/relative_patcher.cc
index 4db0e8a..f746cfb 100644
--- a/dex2oat/linker/relative_patcher.cc
+++ b/dex2oat/linker/relative_patcher.cc
@@ -77,6 +77,12 @@ std::unique_ptr<RelativePatcher> RelativePatcher::Create(
       LOG(FATAL) << "Unexpected relative dex cache array patch.";
     }
 
+    void PatchEntrypointCall(std::vector<uint8_t>* code ATTRIBUTE_UNUSED,
+                             const LinkerPatch& patch ATTRIBUTE_UNUSED,
+                             uint32_t patch_offset ATTRIBUTE_UNUSED) override {
+      LOG(FATAL) << "Unexpected entrypoint call patch.";
+    }
+
     void PatchBakerReadBarrierBranch(std::vector<uint8_t>* code ATTRIBUTE_UNUSED,
                                      const LinkerPatch& patch ATTRIBUTE_UNUSED,
                                      uint32_t patch_offset ATTRIBUTE_UNUSED) override {
diff --git a/dex2oat/linker/relative_patcher.h b/dex2oat/linker/relative_patcher.h
index e8e15c9..c05445c 100644
--- a/dex2oat/linker/relative_patcher.h
+++ b/dex2oat/linker/relative_patcher.h
@@ -137,6 +137,11 @@ class RelativePatcher {
                                         uint32_t patch_offset,
                                         uint32_t target_offset) = 0;
 
+  // Patch a call to an entrypoint trampoline.
+  virtual void PatchEntrypointCall(std::vector<uint8_t>* code,
+                                   const LinkerPatch& patch,
+                                   uint32_t patch_offset) = 0;
+
   // Patch a branch to a Baker read barrier thunk.
   virtual void PatchBakerReadBarrierBranch(std::vector<uint8_t>* code,
                                            const LinkerPatch& patch,
diff --git a/dex2oat/linker/relative_patcher_test.h b/dex2oat/linker/relative_patcher_test.h
index dead38d..dc53ac4 100644
--- a/dex2oat/linker/relative_patcher_test.h
+++ b/dex2oat/linker/relative_patcher_test.h
@@ -174,8 +174,10 @@ class RelativePatcherTest : public testing::Test {
             auto result = method_offset_map_.FindMethodOffset(patch.TargetMethod());
             uint32_t target_offset =
                 result.first ? result.second : kTrampolineOffset + compiled_method->CodeDelta();
-            patcher_->PatchCall(&patched_code_, patch.LiteralOffset(),
-                                offset + patch.LiteralOffset(), target_offset);
+            patcher_->PatchCall(&patched_code_,
+                                patch.LiteralOffset(),
+                                offset + patch.LiteralOffset(),
+                                target_offset);
           } else if (patch.GetType() == LinkerPatch::Type::kStringBssEntry) {
             uint32_t target_offset =
                 bss_begin_ + string_index_to_offset_map_.Get(patch.TargetStringIndex().index_);
@@ -190,6 +192,10 @@ class RelativePatcherTest : public testing::Test {
                                                patch,
                                                offset + patch.LiteralOffset(),
                                                target_offset);
+          } else if (patch.GetType() == LinkerPatch::Type::kCallEntrypoint) {
+            patcher_->PatchEntrypointCall(&patched_code_,
+                                          patch,
+                                          offset + patch.LiteralOffset());
           } else if (patch.GetType() == LinkerPatch::Type::kBakerReadBarrierBranch) {
             patcher_->PatchBakerReadBarrierBranch(&patched_code_,
                                                   patch,
@@ -300,11 +306,10 @@ class RelativePatcherTest : public testing::Test {
      public:
       explicit ThunkKey(const LinkerPatch& patch)
           : type_(patch.GetType()),
-            custom_value1_(patch.GetType() == LinkerPatch::Type::kBakerReadBarrierBranch
-                               ? patch.GetBakerCustomValue1() : 0u),
-            custom_value2_(patch.GetType() == LinkerPatch::Type::kBakerReadBarrierBranch
-                               ? patch.GetBakerCustomValue2() : 0u) {
-        CHECK(patch.GetType() == LinkerPatch::Type::kBakerReadBarrierBranch ||
+            custom_value1_(CustomValue1(patch)),
+            custom_value2_(CustomValue2(patch)) {
+        CHECK(patch.GetType() == LinkerPatch::Type::kCallEntrypoint ||
+              patch.GetType() == LinkerPatch::Type::kBakerReadBarrierBranch ||
               patch.GetType() == LinkerPatch::Type::kCallRelative);
       }
 
@@ -319,6 +324,26 @@ class RelativePatcherTest : public testing::Test {
       }
 
      private:
+      static uint32_t CustomValue1(const LinkerPatch& patch) {
+        switch (patch.GetType()) {
+          case LinkerPatch::Type::kCallEntrypoint:
+            return patch.EntrypointOffset();
+          case LinkerPatch::Type::kBakerReadBarrierBranch:
+            return patch.GetBakerCustomValue1();
+          default:
+            return 0;
+        }
+      }
+
+      static uint32_t CustomValue2(const LinkerPatch& patch) {
+        switch (patch.GetType()) {
+          case LinkerPatch::Type::kBakerReadBarrierBranch:
+            return patch.GetBakerCustomValue2();
+          default:
+            return 0;
+        }
+      }
+
       const LinkerPatch::Type type_;
       const uint32_t custom_value1_;
       const uint32_t custom_value2_;
diff --git a/dex2oat/linker/x86/relative_patcher_x86.cc b/dex2oat/linker/x86/relative_patcher_x86.cc
index cdd2cef..3323506 100644
--- a/dex2oat/linker/x86/relative_patcher_x86.cc
+++ b/dex2oat/linker/x86/relative_patcher_x86.cc
@@ -57,6 +57,12 @@ void X86RelativePatcher::PatchPcRelativeReference(std::vector<uint8_t>* code,
   (*code)[literal_offset + 3u] = static_cast<uint8_t>(diff >> 24);
 }
 
+void X86RelativePatcher::PatchEntrypointCall(std::vector<uint8_t>* code ATTRIBUTE_UNUSED,
+                                             const LinkerPatch& patch ATTRIBUTE_UNUSED,
+                                             uint32_t patch_offset ATTRIBUTE_UNUSED) {
+  LOG(FATAL) << "UNIMPLEMENTED";
+}
+
 void X86RelativePatcher::PatchBakerReadBarrierBranch(std::vector<uint8_t>* code ATTRIBUTE_UNUSED,
                                                      const LinkerPatch& patch ATTRIBUTE_UNUSED,
                                                      uint32_t patch_offset ATTRIBUTE_UNUSED) {
diff --git a/dex2oat/linker/x86/relative_patcher_x86.h b/dex2oat/linker/x86/relative_patcher_x86.h
index 3da62fb..589a498 100644
--- a/dex2oat/linker/x86/relative_patcher_x86.h
+++ b/dex2oat/linker/x86/relative_patcher_x86.h
@@ -30,6 +30,9 @@ class X86RelativePatcher final : public X86BaseRelativePatcher {
                                 const LinkerPatch& patch,
                                 uint32_t patch_offset,
                                 uint32_t target_offset) override;
+  void PatchEntrypointCall(std::vector<uint8_t>* code,
+                           const LinkerPatch& patch,
+                           uint32_t patch_offset) override;
   void PatchBakerReadBarrierBranch(std::vector<uint8_t>* code,
                                    const LinkerPatch& patch,
                                    uint32_t patch_offset) override;
diff --git a/dex2oat/linker/x86_64/relative_patcher_x86_64.cc b/dex2oat/linker/x86_64/relative_patcher_x86_64.cc
index c80f6a9..0b9d07e 100644
--- a/dex2oat/linker/x86_64/relative_patcher_x86_64.cc
+++ b/dex2oat/linker/x86_64/relative_patcher_x86_64.cc
@@ -35,6 +35,12 @@ void X86_64RelativePatcher::PatchPcRelativeReference(std::vector<uint8_t>* code,
   reinterpret_cast<unaligned_int32_t*>(&(*code)[patch.LiteralOffset()])[0] = displacement;
 }
 
+void X86_64RelativePatcher::PatchEntrypointCall(std::vector<uint8_t>* code ATTRIBUTE_UNUSED,
+                                                const LinkerPatch& patch ATTRIBUTE_UNUSED,
+                                                uint32_t patch_offset ATTRIBUTE_UNUSED) {
+  LOG(FATAL) << "UNIMPLEMENTED";
+}
+
 void X86_64RelativePatcher::PatchBakerReadBarrierBranch(std::vector<uint8_t>* code ATTRIBUTE_UNUSED,
                                                         const LinkerPatch& patch ATTRIBUTE_UNUSED,
                                                         uint32_t patch_offset ATTRIBUTE_UNUSED) {
diff --git a/dex2oat/linker/x86_64/relative_patcher_x86_64.h b/dex2oat/linker/x86_64/relative_patcher_x86_64.h
index a82fef3..7b99bd8 100644
--- a/dex2oat/linker/x86_64/relative_patcher_x86_64.h
+++ b/dex2oat/linker/x86_64/relative_patcher_x86_64.h
@@ -30,6 +30,9 @@ class X86_64RelativePatcher final : public X86BaseRelativePatcher {
                                 const LinkerPatch& patch,
                                 uint32_t patch_offset,
                                 uint32_t target_offset) override;
+  void PatchEntrypointCall(std::vector<uint8_t>* code,
+                           const LinkerPatch& patch,
+                           uint32_t patch_offset) override;
   void PatchBakerReadBarrierBranch(std::vector<uint8_t>* code,
                                    const LinkerPatch& patch,
                                    uint32_t patch_offset) override;
diff --git a/libartbase/base/bit_memory_region.h b/libartbase/base/bit_memory_region.h
index 1f1011e..637332e 100644
--- a/libartbase/base/bit_memory_region.h
+++ b/libartbase/base/bit_memory_region.h
@@ -252,6 +252,27 @@ class BitMemoryReader {
     return x;
   }
 
+  // Optimized version to read several consecutive varints.
+  // It reads all the headers at once in a single bit read.
+  template<int N>  // Inference works only with ref-arrays.
+  ALWAYS_INLINE void ReadVarints(uint32_t (&varints)[N]) {
+    static_assert(N * kVarintHeaderBits <= sizeof(uint32_t) * kBitsPerByte, "N too big");
+    uint32_t headers = ReadBits(N * kVarintHeaderBits);
+    uint32_t* out = varints;
+    for (int i = 0; i < N; out++) {
+      uint32_t header = BitFieldExtract(headers, (i++) * kVarintHeaderBits, kVarintHeaderBits);
+      if (LIKELY(header <= kVarintSmallValue)) {
+        // Fast-path: consume one of the headers and continue to the next varint.
+        *out = header;
+      } else {
+        // Slow-path: rollback reader, read large value, and read remaning headers.
+        finished_region_.Resize(finished_region_.size_in_bits() - (N-i) * kVarintHeaderBits);
+        *out = ReadBits((header - kVarintSmallValue) * kBitsPerByte);
+        headers = ReadBits((N-i) * kVarintHeaderBits) << (i * kVarintHeaderBits);
+      }
+    }
+  }
+
  private:
   // Represents all of the bits which were read so far. There is no upper bound.
   // Therefore, by definition, the "cursor" is always at the end of the region.
diff --git a/libartbase/base/bit_table.h b/libartbase/base/bit_table.h
index d6a1d7b..6c91ce5 100644
--- a/libartbase/base/bit_table.h
+++ b/libartbase/base/bit_table.h
@@ -51,9 +51,11 @@ class BitTableBase {
     // Decode row count and column sizes from the table header.
     num_rows_ = reader.ReadVarint();
     if (num_rows_ != 0) {
+      uint32_t column_bits[kNumColumns];
+      reader.ReadVarints(column_bits);
       column_offset_[0] = 0;
       for (uint32_t i = 0; i < kNumColumns; i++) {
-        size_t column_end = column_offset_[i] + reader.ReadVarint();
+        size_t column_end = column_offset_[i] + column_bits[i];
         column_offset_[i + 1] = dchecked_integral_cast<uint16_t>(column_end);
       }
     }
diff --git a/libdexfile/dex/dex_file_verifier.cc b/libdexfile/dex/dex_file_verifier.cc
index 86a28e5..52700a6 100644
--- a/libdexfile/dex/dex_file_verifier.cc
+++ b/libdexfile/dex/dex_file_verifier.cc
@@ -18,6 +18,7 @@
 
 #include <inttypes.h>
 
+#include <algorithm>
 #include <memory>
 
 #include "android-base/stringprintf.h"
@@ -1997,6 +1998,13 @@ bool DexFileVerifier::CheckIntraSection() {
   uint32_t count = map->size_;
   ptr_ = begin_;
 
+  // Preallocate offset map to avoid some allocations. We can only guess from the list items,
+  // not derived things.
+  offset_to_type_map_.reserve(
+      std::min(header_->class_defs_size_, 65535u) +
+      std::min(header_->string_ids_size_, 65535u) +
+      2 * std::min(header_->method_ids_size_, 65535u));
+
   // Check the items listed in the map.
   for (; count != 0u; --count) {
     const size_t current_offset = offset;
diff --git a/libelffile/dwarf/debug_frame_opcode_writer.h b/libelffile/dwarf/debug_frame_opcode_writer.h
index b255f9c..65ca6bf 100644
--- a/libelffile/dwarf/debug_frame_opcode_writer.h
+++ b/libelffile/dwarf/debug_frame_opcode_writer.h
@@ -80,8 +80,10 @@ class DebugFrameOpCodeWriter : private Writer<Vector> {
   }
 
   // Custom alias - spill many registers based on bitmask.
-  void ALWAYS_INLINE RelOffsetForMany(Reg reg_base, int offset,
-                                      uint32_t reg_mask, int reg_size) {
+  void ALWAYS_INLINE RelOffsetForMany(Reg reg_base,
+                                      int32_t offset,
+                                      uint32_t reg_mask,
+                                      int32_t reg_size) {
     DCHECK(reg_size == 4 || reg_size == 8);
     if (UNLIKELY(enabled_)) {
       for (int i = 0; reg_mask != 0u; reg_mask >>= 1, i++) {
diff --git a/runtime/Android.bp b/runtime/Android.bp
index 7bf662c..41e5734 100644
--- a/runtime/Android.bp
+++ b/runtime/Android.bp
@@ -195,6 +195,7 @@ libart_cc_defaults {
         "signal_catcher.cc",
         "stack.cc",
         "stack_map.cc",
+        "string_builder_append.cc",
         "thread.cc",
         "thread_list.cc",
         "thread_pool.cc",
@@ -240,6 +241,7 @@ libart_cc_defaults {
         "entrypoints/quick/quick_jni_entrypoints.cc",
         "entrypoints/quick/quick_lock_entrypoints.cc",
         "entrypoints/quick/quick_math_entrypoints.cc",
+        "entrypoints/quick/quick_string_builder_append_entrypoints.cc",
         "entrypoints/quick/quick_thread_entrypoints.cc",
         "entrypoints/quick/quick_throw_entrypoints.cc",
         "entrypoints/quick/quick_trampoline_entrypoints.cc",
diff --git a/runtime/arch/arm/quick_entrypoints_arm.S b/runtime/arch/arm/quick_entrypoints_arm.S
index b57e119..196a24b 100644
--- a/runtime/arch/arm/quick_entrypoints_arm.S
+++ b/runtime/arch/arm/quick_entrypoints_arm.S
@@ -2215,6 +2215,17 @@ ENTRY art_quick_l2f
     pop   {pc}
 END art_quick_l2f
 
+    .extern artStringBuilderAppend
+ENTRY art_quick_string_builder_append
+    SETUP_SAVE_REFS_ONLY_FRAME r2       @ save callee saves in case of GC
+    add    r1, sp, #(FRAME_SIZE_SAVE_REFS_ONLY + __SIZEOF_POINTER__)  @ pass args
+    mov    r2, rSELF                    @ pass Thread::Current
+    bl     artStringBuilderAppend       @ (uint32_t, const unit32_t*, Thread*)
+    RESTORE_SAVE_REFS_ONLY_FRAME
+    REFRESH_MARKING_REGISTER
+    RETURN_IF_RESULT_IS_NON_ZERO_OR_DELIVER
+END art_quick_string_builder_append
+
 .macro CONDITIONAL_CBZ reg, reg_if, dest
 .ifc \reg, \reg_if
     cbz \reg, \dest
diff --git a/runtime/arch/arm64/quick_entrypoints_arm64.S b/runtime/arch/arm64/quick_entrypoints_arm64.S
index cb74ee8..d2d9c69 100644
--- a/runtime/arch/arm64/quick_entrypoints_arm64.S
+++ b/runtime/arch/arm64/quick_entrypoints_arm64.S
@@ -2468,6 +2468,17 @@ ENTRY art_quick_indexof
 #endif
 END art_quick_indexof
 
+    .extern artStringBuilderAppend
+ENTRY art_quick_string_builder_append
+    SETUP_SAVE_REFS_ONLY_FRAME          // save callee saves in case of GC
+    add    x1, sp, #(FRAME_SIZE_SAVE_REFS_ONLY + __SIZEOF_POINTER__)  // pass args
+    mov    x2, xSELF                    // pass Thread::Current
+    bl     artStringBuilderAppend       // (uint32_t, const unit32_t*, Thread*)
+    RESTORE_SAVE_REFS_ONLY_FRAME
+    REFRESH_MARKING_REGISTER
+    RETURN_IF_RESULT_IS_NON_ZERO_OR_DELIVER
+END art_quick_string_builder_append
+
     /*
      * Create a function `name` calling the ReadBarrier::Mark routine,
      * getting its argument and returning its result through W register
diff --git a/runtime/arch/mips/quick_entrypoints_mips.S b/runtime/arch/mips/quick_entrypoints_mips.S
index b10d1fc..5697185 100644
--- a/runtime/arch/mips/quick_entrypoints_mips.S
+++ b/runtime/arch/mips/quick_entrypoints_mips.S
@@ -2694,6 +2694,16 @@ ENTRY_NO_GP art_quick_string_compareto
     subu   $v0, $t0, $t1  # return (this.charAt(i) - anotherString.charAt(i))
 END art_quick_string_compareto
 
+    .extern artStringBuilderAppend
+ENTRY art_quick_string_builder_append
+    SETUP_SAVE_REFS_ONLY_FRAME        # save callee saves in case of GC
+    la      $t9, artStringBuilderAppend
+    addiu   $a1, $sp, ARG_SLOT_SIZE + FRAME_SIZE_SAVE_REFS_ONLY + __SIZEOF_POINTER__ # pass args
+    jalr    $t9                       # (uint32_t, const unit32_t*, Thread*)
+    move    $a2, rSELF                # pass Thread::Current
+    RETURN_IF_RESULT_IS_NON_ZERO_OR_DELIVER
+END art_quick_string_builder_append
+
     /*
      * Create a function `name` calling the ReadBarrier::Mark routine,
      * getting its argument and returning its result through register
diff --git a/runtime/arch/mips64/quick_entrypoints_mips64.S b/runtime/arch/mips64/quick_entrypoints_mips64.S
index ebf1d5b..c54e7bb 100644
--- a/runtime/arch/mips64/quick_entrypoints_mips64.S
+++ b/runtime/arch/mips64/quick_entrypoints_mips64.S
@@ -2476,6 +2476,16 @@ ENTRY_NO_GP art_quick_indexof
 #endif
 END art_quick_indexof
 
+    .extern artStringBuilderAppend
+ENTRY art_quick_string_builder_append
+    SETUP_SAVE_REFS_ONLY_FRAME        # save callee saves in case of GC
+    dla     $t9, artStringBuilderAppend
+    daddiu  $a1, $sp, FRAME_SIZE_SAVE_REFS_ONLY + __SIZEOF_POINTER__  # pass args
+    jalr    $t9                       # (uint32_t, const unit32_t*, Thread*)
+    move    $a2, rSELF                # pass Thread::Current
+    RETURN_IF_RESULT_IS_NON_ZERO_OR_DELIVER
+END art_quick_string_builder_append
+
     /*
      * Create a function `name` calling the ReadBarrier::Mark routine,
      * getting its argument and returning its result through register
diff --git a/runtime/arch/x86/quick_entrypoints_x86.S b/runtime/arch/x86/quick_entrypoints_x86.S
index 306c4eb..b8ccd8e 100644
--- a/runtime/arch/x86/quick_entrypoints_x86.S
+++ b/runtime/arch/x86/quick_entrypoints_x86.S
@@ -2246,6 +2246,25 @@ DEFINE_FUNCTION art_quick_string_compareto
     ret
 END_FUNCTION art_quick_string_compareto
 
+DEFINE_FUNCTION art_quick_string_builder_append
+    SETUP_SAVE_REFS_ONLY_FRAME ebx, ebx       // save ref containing registers for GC
+    // Outgoing argument set up
+    leal FRAME_SIZE_SAVE_REFS_ONLY + __SIZEOF_POINTER__(%esp), %edi  // prepare args
+    push %eax                                 // push padding
+    CFI_ADJUST_CFA_OFFSET(4)
+    pushl %fs:THREAD_SELF_OFFSET              // pass Thread::Current()
+    CFI_ADJUST_CFA_OFFSET(4)
+    push %edi                                 // pass args
+    CFI_ADJUST_CFA_OFFSET(4)
+    push %eax                                 // pass format
+    CFI_ADJUST_CFA_OFFSET(4)
+    call SYMBOL(artStringBuilderAppend)       // (uint32_t, const unit32_t*, Thread*)
+    addl MACRO_LITERAL(16), %esp              // pop arguments
+    CFI_ADJUST_CFA_OFFSET(-16)
+    RESTORE_SAVE_REFS_ONLY_FRAME              // restore frame up to return address
+    RETURN_IF_RESULT_IS_NON_ZERO_OR_DELIVER   // return or deliver exception
+END_FUNCTION art_quick_string_builder_append
+
 // Create a function `name` calling the ReadBarrier::Mark routine,
 // getting its argument and returning its result through register
 // `reg`, saving and restoring all caller-save registers.
diff --git a/runtime/arch/x86_64/quick_entrypoints_x86_64.S b/runtime/arch/x86_64/quick_entrypoints_x86_64.S
index 39bf6e8..1177477 100644
--- a/runtime/arch/x86_64/quick_entrypoints_x86_64.S
+++ b/runtime/arch/x86_64/quick_entrypoints_x86_64.S
@@ -2201,6 +2201,16 @@ DEFINE_FUNCTION art_quick_instance_of
     ret
 END_FUNCTION art_quick_instance_of
 
+DEFINE_FUNCTION art_quick_string_builder_append
+    SETUP_SAVE_REFS_ONLY_FRAME                // save ref containing registers for GC
+    // Outgoing argument set up
+    leaq FRAME_SIZE_SAVE_REFS_ONLY + __SIZEOF_POINTER__(%rsp), %rsi  // pass args
+    movq %gs:THREAD_SELF_OFFSET, %rdx         // pass Thread::Current()
+    call artStringBuilderAppend               // (uint32_t, const unit32_t*, Thread*)
+    RESTORE_SAVE_REFS_ONLY_FRAME              // restore frame up to return address
+    RETURN_IF_RESULT_IS_NON_ZERO_OR_DELIVER   // return or deliver exception
+END_FUNCTION art_quick_string_builder_append
+
 // Create a function `name` calling the ReadBarrier::Mark routine,
 // getting its argument and returning its result through register
 // `reg`, saving and restoring all caller-save registers.
diff --git a/runtime/entrypoints/entrypoint_utils.cc b/runtime/entrypoints/entrypoint_utils.cc
index ee2ab56..71196d4 100644
--- a/runtime/entrypoints/entrypoint_utils.cc
+++ b/runtime/entrypoints/entrypoint_utils.cc
@@ -203,13 +203,15 @@ static inline ArtMethod* DoGetCalleeSaveMethodCaller(ArtMethod* outer_method,
       const OatQuickMethodHeader* current_code = outer_method->GetOatQuickMethodHeader(caller_pc);
       DCHECK(current_code != nullptr);
       DCHECK(current_code->IsOptimized());
-      uintptr_t native_pc_offset = current_code->NativeQuickPcOffset(caller_pc);
-      CodeInfo code_info(current_code, CodeInfo::DecodeFlags::InlineInfoOnly);
-      StackMap stack_map = code_info.GetStackMapForNativePcOffset(native_pc_offset);
-      DCHECK(stack_map.IsValid());
-      BitTableRange<InlineInfo> inline_infos = code_info.GetInlineInfosOf(stack_map);
-      if (!inline_infos.empty()) {
-        caller = GetResolvedMethod(outer_method, code_info, inline_infos);
+      if (CodeInfo::HasInlineInfo(current_code->GetOptimizedCodeInfoPtr())) {
+        uintptr_t native_pc_offset = current_code->NativeQuickPcOffset(caller_pc);
+        CodeInfo code_info(current_code, CodeInfo::DecodeFlags::InlineInfoOnly);
+        StackMap stack_map = code_info.GetStackMapForNativePcOffset(native_pc_offset);
+        DCHECK(stack_map.IsValid());
+        BitTableRange<InlineInfo> inline_infos = code_info.GetInlineInfosOf(stack_map);
+        if (!inline_infos.empty()) {
+          caller = GetResolvedMethod(outer_method, code_info, inline_infos);
+        }
       }
     }
     if (kIsDebugBuild && do_caller_check) {
diff --git a/runtime/entrypoints/quick/quick_default_init_entrypoints.h b/runtime/entrypoints/quick/quick_default_init_entrypoints.h
index ce12fde..f5bb5a3 100644
--- a/runtime/entrypoints/quick/quick_default_init_entrypoints.h
+++ b/runtime/entrypoints/quick/quick_default_init_entrypoints.h
@@ -121,6 +121,9 @@ static void DefaultInitEntryPoints(JniEntryPoints* jpoints, QuickEntryPoints* qp
 
   // Deoptimize
   qpoints->pDeoptimize = art_quick_deoptimize_from_compiled_code;
+
+  // StringBuilder append
+  qpoints->pStringBuilderAppend = art_quick_string_builder_append;
 }
 
 }  // namespace art
diff --git a/runtime/entrypoints/quick/quick_entrypoints.h b/runtime/entrypoints/quick/quick_entrypoints.h
index 243f7ec..954450f 100644
--- a/runtime/entrypoints/quick/quick_entrypoints.h
+++ b/runtime/entrypoints/quick/quick_entrypoints.h
@@ -34,6 +34,7 @@ class Array;
 class Class;
 template<class MirrorType> class CompressedReference;
 class Object;
+class String;
 }  // namespace mirror
 
 class ArtMethod;
@@ -78,6 +79,11 @@ extern mirror::Object* JniMethodEndWithReferenceSynchronized(jobject result,
                                                              jobject locked, Thread* self)
     NO_THREAD_SAFETY_ANALYSIS HOT_ATTR;
 
+extern "C" mirror::String* artStringBuilderAppend(uint32_t format,
+                                                  const uint32_t* args,
+                                                  Thread* self)
+    REQUIRES_SHARED(Locks::mutator_lock_) HOT_ATTR;
+
 extern void ReadBarrierJni(mirror::CompressedReference<mirror::Object>* handle_on_stack,
                            Thread* self)
     NO_THREAD_SAFETY_ANALYSIS HOT_ATTR;
diff --git a/runtime/entrypoints/quick/quick_entrypoints_list.h b/runtime/entrypoints/quick/quick_entrypoints_list.h
index 42b680e..21e248c 100644
--- a/runtime/entrypoints/quick/quick_entrypoints_list.h
+++ b/runtime/entrypoints/quick/quick_entrypoints_list.h
@@ -168,6 +168,8 @@
   V(NewStringFromString, void, void) \
   V(NewStringFromStringBuffer, void, void) \
   V(NewStringFromStringBuilder, void, void) \
+\
+  V(StringBuilderAppend, void*, uint32_t) \
 \
   V(ReadBarrierJni, void, mirror::CompressedReference<mirror::Object>*, Thread*) \
   V(ReadBarrierMarkReg00, mirror::Object*, mirror::Object*) \
diff --git a/runtime/entrypoints/runtime_asm_entrypoints.h b/runtime/entrypoints/runtime_asm_entrypoints.h
index fa287cb..3f4e91e 100644
--- a/runtime/entrypoints/runtime_asm_entrypoints.h
+++ b/runtime/entrypoints/runtime_asm_entrypoints.h
@@ -87,6 +87,8 @@ static inline const void* GetQuickInstrumentationExitPc() {
   return reinterpret_cast<const void*>(art_quick_instrumentation_exit);
 }
 
+extern "C" void* art_quick_string_builder_append(uint32_t format);
+
 }  // namespace art
 
 #endif  // ART_RUNTIME_ENTRYPOINTS_RUNTIME_ASM_ENTRYPOINTS_H_
diff --git a/runtime/entrypoints_order_test.cc b/runtime/entrypoints_order_test.cc
index 040a8c5..5b4275c 100644
--- a/runtime/entrypoints_order_test.cc
+++ b/runtime/entrypoints_order_test.cc
@@ -331,7 +331,9 @@ class EntrypointsOrderTest : public CommonRuntimeTest {
                          sizeof(void*));
     EXPECT_OFFSET_DIFFNP(QuickEntryPoints, pNewStringFromStringBuffer, pNewStringFromStringBuilder,
                          sizeof(void*));
-    EXPECT_OFFSET_DIFFNP(QuickEntryPoints, pNewStringFromStringBuilder, pReadBarrierJni,
+    EXPECT_OFFSET_DIFFNP(QuickEntryPoints, pNewStringFromStringBuilder, pStringBuilderAppend,
+                         sizeof(void*));
+    EXPECT_OFFSET_DIFFNP(QuickEntryPoints, pStringBuilderAppend, pReadBarrierJni,
                          sizeof(void*));
     EXPECT_OFFSET_DIFFNP(QuickEntryPoints, pReadBarrierJni, pReadBarrierMarkReg00, sizeof(void*));
     EXPECT_OFFSET_DIFFNP(QuickEntryPoints, pReadBarrierMarkReg00, pReadBarrierMarkReg01,
diff --git a/runtime/gc/collector/concurrent_copying.cc b/runtime/gc/collector/concurrent_copying.cc
index 9428a0b..4337927 100644
--- a/runtime/gc/collector/concurrent_copying.cc
+++ b/runtime/gc/collector/concurrent_copying.cc
@@ -446,15 +446,17 @@ class ConcurrentCopying::ThreadFlipVisitor : public Closure, public RootVisitor
         << thread->GetState() << " thread " << thread << " self " << self;
     thread->SetIsGcMarkingAndUpdateEntrypoints(true);
     if (use_tlab_ && thread->HasTlab()) {
+      // We should not reuse the partially utilized TLABs revoked here as they
+      // are going to be part of from-space.
       if (ConcurrentCopying::kEnableFromSpaceAccountingCheck) {
         // This must come before the revoke.
         size_t thread_local_objects = thread->GetThreadLocalObjectsAllocated();
-        concurrent_copying_->region_space_->RevokeThreadLocalBuffers(thread);
+        concurrent_copying_->region_space_->RevokeThreadLocalBuffers(thread, /*reuse=*/ false);
         reinterpret_cast<Atomic<size_t>*>(
             &concurrent_copying_->from_space_num_objects_at_first_pause_)->
                 fetch_add(thread_local_objects, std::memory_order_relaxed);
       } else {
-        concurrent_copying_->region_space_->RevokeThreadLocalBuffers(thread);
+        concurrent_copying_->region_space_->RevokeThreadLocalBuffers(thread, /*reuse=*/ false);
       }
     }
     if (kUseThreadLocalAllocationStack) {
diff --git a/runtime/gc/heap.cc b/runtime/gc/heap.cc
index ff53f78..c2a3cbd 100644
--- a/runtime/gc/heap.cc
+++ b/runtime/gc/heap.cc
@@ -145,10 +145,6 @@ static const char* kRegionSpaceName = "main space (region space)";
 // If true, we log all GCs in the both the foreground and background. Used for debugging.
 static constexpr bool kLogAllGCs = false;
 
-// How much we grow the TLAB if we can do it.
-static constexpr size_t kPartialTlabSize = 16 * KB;
-static constexpr bool kUsePartialTlabs = true;
-
 // Use Max heap for 2 seconds, this is smaller than the usual 5s window since we don't want to leave
 // allocate with relaxed ergonomics for that long.
 static constexpr size_t kPostForkMaxHeapDurationMS = 2000;
@@ -4355,14 +4351,13 @@ mirror::Object* Heap::AllocWithNewTLAB(Thread* self,
             ? std::max(alloc_size, kPartialTlabSize)
             : gc::space::RegionSpace::kRegionSize;
         // Try to allocate a tlab.
-        if (!region_space_->AllocNewTlab(self, new_tlab_size)) {
+        if (!region_space_->AllocNewTlab(self, new_tlab_size, bytes_tl_bulk_allocated)) {
           // Failed to allocate a tlab. Try non-tlab.
           return region_space_->AllocNonvirtual<false>(alloc_size,
                                                        bytes_allocated,
                                                        usable_size,
                                                        bytes_tl_bulk_allocated);
         }
-        *bytes_tl_bulk_allocated = new_tlab_size;
         // Fall-through to using the TLAB below.
       } else {
         // Check OOME for a non-tlab allocation.
diff --git a/runtime/gc/heap.h b/runtime/gc/heap.h
index 5cf1978..3562774 100644
--- a/runtime/gc/heap.h
+++ b/runtime/gc/heap.h
@@ -127,6 +127,10 @@ static constexpr bool kUseThreadLocalAllocationStack = true;
 
 class Heap {
  public:
+  // How much we grow the TLAB if we can do it.
+  static constexpr size_t kPartialTlabSize = 16 * KB;
+  static constexpr bool kUsePartialTlabs = true;
+
   static constexpr size_t kDefaultStartingSize = kPageSize;
   static constexpr size_t kDefaultInitialSize = 2 * MB;
   static constexpr size_t kDefaultMaximumSize = 256 * MB;
diff --git a/runtime/gc/space/region_space-inl.h b/runtime/gc/space/region_space-inl.h
index 86a0a6e..b6077aa 100644
--- a/runtime/gc/space/region_space-inl.h
+++ b/runtime/gc/space/region_space-inl.h
@@ -503,7 +503,7 @@ inline size_t RegionSpace::Region::BytesAllocated() const {
     DCHECK_LE(begin_, Top());
     size_t bytes;
     if (is_a_tlab_) {
-      bytes = thread_->GetThreadLocalBytesAllocated();
+      bytes = thread_->GetTlabEnd() - begin_;
     } else {
       bytes = static_cast<size_t>(Top() - begin_);
     }
diff --git a/runtime/gc/space/region_space.cc b/runtime/gc/space/region_space.cc
index 823043e..b2c5a2f 100644
--- a/runtime/gc/space/region_space.cc
+++ b/runtime/gc/space/region_space.cc
@@ -337,6 +337,10 @@ void RegionSpace::SetFromSpace(accounting::ReadBarrierTable* rb_table,
     rb_table->SetAll();
   }
   MutexLock mu(Thread::Current(), region_lock_);
+  // We cannot use the partially utilized TLABs across a GC. Therefore, revoke
+  // them during the thread-flip.
+  partial_tlabs_.clear();
+
   // Counter for the number of expected large tail regions following a large region.
   size_t num_expected_large_tails = 0U;
   // Flag to store whether the previously seen large region has been evacuated.
@@ -833,17 +837,40 @@ void RegionSpace::RecordAlloc(mirror::Object* ref) {
   r->objects_allocated_.fetch_add(1, std::memory_order_relaxed);
 }
 
-bool RegionSpace::AllocNewTlab(Thread* self, size_t min_bytes) {
+bool RegionSpace::AllocNewTlab(Thread* self,
+                               const size_t tlab_size,
+                               size_t* bytes_tl_bulk_allocated) {
   MutexLock mu(self, region_lock_);
-  RevokeThreadLocalBuffersLocked(self);
-  // Retain sufficient free regions for full evacuation.
-
-  Region* r = AllocateRegion(/*for_evac=*/ false);
+  RevokeThreadLocalBuffersLocked(self, /*reuse=*/ gc::Heap::kUsePartialTlabs);
+  Region* r = nullptr;
+  uint8_t* pos = nullptr;
+  *bytes_tl_bulk_allocated = tlab_size;
+  // First attempt to get a partially used TLAB, if available.
+  if (tlab_size < kRegionSize) {
+    // Fetch the largest partial TLAB. The multimap is ordered in decreasing
+    // size.
+    auto largest_partial_tlab = partial_tlabs_.begin();
+    if (largest_partial_tlab != partial_tlabs_.end() && largest_partial_tlab->first >= tlab_size) {
+      r = largest_partial_tlab->second;
+      pos = r->End() - largest_partial_tlab->first;
+      partial_tlabs_.erase(largest_partial_tlab);
+      DCHECK_GT(r->End(), pos);
+      DCHECK_LE(r->Begin(), pos);
+      DCHECK_GE(r->Top(), pos);
+      *bytes_tl_bulk_allocated -= r->Top() - pos;
+    }
+  }
+  if (r == nullptr) {
+    // Fallback to allocating an entire region as TLAB.
+    r = AllocateRegion(/*for_evac=*/ false);
+  }
   if (r != nullptr) {
+    uint8_t* start = pos != nullptr ? pos : r->Begin();
+    DCHECK_ALIGNED(start, kObjectAlignment);
     r->is_a_tlab_ = true;
     r->thread_ = self;
     r->SetTop(r->End());
-    self->SetTlab(r->Begin(), r->Begin() + min_bytes, r->End());
+    self->SetTlab(start, start + tlab_size, r->End());
     return true;
   }
   return false;
@@ -851,22 +878,33 @@ bool RegionSpace::AllocNewTlab(Thread* self, size_t min_bytes) {
 
 size_t RegionSpace::RevokeThreadLocalBuffers(Thread* thread) {
   MutexLock mu(Thread::Current(), region_lock_);
-  RevokeThreadLocalBuffersLocked(thread);
+  RevokeThreadLocalBuffersLocked(thread, /*reuse=*/ gc::Heap::kUsePartialTlabs);
+  return 0U;
+}
+
+size_t RegionSpace::RevokeThreadLocalBuffers(Thread* thread, const bool reuse) {
+  MutexLock mu(Thread::Current(), region_lock_);
+  RevokeThreadLocalBuffersLocked(thread, reuse);
   return 0U;
 }
 
-void RegionSpace::RevokeThreadLocalBuffersLocked(Thread* thread) {
+void RegionSpace::RevokeThreadLocalBuffersLocked(Thread* thread, bool reuse) {
   uint8_t* tlab_start = thread->GetTlabStart();
   DCHECK_EQ(thread->HasTlab(), tlab_start != nullptr);
   if (tlab_start != nullptr) {
-    DCHECK_ALIGNED(tlab_start, kRegionSize);
     Region* r = RefToRegionLocked(reinterpret_cast<mirror::Object*>(tlab_start));
+    r->is_a_tlab_ = false;
+    r->thread_ = nullptr;
     DCHECK(r->IsAllocated());
     DCHECK_LE(thread->GetThreadLocalBytesAllocated(), kRegionSize);
     r->RecordThreadLocalAllocations(thread->GetThreadLocalObjectsAllocated(),
-                                    thread->GetThreadLocalBytesAllocated());
-    r->is_a_tlab_ = false;
-    r->thread_ = nullptr;
+                                    thread->GetTlabEnd() - r->Begin());
+    DCHECK_GE(r->End(), thread->GetTlabPos());
+    DCHECK_LE(r->Begin(), thread->GetTlabPos());
+    size_t remaining_bytes = r->End() - thread->GetTlabPos();
+    if (reuse && remaining_bytes >= gc::Heap::kPartialTlabSize) {
+      partial_tlabs_.insert(std::make_pair(remaining_bytes, r));
+    }
   }
   thread->SetTlab(nullptr, nullptr, nullptr);
 }
diff --git a/runtime/gc/space/region_space.h b/runtime/gc/space/region_space.h
index 26af633..f7036ea 100644
--- a/runtime/gc/space/region_space.h
+++ b/runtime/gc/space/region_space.h
@@ -22,6 +22,9 @@
 #include "space.h"
 #include "thread.h"
 
+#include <functional>
+#include <map>
+
 namespace art {
 namespace gc {
 
@@ -141,7 +144,7 @@ class RegionSpace final : public ContinuousMemMapAllocSpace {
   void DumpNonFreeRegions(std::ostream& os) REQUIRES(!region_lock_);
 
   size_t RevokeThreadLocalBuffers(Thread* thread) override REQUIRES(!region_lock_);
-  void RevokeThreadLocalBuffersLocked(Thread* thread) REQUIRES(region_lock_);
+  size_t RevokeThreadLocalBuffers(Thread* thread, const bool reuse) REQUIRES(!region_lock_);
   size_t RevokeAllThreadLocalBuffers() override
       REQUIRES(!Locks::runtime_shutdown_lock_, !Locks::thread_list_lock_, !region_lock_);
   void AssertThreadLocalBuffersAreRevoked(Thread* thread) REQUIRES(!region_lock_);
@@ -189,6 +192,9 @@ class RegionSpace final : public ContinuousMemMapAllocSpace {
   size_t GetNumRegions() const {
     return num_regions_;
   }
+  size_t GetNumNonFreeRegions() const NO_THREAD_SAFETY_ANALYSIS {
+    return num_non_free_regions_;
+  }
 
   bool CanMoveObjects() const override {
     return true;
@@ -363,7 +369,8 @@ class RegionSpace final : public ContinuousMemMapAllocSpace {
   // Increment object allocation count for region containing ref.
   void RecordAlloc(mirror::Object* ref) REQUIRES(!region_lock_);
 
-  bool AllocNewTlab(Thread* self, size_t min_bytes) REQUIRES(!region_lock_);
+  bool AllocNewTlab(Thread* self, const size_t tlab_size, size_t* bytes_tl_bulk_allocated)
+      REQUIRES(!region_lock_);
 
   uint32_t Time() {
     return time_;
@@ -587,9 +594,8 @@ class RegionSpace final : public ContinuousMemMapAllocSpace {
 
     void RecordThreadLocalAllocations(size_t num_objects, size_t num_bytes) {
       DCHECK(IsAllocated());
-      DCHECK_EQ(objects_allocated_.load(std::memory_order_relaxed), 0U);
       DCHECK_EQ(Top(), end_);
-      objects_allocated_.store(num_objects, std::memory_order_relaxed);
+      objects_allocated_.fetch_add(num_objects, std::memory_order_relaxed);
       top_.store(begin_ + num_bytes, std::memory_order_relaxed);
       DCHECK_LE(Top(), end_);
     }
@@ -691,6 +697,7 @@ class RegionSpace final : public ContinuousMemMapAllocSpace {
   }
 
   Region* AllocateRegion(bool for_evac) REQUIRES(region_lock_);
+  void RevokeThreadLocalBuffersLocked(Thread* thread, bool reuse) REQUIRES(region_lock_);
 
   // Scan region range [`begin`, `end`) in increasing order to try to
   // allocate a large region having a size of `num_regs_in_large_region`
@@ -739,6 +746,9 @@ class RegionSpace final : public ContinuousMemMapAllocSpace {
   // The pointer to the region array.
   std::unique_ptr<Region[]> regions_ GUARDED_BY(region_lock_);
 
+  // To hold partially used TLABs which can be reassigned to threads later for
+  // utilizing the un-used portion.
+  std::multimap<size_t, Region*, std::greater<size_t>> partial_tlabs_ GUARDED_BY(region_lock_);
   // The upper-bound index of the non-free regions. Used to avoid scanning all regions in
   // RegionSpace::SetFromSpace and RegionSpace::ClearFromSpace.
   //
diff --git a/runtime/hidden_api.h b/runtime/hidden_api.h
index e6a0ed3..d76fcb9 100644
--- a/runtime/hidden_api.h
+++ b/runtime/hidden_api.h
@@ -356,6 +356,15 @@ ALWAYS_INLINE inline uint32_t GetRuntimeFlags(ArtMethod* method)
       case Intrinsics::kVarHandleWeakCompareAndSetRelease:
         return 0u;
       case Intrinsics::kUnsafeGetLong:
+      case Intrinsics::kFP16Ceil:
+      case Intrinsics::kFP16Floor:
+      case Intrinsics::kFP16Greater:
+      case Intrinsics::kFP16GreaterEquals:
+      case Intrinsics::kFP16Less:
+      case Intrinsics::kFP16LessEquals:
+      case Intrinsics::kFP16ToFloat:
+      case Intrinsics::kFP16ToHalf:
+      case Intrinsics::kFP16Rint:
         return kAccCorePlatformApi;
       default:
         // Remaining intrinsics are public API. We DCHECK that in SetIntrinsic().
diff --git a/runtime/image.cc b/runtime/image.cc
index b6bb0b1..0858378 100644
--- a/runtime/image.cc
+++ b/runtime/image.cc
@@ -29,7 +29,7 @@
 namespace art {
 
 const uint8_t ImageHeader::kImageMagic[] = { 'a', 'r', 't', '\n' };
-const uint8_t ImageHeader::kImageVersion[] = { '0', '7', '4', '\0' };  // CRC32UpdateBB intrinsic
+const uint8_t ImageHeader::kImageVersion[] = { '0', '8', '4', '\0' };  // FP16 gt/ge/lt/le intrinsic
 
 ImageHeader::ImageHeader(uint32_t image_reservation_size,
                          uint32_t component_count,
diff --git a/runtime/interpreter/interpreter_intrinsics.cc b/runtime/interpreter/interpreter_intrinsics.cc
index c8878e1..63bd967 100644
--- a/runtime/interpreter/interpreter_intrinsics.cc
+++ b/runtime/interpreter/interpreter_intrinsics.cc
@@ -526,7 +526,19 @@ bool MterpHandleIntrinsic(ShadowFrame* shadow_frame,
     UNIMPLEMENTED_CASE(StringBufferAppend /* (Ljava/lang/String;)Ljava/lang/StringBuffer; */)
     UNIMPLEMENTED_CASE(StringBufferLength /* ()I */)
     UNIMPLEMENTED_CASE(StringBufferToString /* ()Ljava/lang/String; */)
-    UNIMPLEMENTED_CASE(StringBuilderAppend /* (Ljava/lang/String;)Ljava/lang/StringBuilder; */)
+    UNIMPLEMENTED_CASE(
+        StringBuilderAppendObject /* (Ljava/lang/Object;)Ljava/lang/StringBuilder; */)
+    UNIMPLEMENTED_CASE(
+        StringBuilderAppendString /* (Ljava/lang/String;)Ljava/lang/StringBuilder; */)
+    UNIMPLEMENTED_CASE(
+        StringBuilderAppendCharSequence /* (Ljava/lang/CharSequence;)Ljava/lang/StringBuilder; */)
+    UNIMPLEMENTED_CASE(StringBuilderAppendCharArray /* ([C)Ljava/lang/StringBuilder; */)
+    UNIMPLEMENTED_CASE(StringBuilderAppendBoolean /* (Z)Ljava/lang/StringBuilder; */)
+    UNIMPLEMENTED_CASE(StringBuilderAppendChar /* (C)Ljava/lang/StringBuilder; */)
+    UNIMPLEMENTED_CASE(StringBuilderAppendInt /* (I)Ljava/lang/StringBuilder; */)
+    UNIMPLEMENTED_CASE(StringBuilderAppendLong /* (J)Ljava/lang/StringBuilder; */)
+    UNIMPLEMENTED_CASE(StringBuilderAppendFloat /* (F)Ljava/lang/StringBuilder; */)
+    UNIMPLEMENTED_CASE(StringBuilderAppendDouble /* (D)Ljava/lang/StringBuilder; */)
     UNIMPLEMENTED_CASE(StringBuilderLength /* ()I */)
     UNIMPLEMENTED_CASE(StringBuilderToString /* ()Ljava/lang/String; */)
     UNIMPLEMENTED_CASE(UnsafeCASInt /* (Ljava/lang/Object;JII)Z */)
@@ -561,6 +573,15 @@ bool MterpHandleIntrinsic(ShadowFrame* shadow_frame,
     UNIMPLEMENTED_CASE(CRC32Update /* (II)I */)
     UNIMPLEMENTED_CASE(CRC32UpdateBytes /* (I[BII)I */)
     UNIMPLEMENTED_CASE(CRC32UpdateByteBuffer /* (IJII)I */)
+    UNIMPLEMENTED_CASE(FP16ToFloat /* (S)F */)
+    UNIMPLEMENTED_CASE(FP16ToHalf /* (F)S */)
+    UNIMPLEMENTED_CASE(FP16Floor /* (S)S */)
+    UNIMPLEMENTED_CASE(FP16Ceil /* (S)S */)
+    UNIMPLEMENTED_CASE(FP16Rint /* (S)S */)
+    UNIMPLEMENTED_CASE(FP16Greater /* (SS)Z */)
+    UNIMPLEMENTED_CASE(FP16GreaterEquals /* (SS)Z */)
+    UNIMPLEMENTED_CASE(FP16Less /* (SS)Z */)
+    UNIMPLEMENTED_CASE(FP16LessEquals /* (SS)Z */)
     INTRINSIC_CASE(VarHandleFullFence)
     INTRINSIC_CASE(VarHandleAcquireFence)
     INTRINSIC_CASE(VarHandleReleaseFence)
diff --git a/runtime/interpreter/interpreter_switch_impl-inl.h b/runtime/interpreter/interpreter_switch_impl-inl.h
index 36cfee4..f74260b 100644
--- a/runtime/interpreter/interpreter_switch_impl-inl.h
+++ b/runtime/interpreter/interpreter_switch_impl-inl.h
@@ -2626,11 +2626,6 @@ ATTRIBUTE_NO_SANITIZE_ADDRESS void ExecuteSwitchImplCpp(SwitchImplContext* ctx)
   Thread* self = ctx->self;
   const CodeItemDataAccessor& accessor = ctx->accessor;
   ShadowFrame& shadow_frame = ctx->shadow_frame;
-  if (UNLIKELY(!shadow_frame.HasReferenceArray())) {
-    LOG(FATAL) << "Invalid shadow frame for interpreter use";
-    ctx->result = JValue();
-    return;
-  }
   self->VerifyStack();
 
   uint32_t dex_pc = shadow_frame.GetDexPC();
diff --git a/runtime/interpreter/shadow_frame-inl.h b/runtime/interpreter/shadow_frame-inl.h
index 7eaad59..799b2d2 100644
--- a/runtime/interpreter/shadow_frame-inl.h
+++ b/runtime/interpreter/shadow_frame-inl.h
@@ -33,9 +33,7 @@ inline void ShadowFrame::SetVRegReference(size_t i, ObjPtr<mirror::Object> val)
   ReadBarrier::MaybeAssertToSpaceInvariant(val.Ptr());
   uint32_t* vreg = &vregs_[i];
   reinterpret_cast<StackReference<mirror::Object>*>(vreg)->Assign(val);
-  if (HasReferenceArray()) {
-    References()[i].Assign(val);
-  }
+  References()[i].Assign(val);
 }
 
 }  // namespace art
diff --git a/runtime/interpreter/shadow_frame.h b/runtime/interpreter/shadow_frame.h
index 3f6b729..bd04793 100644
--- a/runtime/interpreter/shadow_frame.h
+++ b/runtime/interpreter/shadow_frame.h
@@ -92,12 +92,6 @@ class ShadowFrame {
 
   ~ShadowFrame() {}
 
-  // TODO(iam): Clean references array up since they're always there,
-  // we don't need to do conditionals.
-  bool HasReferenceArray() const {
-    return true;
-  }
-
   uint32_t NumberOfVRegs() const {
     return number_of_vregs_;
   }
@@ -152,7 +146,6 @@ class ShadowFrame {
   }
 
   uint32_t* GetShadowRefAddr(size_t i) {
-    DCHECK(HasReferenceArray());
     DCHECK_LT(i, NumberOfVRegs());
     return &vregs_[i + NumberOfVRegs()];
   }
@@ -189,7 +182,6 @@ class ShadowFrame {
   mirror::Object* GetVRegReference(size_t i) const REQUIRES_SHARED(Locks::mutator_lock_) {
     DCHECK_LT(i, NumberOfVRegs());
     mirror::Object* ref;
-    DCHECK(HasReferenceArray());
     ref = References()[i].AsMirrorPtr();
     ReadBarrier::MaybeAssertToSpaceInvariant(ref);
     if (kVerifyFlags & kVerifyReads) {
@@ -209,9 +201,7 @@ class ShadowFrame {
     *reinterpret_cast<int32_t*>(vreg) = val;
     // This is needed for moving collectors since these can update the vreg references if they
     // happen to agree with references in the reference array.
-    if (kMovingCollector && HasReferenceArray()) {
-      References()[i].Clear();
-    }
+    References()[i].Clear();
   }
 
   void SetVRegFloat(size_t i, float val) {
@@ -220,9 +210,7 @@ class ShadowFrame {
     *reinterpret_cast<float*>(vreg) = val;
     // This is needed for moving collectors since these can update the vreg references if they
     // happen to agree with references in the reference array.
-    if (kMovingCollector && HasReferenceArray()) {
-      References()[i].Clear();
-    }
+    References()[i].Clear();
   }
 
   void SetVRegLong(size_t i, int64_t val) {
@@ -232,10 +220,8 @@ class ShadowFrame {
     *reinterpret_cast<unaligned_int64*>(vreg) = val;
     // This is needed for moving collectors since these can update the vreg references if they
     // happen to agree with references in the reference array.
-    if (kMovingCollector && HasReferenceArray()) {
-      References()[i].Clear();
-      References()[i + 1].Clear();
-    }
+    References()[i].Clear();
+    References()[i + 1].Clear();
   }
 
   void SetVRegDouble(size_t i, double val) {
@@ -245,10 +231,8 @@ class ShadowFrame {
     *reinterpret_cast<unaligned_double*>(vreg) = val;
     // This is needed for moving collectors since these can update the vreg references if they
     // happen to agree with references in the reference array.
-    if (kMovingCollector && HasReferenceArray()) {
-      References()[i].Clear();
-      References()[i + 1].Clear();
-    }
+    References()[i].Clear();
+    References()[i + 1].Clear();
   }
 
   template<VerifyObjectFlags kVerifyFlags = kDefaultVerifyFlags>
@@ -271,14 +255,8 @@ class ShadowFrame {
   mirror::Object* GetThisObject(uint16_t num_ins) const REQUIRES_SHARED(Locks::mutator_lock_);
 
   bool Contains(StackReference<mirror::Object>* shadow_frame_entry_obj) const {
-    if (HasReferenceArray()) {
-      return ((&References()[0] <= shadow_frame_entry_obj) &&
-              (shadow_frame_entry_obj <= (&References()[NumberOfVRegs() - 1])));
-    } else {
-      uint32_t* shadow_frame_entry = reinterpret_cast<uint32_t*>(shadow_frame_entry_obj);
-      return ((&vregs_[0] <= shadow_frame_entry) &&
-              (shadow_frame_entry <= (&vregs_[NumberOfVRegs() - 1])));
-    }
+    return ((&References()[0] <= shadow_frame_entry_obj) &&
+            (shadow_frame_entry_obj <= (&References()[NumberOfVRegs() - 1])));
   }
 
   LockCountData& GetLockCountData() {
@@ -335,7 +313,7 @@ class ShadowFrame {
                                             ArtMethod* method,
                                             uint32_t dex_pc,
                                             void* memory) {
-    return new (memory) ShadowFrame(num_vregs, link, method, dex_pc, true);
+    return new (memory) ShadowFrame(num_vregs, link, method, dex_pc);
   }
 
   const uint16_t* GetDexPCPtr() {
@@ -386,8 +364,7 @@ class ShadowFrame {
   }
 
  private:
-  ShadowFrame(uint32_t num_vregs, ShadowFrame* link, ArtMethod* method,
-              uint32_t dex_pc, bool has_reference_array)
+  ShadowFrame(uint32_t num_vregs, ShadowFrame* link, ArtMethod* method, uint32_t dex_pc)
       : link_(link),
         method_(method),
         result_register_(nullptr),
@@ -398,13 +375,7 @@ class ShadowFrame {
         cached_hotness_countdown_(0),
         hotness_countdown_(0),
         frame_flags_(0) {
-    // TODO(iam): Remove this parameter, it's an an artifact of portable removal
-    DCHECK(has_reference_array);
-    if (has_reference_array) {
-      memset(vregs_, 0, num_vregs * (sizeof(uint32_t) + sizeof(StackReference<mirror::Object>)));
-    } else {
-      memset(vregs_, 0, num_vregs * sizeof(uint32_t));
-    }
+    memset(vregs_, 0, num_vregs * (sizeof(uint32_t) + sizeof(StackReference<mirror::Object>)));
   }
 
   void UpdateFrameFlag(bool enable, FrameFlags flag) {
@@ -420,7 +391,6 @@ class ShadowFrame {
   }
 
   const StackReference<mirror::Object>* References() const {
-    DCHECK(HasReferenceArray());
     const uint32_t* vreg_end = &vregs_[NumberOfVRegs()];
     return reinterpret_cast<const StackReference<mirror::Object>*>(vreg_end);
   }
diff --git a/runtime/intrinsics_list.h b/runtime/intrinsics_list.h
index 57e81a7..fc4734e 100644
--- a/runtime/intrinsics_list.h
+++ b/runtime/intrinsics_list.h
@@ -165,6 +165,15 @@
   V(MemoryPokeIntNative, kStatic, kNeedsEnvironmentOrCache, kWriteSideEffects, kCanThrow, "Llibcore/io/Memory;", "pokeIntNative", "(JI)V") \
   V(MemoryPokeLongNative, kStatic, kNeedsEnvironmentOrCache, kWriteSideEffects, kCanThrow, "Llibcore/io/Memory;", "pokeLongNative", "(JJ)V") \
   V(MemoryPokeShortNative, kStatic, kNeedsEnvironmentOrCache, kWriteSideEffects, kCanThrow, "Llibcore/io/Memory;", "pokeShortNative", "(JS)V") \
+  V(FP16Ceil, kStatic, kNeedsEnvironmentOrCache, kNoSideEffects, kNoThrow, "Llibcore/util/FP16;", "ceil", "(S)S") \
+  V(FP16Floor, kStatic, kNeedsEnvironmentOrCache, kNoSideEffects, kNoThrow, "Llibcore/util/FP16;", "floor", "(S)S") \
+  V(FP16Rint, kStatic, kNeedsEnvironmentOrCache, kNoSideEffects, kNoThrow, "Llibcore/util/FP16;", "rint", "(S)S") \
+  V(FP16ToFloat, kStatic, kNeedsEnvironmentOrCache, kNoSideEffects, kNoThrow, "Llibcore/util/FP16;", "toFloat", "(S)F") \
+  V(FP16ToHalf, kStatic, kNeedsEnvironmentOrCache, kNoSideEffects, kNoThrow, "Llibcore/util/FP16;", "toHalf", "(F)S") \
+  V(FP16Greater, kStatic, kNeedsEnvironmentOrCache, kNoSideEffects, kNoThrow, "Llibcore/util/FP16;", "greater", "(SS)Z") \
+  V(FP16GreaterEquals, kStatic, kNeedsEnvironmentOrCache, kNoSideEffects, kNoThrow, "Llibcore/util/FP16;", "greaterEquals", "(SS)Z") \
+  V(FP16Less, kStatic, kNeedsEnvironmentOrCache, kNoSideEffects, kNoThrow, "Llibcore/util/FP16;", "less", "(SS)Z") \
+  V(FP16LessEquals, kStatic, kNeedsEnvironmentOrCache, kNoSideEffects, kNoThrow, "Llibcore/util/FP16;", "lessEquals", "(SS)Z") \
   V(StringCharAt, kVirtual, kNeedsEnvironmentOrCache, kReadSideEffects, kCanThrow, "Ljava/lang/String;", "charAt", "(I)C") \
   V(StringCompareTo, kVirtual, kNeedsEnvironmentOrCache, kReadSideEffects, kCanThrow, "Ljava/lang/String;", "compareTo", "(Ljava/lang/String;)I") \
   V(StringEquals, kVirtual, kNeedsEnvironmentOrCache, kReadSideEffects, kCanThrow, "Ljava/lang/String;", "equals", "(Ljava/lang/Object;)Z") \
@@ -181,7 +190,16 @@
   V(StringBufferAppend, kVirtual, kNeedsEnvironmentOrCache, kAllSideEffects, kCanThrow, "Ljava/lang/StringBuffer;", "append", "(Ljava/lang/String;)Ljava/lang/StringBuffer;") \
   V(StringBufferLength, kVirtual, kNeedsEnvironmentOrCache, kAllSideEffects, kNoThrow, "Ljava/lang/StringBuffer;", "length", "()I") \
   V(StringBufferToString, kVirtual, kNeedsEnvironmentOrCache, kAllSideEffects, kCanThrow, "Ljava/lang/StringBuffer;", "toString", "()Ljava/lang/String;") \
-  V(StringBuilderAppend, kVirtual, kNeedsEnvironmentOrCache, kAllSideEffects, kCanThrow, "Ljava/lang/StringBuilder;", "append", "(Ljava/lang/String;)Ljava/lang/StringBuilder;") \
+  V(StringBuilderAppendObject, kVirtual, kNeedsEnvironmentOrCache, kAllSideEffects, kCanThrow, "Ljava/lang/StringBuilder;", "append", "(Ljava/lang/Object;)Ljava/lang/StringBuilder;") \
+  V(StringBuilderAppendString, kVirtual, kNeedsEnvironmentOrCache, kAllSideEffects, kCanThrow, "Ljava/lang/StringBuilder;", "append", "(Ljava/lang/String;)Ljava/lang/StringBuilder;") \
+  V(StringBuilderAppendCharSequence, kVirtual, kNeedsEnvironmentOrCache, kAllSideEffects, kCanThrow, "Ljava/lang/StringBuilder;", "append", "(Ljava/lang/CharSequence;)Ljava/lang/StringBuilder;") \
+  V(StringBuilderAppendCharArray, kVirtual, kNeedsEnvironmentOrCache, kAllSideEffects, kCanThrow, "Ljava/lang/StringBuilder;", "append", "([C)Ljava/lang/StringBuilder;") \
+  V(StringBuilderAppendBoolean, kVirtual, kNeedsEnvironmentOrCache, kAllSideEffects, kCanThrow, "Ljava/lang/StringBuilder;", "append", "(Z)Ljava/lang/StringBuilder;") \
+  V(StringBuilderAppendChar, kVirtual, kNeedsEnvironmentOrCache, kAllSideEffects, kCanThrow, "Ljava/lang/StringBuilder;", "append", "(C)Ljava/lang/StringBuilder;") \
+  V(StringBuilderAppendInt, kVirtual, kNeedsEnvironmentOrCache, kAllSideEffects, kCanThrow, "Ljava/lang/StringBuilder;", "append", "(I)Ljava/lang/StringBuilder;") \
+  V(StringBuilderAppendLong, kVirtual, kNeedsEnvironmentOrCache, kAllSideEffects, kCanThrow, "Ljava/lang/StringBuilder;", "append", "(J)Ljava/lang/StringBuilder;") \
+  V(StringBuilderAppendFloat, kVirtual, kNeedsEnvironmentOrCache, kAllSideEffects, kCanThrow, "Ljava/lang/StringBuilder;", "append", "(F)Ljava/lang/StringBuilder;") \
+  V(StringBuilderAppendDouble, kVirtual, kNeedsEnvironmentOrCache, kAllSideEffects, kCanThrow, "Ljava/lang/StringBuilder;", "append", "(D)Ljava/lang/StringBuilder;") \
   V(StringBuilderLength, kVirtual, kNeedsEnvironmentOrCache, kReadSideEffects, kNoThrow, "Ljava/lang/StringBuilder;", "length", "()I") \
   V(StringBuilderToString, kVirtual, kNeedsEnvironmentOrCache, kAllSideEffects, kCanThrow, "Ljava/lang/StringBuilder;", "toString", "()Ljava/lang/String;") \
   V(UnsafeCASInt, kVirtual, kNeedsEnvironmentOrCache, kAllSideEffects, kCanThrow, "Lsun/misc/Unsafe;", "compareAndSwapInt", "(Ljava/lang/Object;JII)Z") \
diff --git a/runtime/mirror/string.h b/runtime/mirror/string.h
index c98aa6b..4dcec1a 100644
--- a/runtime/mirror/string.h
+++ b/runtime/mirror/string.h
@@ -27,6 +27,7 @@ namespace art {
 
 template<class T> class Handle;
 template<class MirrorType> class ObjPtr;
+class StringBuilderAppend;
 struct StringOffsets;
 class StubTest_ReadBarrierForRoot_Test;
 
@@ -269,6 +270,7 @@ class MANAGED String final : public Object {
     uint8_t value_compressed_[0];
   };
 
+  friend class art::StringBuilderAppend;
   friend struct art::StringOffsets;  // for verifying offset information
 
   DISALLOW_IMPLICIT_CONSTRUCTORS(String);
diff --git a/runtime/native/dalvik_system_DexFile.cc b/runtime/native/dalvik_system_DexFile.cc
index eee8cfc..6dfe62e 100644
--- a/runtime/native/dalvik_system_DexFile.cc
+++ b/runtime/native/dalvik_system_DexFile.cc
@@ -897,7 +897,7 @@ static void DexFile_setTrusted(JNIEnv* env, jclass, jobject j_cookie) {
   }
 }
 
-static JNINativeMethod gMethods[] = {
+static const JNINativeMethod gMethods[] = {
   NATIVE_METHOD(DexFile, closeDexFile, "(Ljava/lang/Object;)Z"),
   NATIVE_METHOD(DexFile,
                 defineClassNative,
diff --git a/runtime/native/dalvik_system_VMDebug.cc b/runtime/native/dalvik_system_VMDebug.cc
index 83398ec..5b08fa7 100644
--- a/runtime/native/dalvik_system_VMDebug.cc
+++ b/runtime/native/dalvik_system_VMDebug.cc
@@ -623,7 +623,7 @@ static void VMDebug_setAllocTrackerStackDepth(JNIEnv* env, jclass, jint stack_de
   }
 }
 
-static JNINativeMethod gMethods[] = {
+static const JNINativeMethod gMethods[] = {
   NATIVE_METHOD(VMDebug, countInstancesOfClass, "(Ljava/lang/Class;Z)J"),
   NATIVE_METHOD(VMDebug, countInstancesOfClasses, "([Ljava/lang/Class;Z)[J"),
   NATIVE_METHOD(VMDebug, crash, "()V"),
diff --git a/runtime/native/dalvik_system_VMRuntime.cc b/runtime/native/dalvik_system_VMRuntime.cc
index 399813c..36569c1 100644
--- a/runtime/native/dalvik_system_VMRuntime.cc
+++ b/runtime/native/dalvik_system_VMRuntime.cc
@@ -723,7 +723,7 @@ static jboolean VMRuntime_hasBootImageSpaces(JNIEnv* env ATTRIBUTE_UNUSED,
   return Runtime::Current()->GetHeap()->HasBootImageSpace() ? JNI_TRUE : JNI_FALSE;
 }
 
-static JNINativeMethod gMethods[] = {
+static const JNINativeMethod gMethods[] = {
   FAST_NATIVE_METHOD(VMRuntime, addressOf, "(Ljava/lang/Object;)J"),
   NATIVE_METHOD(VMRuntime, bootClassPath, "()Ljava/lang/String;"),
   NATIVE_METHOD(VMRuntime, clampGrowthLimit, "()V"),
diff --git a/runtime/native/dalvik_system_VMStack.cc b/runtime/native/dalvik_system_VMStack.cc
index 32733a8..33d352f 100644
--- a/runtime/native/dalvik_system_VMStack.cc
+++ b/runtime/native/dalvik_system_VMStack.cc
@@ -169,7 +169,7 @@ static jobjectArray VMStack_getAnnotatedThreadStackTrace(JNIEnv* env, jclass, jo
   return GetThreadStack(soa, javaThread, fn);
 }
 
-static JNINativeMethod gMethods[] = {
+static const JNINativeMethod gMethods[] = {
   FAST_NATIVE_METHOD(VMStack, fillStackTraceElements, "(Ljava/lang/Thread;[Ljava/lang/StackTraceElement;)I"),
   FAST_NATIVE_METHOD(VMStack, getCallingClassLoader, "()Ljava/lang/ClassLoader;"),
   FAST_NATIVE_METHOD(VMStack, getClosestUserClassLoader, "()Ljava/lang/ClassLoader;"),
diff --git a/runtime/native/dalvik_system_ZygoteHooks.cc b/runtime/native/dalvik_system_ZygoteHooks.cc
index de28c28..1334df7 100644
--- a/runtime/native/dalvik_system_ZygoteHooks.cc
+++ b/runtime/native/dalvik_system_ZygoteHooks.cc
@@ -420,7 +420,7 @@ static void ZygoteHooks_stopZygoteNoThreadCreation(JNIEnv* env ATTRIBUTE_UNUSED,
   Runtime::Current()->SetZygoteNoThreadSection(false);
 }
 
-static JNINativeMethod gMethods[] = {
+static const JNINativeMethod gMethods[] = {
   NATIVE_METHOD(ZygoteHooks, nativePreFork, "()J"),
   NATIVE_METHOD(ZygoteHooks, nativePostZygoteFork, "()V"),
   NATIVE_METHOD(ZygoteHooks, nativePostForkSystemServer, "()V"),
diff --git a/runtime/native/java_lang_Class.cc b/runtime/native/java_lang_Class.cc
index 2b75c59..0511875 100644
--- a/runtime/native/java_lang_Class.cc
+++ b/runtime/native/java_lang_Class.cc
@@ -870,7 +870,7 @@ static jobject Class_newInstance(JNIEnv* env, jobject javaThis) {
   return soa.AddLocalReference<jobject>(receiver.Get());
 }
 
-static JNINativeMethod gMethods[] = {
+static const JNINativeMethod gMethods[] = {
   FAST_NATIVE_METHOD(Class, classForName,
                 "(Ljava/lang/String;ZLjava/lang/ClassLoader;)Ljava/lang/Class;"),
   FAST_NATIVE_METHOD(Class, getDeclaredAnnotation,
diff --git a/runtime/native/java_lang_Object.cc b/runtime/native/java_lang_Object.cc
index 48540f8..aaf934e 100644
--- a/runtime/native/java_lang_Object.cc
+++ b/runtime/native/java_lang_Object.cc
@@ -52,7 +52,7 @@ static jint Object_identityHashCodeNative(JNIEnv* env, jclass, jobject javaObjec
   return static_cast<jint>(o->IdentityHashCode());
 }
 
-static JNINativeMethod gMethods[] = {
+static const JNINativeMethod gMethods[] = {
   FAST_NATIVE_METHOD(Object, internalClone, "()Ljava/lang/Object;"),
   FAST_NATIVE_METHOD(Object, notify, "()V"),
   FAST_NATIVE_METHOD(Object, notifyAll, "()V"),
diff --git a/runtime/native/java_lang_String.cc b/runtime/native/java_lang_String.cc
index 83498f6..80c8dbc 100644
--- a/runtime/native/java_lang_String.cc
+++ b/runtime/native/java_lang_String.cc
@@ -109,7 +109,7 @@ static jcharArray String_toCharArray(JNIEnv* env, jobject java_this) {
   return soa.AddLocalReference<jcharArray>(s->ToCharArray(soa.Self()));
 }
 
-static JNINativeMethod gMethods[] = {
+static const JNINativeMethod gMethods[] = {
   FAST_NATIVE_METHOD(String, charAt, "(I)C"),
   FAST_NATIVE_METHOD(String, compareTo, "(Ljava/lang/String;)I"),
   FAST_NATIVE_METHOD(String, concat, "(Ljava/lang/String;)Ljava/lang/String;"),
diff --git a/runtime/native/java_lang_StringFactory.cc b/runtime/native/java_lang_StringFactory.cc
index 13f8d5b..8e59466 100644
--- a/runtime/native/java_lang_StringFactory.cc
+++ b/runtime/native/java_lang_StringFactory.cc
@@ -89,7 +89,7 @@ static jstring StringFactory_newStringFromString(JNIEnv* env, jclass, jstring to
   return soa.AddLocalReference<jstring>(result);
 }
 
-static JNINativeMethod gMethods[] = {
+static const JNINativeMethod gMethods[] = {
   FAST_NATIVE_METHOD(StringFactory, newStringFromBytes, "([BIII)Ljava/lang/String;"),
   FAST_NATIVE_METHOD(StringFactory, newStringFromChars, "(II[C)Ljava/lang/String;"),
   FAST_NATIVE_METHOD(StringFactory, newStringFromString, "(Ljava/lang/String;)Ljava/lang/String;"),
diff --git a/runtime/native/java_lang_System.cc b/runtime/native/java_lang_System.cc
index 63cbd2c..e7b3894 100644
--- a/runtime/native/java_lang_System.cc
+++ b/runtime/native/java_lang_System.cc
@@ -239,7 +239,7 @@ static void System_arraycopyBooleanUnchecked(JNIEnv* env,
       javaDst, dstPos, count);
 }
 
-static JNINativeMethod gMethods[] = {
+static const JNINativeMethod gMethods[] = {
   FAST_NATIVE_METHOD(System, arraycopy, "(Ljava/lang/Object;ILjava/lang/Object;II)V"),
   FAST_NATIVE_METHOD(System, arraycopyCharUnchecked, "([CI[CII)V"),
   FAST_NATIVE_METHOD(System, arraycopyByteUnchecked, "([BI[BII)V"),
diff --git a/runtime/native/java_lang_Thread.cc b/runtime/native/java_lang_Thread.cc
index 37b3fe6..5f3f54a 100644
--- a/runtime/native/java_lang_Thread.cc
+++ b/runtime/native/java_lang_Thread.cc
@@ -194,7 +194,7 @@ static void Thread_yield(JNIEnv*, jobject) {
   sched_yield();
 }
 
-static JNINativeMethod gMethods[] = {
+static const JNINativeMethod gMethods[] = {
   FAST_NATIVE_METHOD(Thread, currentThread, "()Ljava/lang/Thread;"),
   FAST_NATIVE_METHOD(Thread, interrupted, "()Z"),
   FAST_NATIVE_METHOD(Thread, isInterrupted, "()Z"),
diff --git a/runtime/native/java_lang_Throwable.cc b/runtime/native/java_lang_Throwable.cc
index b5ef7d8..7a6a44b 100644
--- a/runtime/native/java_lang_Throwable.cc
+++ b/runtime/native/java_lang_Throwable.cc
@@ -38,7 +38,7 @@ static jobjectArray Throwable_nativeGetStackTrace(JNIEnv* env, jclass, jobject j
   return Thread::InternalStackTraceToStackTraceElementArray(soa, javaStackState);
 }
 
-static JNINativeMethod gMethods[] = {
+static const JNINativeMethod gMethods[] = {
   FAST_NATIVE_METHOD(Throwable, nativeFillInStackTrace, "()Ljava/lang/Object;"),
   FAST_NATIVE_METHOD(Throwable, nativeGetStackTrace, "(Ljava/lang/Object;)[Ljava/lang/StackTraceElement;"),
 };
diff --git a/runtime/native/java_lang_VMClassLoader.cc b/runtime/native/java_lang_VMClassLoader.cc
index 11e02a2..ce37a33 100644
--- a/runtime/native/java_lang_VMClassLoader.cc
+++ b/runtime/native/java_lang_VMClassLoader.cc
@@ -155,7 +155,7 @@ static jobjectArray VMClassLoader_getBootClassPathEntries(JNIEnv* env, jclass) {
   return array;
 }
 
-static JNINativeMethod gMethods[] = {
+static const JNINativeMethod gMethods[] = {
   FAST_NATIVE_METHOD(VMClassLoader, findLoadedClass, "(Ljava/lang/ClassLoader;Ljava/lang/String;)Ljava/lang/Class;"),
   NATIVE_METHOD(VMClassLoader, getBootClassPathEntries, "()[Ljava/lang/String;"),
 };
diff --git a/runtime/native/java_lang_invoke_MethodHandleImpl.cc b/runtime/native/java_lang_invoke_MethodHandleImpl.cc
index 0b26bd7..6662fe4 100644
--- a/runtime/native/java_lang_invoke_MethodHandleImpl.cc
+++ b/runtime/native/java_lang_invoke_MethodHandleImpl.cc
@@ -68,7 +68,7 @@ static jobject MethodHandleImpl_getMemberInternal(JNIEnv* env, jobject thiz) {
   return soa.AddLocalReference<jobject>(h_object.Get());
 }
 
-static JNINativeMethod gMethods[] = {
+static const JNINativeMethod gMethods[] = {
   NATIVE_METHOD(MethodHandleImpl, getMemberInternal, "()Ljava/lang/reflect/Member;"),
 };
 
diff --git a/runtime/native/java_lang_ref_FinalizerReference.cc b/runtime/native/java_lang_ref_FinalizerReference.cc
index 535b243..0a8dfb6 100644
--- a/runtime/native/java_lang_ref_FinalizerReference.cc
+++ b/runtime/native/java_lang_ref_FinalizerReference.cc
@@ -42,7 +42,7 @@ static jobject FinalizerReference_getReferent(JNIEnv* env, jobject javaThis) {
   return soa.AddLocalReference<jobject>(referent);
 }
 
-static JNINativeMethod gMethods[] = {
+static const JNINativeMethod gMethods[] = {
   FAST_NATIVE_METHOD(FinalizerReference, makeCircularListIfUnenqueued, "()Z"),
   FAST_NATIVE_METHOD(FinalizerReference, getReferent, "()Ljava/lang/Object;"),
 };
diff --git a/runtime/native/java_lang_ref_Reference.cc b/runtime/native/java_lang_ref_Reference.cc
index b241b1d..89a3818 100644
--- a/runtime/native/java_lang_ref_Reference.cc
+++ b/runtime/native/java_lang_ref_Reference.cc
@@ -42,7 +42,7 @@ static void Reference_clearReferent(JNIEnv* env, jobject javaThis) {
   Runtime::Current()->GetHeap()->GetReferenceProcessor()->ClearReferent(ref);
 }
 
-static JNINativeMethod gMethods[] = {
+static const JNINativeMethod gMethods[] = {
   FAST_NATIVE_METHOD(Reference, getReferent, "()Ljava/lang/Object;"),
   FAST_NATIVE_METHOD(Reference, clearReferent, "()V"),
 };
diff --git a/runtime/native/java_lang_reflect_Array.cc b/runtime/native/java_lang_reflect_Array.cc
index ff94593..e359e7d 100644
--- a/runtime/native/java_lang_reflect_Array.cc
+++ b/runtime/native/java_lang_reflect_Array.cc
@@ -74,7 +74,7 @@ static jobject Array_createObjectArray(JNIEnv* env, jclass, jclass javaElementCl
   return soa.AddLocalReference<jobject>(new_array);
 }
 
-static JNINativeMethod gMethods[] = {
+static const JNINativeMethod gMethods[] = {
   FAST_NATIVE_METHOD(Array, createMultiArray, "(Ljava/lang/Class;[I)Ljava/lang/Object;"),
   FAST_NATIVE_METHOD(Array, createObjectArray, "(Ljava/lang/Class;I)Ljava/lang/Object;"),
 };
diff --git a/runtime/native/java_lang_reflect_Constructor.cc b/runtime/native/java_lang_reflect_Constructor.cc
index f9cdc36..bc8c38d 100644
--- a/runtime/native/java_lang_reflect_Constructor.cc
+++ b/runtime/native/java_lang_reflect_Constructor.cc
@@ -128,7 +128,7 @@ static jobject Constructor_newInstanceFromSerialization(JNIEnv* env, jclass unus
     return env->NewObject(allocClass, ctor);
 }
 
-static JNINativeMethod gMethods[] = {
+static const JNINativeMethod gMethods[] = {
   FAST_NATIVE_METHOD(Constructor, getExceptionTypes, "()[Ljava/lang/Class;"),
   FAST_NATIVE_METHOD(Constructor, newInstance0, "([Ljava/lang/Object;)Ljava/lang/Object;"),
   FAST_NATIVE_METHOD(Constructor, newInstanceFromSerialization, "(Ljava/lang/Class;Ljava/lang/Class;)Ljava/lang/Object;"),
diff --git a/runtime/native/java_lang_reflect_Executable.cc b/runtime/native/java_lang_reflect_Executable.cc
index 2ce56b5..4ba93e3 100644
--- a/runtime/native/java_lang_reflect_Executable.cc
+++ b/runtime/native/java_lang_reflect_Executable.cc
@@ -383,7 +383,7 @@ static jint Executable_getParameterCountInternal(JNIEnv* env, jobject javaMethod
 }
 
 
-static JNINativeMethod gMethods[] = {
+static const JNINativeMethod gMethods[] = {
   FAST_NATIVE_METHOD(Executable, compareMethodParametersInternal,
                      "(Ljava/lang/reflect/Method;)I"),
   FAST_NATIVE_METHOD(Executable, getAnnotationNative,
diff --git a/runtime/native/java_lang_reflect_Field.cc b/runtime/native/java_lang_reflect_Field.cc
index f21ded9..eefb3cc 100644
--- a/runtime/native/java_lang_reflect_Field.cc
+++ b/runtime/native/java_lang_reflect_Field.cc
@@ -505,7 +505,7 @@ static jboolean Field_isAnnotationPresentNative(JNIEnv* env,
   return annotations::IsFieldAnnotationPresent(field, klass);
 }
 
-static JNINativeMethod gMethods[] = {
+static const JNINativeMethod gMethods[] = {
   FAST_NATIVE_METHOD(Field, get,        "(Ljava/lang/Object;)Ljava/lang/Object;"),
   FAST_NATIVE_METHOD(Field, getBoolean, "(Ljava/lang/Object;)Z"),
   FAST_NATIVE_METHOD(Field, getByte,    "(Ljava/lang/Object;)B"),
diff --git a/runtime/native/java_lang_reflect_Method.cc b/runtime/native/java_lang_reflect_Method.cc
index 4525157..e435ee7 100644
--- a/runtime/native/java_lang_reflect_Method.cc
+++ b/runtime/native/java_lang_reflect_Method.cc
@@ -84,7 +84,7 @@ static jobject Method_invoke(JNIEnv* env, jobject javaMethod, jobject javaReceiv
   return InvokeMethod(soa, javaMethod, javaReceiver, javaArgs);
 }
 
-static JNINativeMethod gMethods[] = {
+static const JNINativeMethod gMethods[] = {
   FAST_NATIVE_METHOD(Method, getDefaultValue, "()Ljava/lang/Object;"),
   FAST_NATIVE_METHOD(Method, getExceptionTypes, "()[Ljava/lang/Class;"),
   FAST_NATIVE_METHOD(Method, invoke, "(Ljava/lang/Object;[Ljava/lang/Object;)Ljava/lang/Object;"),
diff --git a/runtime/native/java_lang_reflect_Parameter.cc b/runtime/native/java_lang_reflect_Parameter.cc
index 263a567..92bf907 100644
--- a/runtime/native/java_lang_reflect_Parameter.cc
+++ b/runtime/native/java_lang_reflect_Parameter.cc
@@ -98,7 +98,7 @@ static jobject Parameter_getAnnotationNative(JNIEnv* env,
       annotations::GetAnnotationForMethodParameter(method, parameterIndex, klass));
 }
 
-static JNINativeMethod gMethods[] = {
+static const JNINativeMethod gMethods[] = {
   FAST_NATIVE_METHOD(
       Parameter,
       getAnnotationNative,
diff --git a/runtime/native/java_lang_reflect_Proxy.cc b/runtime/native/java_lang_reflect_Proxy.cc
index f723ed2..c2b533d 100644
--- a/runtime/native/java_lang_reflect_Proxy.cc
+++ b/runtime/native/java_lang_reflect_Proxy.cc
@@ -37,7 +37,7 @@ static jclass Proxy_generateProxy(JNIEnv* env, jclass, jstring name, jobjectArra
       soa, name, interfaces, loader, methods, throws));
 }
 
-static JNINativeMethod gMethods[] = {
+static const JNINativeMethod gMethods[] = {
   FAST_NATIVE_METHOD(Proxy, generateProxy, "(Ljava/lang/String;[Ljava/lang/Class;Ljava/lang/ClassLoader;[Ljava/lang/reflect/Method;[[Ljava/lang/Class;)Ljava/lang/Class;"),
 };
 
diff --git a/runtime/native/java_util_concurrent_atomic_AtomicLong.cc b/runtime/native/java_util_concurrent_atomic_AtomicLong.cc
index fa288ed..299ac5a 100644
--- a/runtime/native/java_util_concurrent_atomic_AtomicLong.cc
+++ b/runtime/native/java_util_concurrent_atomic_AtomicLong.cc
@@ -30,7 +30,7 @@ static jboolean AtomicLong_VMSupportsCS8(JNIEnv*, jclass) {
   return QuasiAtomic::LongAtomicsUseMutexes(kRuntimeISA) ? JNI_FALSE : JNI_TRUE;
 }
 
-static JNINativeMethod gMethods[] = {
+static const JNINativeMethod gMethods[] = {
   NATIVE_METHOD(AtomicLong, VMSupportsCS8, "()Z"),
 };
 
diff --git a/runtime/native/libcore_util_CharsetUtils.cc b/runtime/native/libcore_util_CharsetUtils.cc
index 95e0d79..b86bda4 100644
--- a/runtime/native/libcore_util_CharsetUtils.cc
+++ b/runtime/native/libcore_util_CharsetUtils.cc
@@ -252,7 +252,7 @@ static jbyteArray CharsetUtils_toUtf8Bytes(JNIEnv* env, jclass, jstring java_str
   return out.toByteArray();
 }
 
-static JNINativeMethod gMethods[] = {
+static const JNINativeMethod gMethods[] = {
   FAST_NATIVE_METHOD(CharsetUtils, asciiBytesToChars, "([BII[C)V"),
   FAST_NATIVE_METHOD(CharsetUtils, isoLatin1BytesToChars, "([BII[C)V"),
   FAST_NATIVE_METHOD(CharsetUtils, toAsciiBytes, "(Ljava/lang/String;II)[B"),
diff --git a/runtime/native/org_apache_harmony_dalvik_ddmc_DdmServer.cc b/runtime/native/org_apache_harmony_dalvik_ddmc_DdmServer.cc
index 419aed8..ccbef4f 100644
--- a/runtime/native/org_apache_harmony_dalvik_ddmc_DdmServer.cc
+++ b/runtime/native/org_apache_harmony_dalvik_ddmc_DdmServer.cc
@@ -38,7 +38,7 @@ static void DdmServer_nativeSendChunk(JNIEnv* env, jclass, jint type,
   Runtime::Current()->GetRuntimeCallbacks()->DdmPublishChunk(static_cast<uint32_t>(type), chunk);
 }
 
-static JNINativeMethod gMethods[] = {
+static const JNINativeMethod gMethods[] = {
   FAST_NATIVE_METHOD(DdmServer, nativeSendChunk, "(I[BII)V"),
 };
 
diff --git a/runtime/native/org_apache_harmony_dalvik_ddmc_DdmVmInternal.cc b/runtime/native/org_apache_harmony_dalvik_ddmc_DdmVmInternal.cc
index 028675d..2ca9089 100644
--- a/runtime/native/org_apache_harmony_dalvik_ddmc_DdmVmInternal.cc
+++ b/runtime/native/org_apache_harmony_dalvik_ddmc_DdmVmInternal.cc
@@ -175,7 +175,7 @@ static void DdmVmInternal_threadNotify(JNIEnv*, jclass, jboolean enable) {
   Dbg::DdmSetThreadNotification(enable);
 }
 
-static JNINativeMethod gMethods[] = {
+static const JNINativeMethod gMethods[] = {
   NATIVE_METHOD(DdmVmInternal, enableRecentAllocations, "(Z)V"),
   FAST_NATIVE_METHOD(DdmVmInternal, getRecentAllocations, "()[B"),
   FAST_NATIVE_METHOD(DdmVmInternal, getRecentAllocationStatus, "()Z"),
diff --git a/runtime/native/sun_misc_Unsafe.cc b/runtime/native/sun_misc_Unsafe.cc
index 5014f34..bb9226d 100644
--- a/runtime/native/sun_misc_Unsafe.cc
+++ b/runtime/native/sun_misc_Unsafe.cc
@@ -541,7 +541,7 @@ static void Unsafe_unpark(JNIEnv* env, jobject, jobject jthread) {
   }
 }
 
-static JNINativeMethod gMethods[] = {
+static const JNINativeMethod gMethods[] = {
   FAST_NATIVE_METHOD(Unsafe, compareAndSwapInt, "(Ljava/lang/Object;JII)Z"),
   FAST_NATIVE_METHOD(Unsafe, compareAndSwapLong, "(Ljava/lang/Object;JJJ)Z"),
   FAST_NATIVE_METHOD(Unsafe, compareAndSwapObject, "(Ljava/lang/Object;JLjava/lang/Object;Ljava/lang/Object;)Z"),
diff --git a/runtime/oat.h b/runtime/oat.h
index 15059a8..f4b5a6e 100644
--- a/runtime/oat.h
+++ b/runtime/oat.h
@@ -32,8 +32,8 @@ class InstructionSetFeatures;
 class PACKED(4) OatHeader {
  public:
   static constexpr std::array<uint8_t, 4> kOatMagic { { 'o', 'a', 't', '\n' } };
-  // Last oat version changed reason: Remove unused trampoline entrypoints.
-  static constexpr std::array<uint8_t, 4> kOatVersion { { '1', '7', '0', '\0' } };
+  // Last oat version changed reason: Optimize stack maps: add fast path for no inline info.
+  static constexpr std::array<uint8_t, 4> kOatVersion { { '1', '7', '1', '\0' } };
 
   static constexpr const char* kDex2OatCmdLineKey = "dex2oat-cmdline";
   static constexpr const char* kDebuggableKey = "debuggable";
diff --git a/runtime/stack.cc b/runtime/stack.cc
index 80a563b..8b0c7ce 100644
--- a/runtime/stack.cc
+++ b/runtime/stack.cc
@@ -813,8 +813,8 @@ void StackVisitor::WalkStack(bool include_transitions) {
         if ((walk_kind_ == StackWalkKind::kIncludeInlinedFrames)
             && (cur_oat_quick_method_header_ != nullptr)
             && cur_oat_quick_method_header_->IsOptimized()
-            // JNI methods cannot have any inlined frames.
-            && !method->IsNative()) {
+            && !method->IsNative()  // JNI methods cannot have any inlined frames.
+            && CodeInfo::HasInlineInfo(cur_oat_quick_method_header_->GetOptimizedCodeInfoPtr())) {
           DCHECK_NE(cur_quick_frame_pc_, 0u);
           current_code_info_ = CodeInfo(cur_oat_quick_method_header_,
                                         CodeInfo::DecodeFlags::InlineInfoOnly);
diff --git a/runtime/stack_map.cc b/runtime/stack_map.cc
index 62dec15..eef7378 100644
--- a/runtime/stack_map.cc
+++ b/runtime/stack_map.cc
@@ -35,7 +35,7 @@ CodeInfo::CodeInfo(const OatQuickMethodHeader* header, DecodeFlags flags)
 template<typename Accessor>
 ALWAYS_INLINE static bool DecodeTable(BitTable<Accessor>& table, BitMemoryReader& reader) {
   bool is_deduped = reader.ReadBit();
-  if (is_deduped) {
+  if (UNLIKELY(is_deduped)) {
     ssize_t bit_offset = reader.NumberOfReadBits() - reader.ReadVarint();
     BitMemoryReader reader2(reader.data(), bit_offset);  // The offset is negative.
     table.Decode(reader2);
@@ -47,13 +47,20 @@ ALWAYS_INLINE static bool DecodeTable(BitTable<Accessor>& table, BitMemoryReader
 
 void CodeInfo::Decode(const uint8_t* data, DecodeFlags flags) {
   BitMemoryReader reader(data);
-  ForEachHeaderField([this, &reader](auto member_pointer) {
-    this->*member_pointer = reader.ReadVarint();
-  });
+  uint32_t header[5];
+  reader.ReadVarints(header);
+  flags_ = header[0];
+  packed_frame_size_ = header[1];
+  core_spill_mask_ = header[2];
+  fp_spill_mask_ = header[3];
+  number_of_dex_registers_ = header[4];
   ForEachBitTableField([this, &reader](auto member_pointer) {
     DecodeTable(this->*member_pointer, reader);
   }, flags);
   size_in_bits_ = reader.NumberOfReadBits();
+  if (flags == AllTables) {
+    DCHECK_EQ(HasInlineInfo(data), HasInlineInfo());
+  }
 }
 
 size_t CodeInfo::Deduper::Dedupe(const uint8_t* code_info_data) {
@@ -227,6 +234,7 @@ void CodeInfo::Dump(VariableIndentationOutputStream* vios,
                     bool verbose,
                     InstructionSet instruction_set) const {
   vios->Stream() << "CodeInfo BitSize=" << size_in_bits_
+    << " Flags:" << flags_
     << " FrameSize:" << packed_frame_size_ * kStackAlignment
     << " CoreSpillMask:" << std::hex << core_spill_mask_
     << " FpSpillMask:" << std::hex << fp_spill_mask_
diff --git a/runtime/stack_map.h b/runtime/stack_map.h
index 87133cf..a971467 100644
--- a/runtime/stack_map.h
+++ b/runtime/stack_map.h
@@ -438,12 +438,15 @@ class CodeInfo {
   // Accumulate code info size statistics into the given Stats tree.
   static void CollectSizeStats(const uint8_t* code_info, /*out*/ Stats* parent);
 
-  ALWAYS_INLINE static QuickMethodFrameInfo DecodeFrameInfo(const uint8_t* data) {
-    BitMemoryReader reader(data);
-    return QuickMethodFrameInfo(
-        reader.ReadVarint() * kStackAlignment,  // Decode packed_frame_size_ and unpack.
-        reader.ReadVarint(),  // core_spill_mask_.
-        reader.ReadVarint());  // fp_spill_mask_.
+  ALWAYS_INLINE static bool HasInlineInfo(const uint8_t* code_info_data) {
+    return (*code_info_data & kHasInlineInfo) != 0;
+  }
+
+  ALWAYS_INLINE static QuickMethodFrameInfo DecodeFrameInfo(const uint8_t* code_info_data) {
+    BitMemoryReader reader(code_info_data);
+    uint32_t header[4];  // flags, packed_frame_size, core_spill_mask, fp_spill_mask.
+    reader.ReadVarints(header);
+    return QuickMethodFrameInfo(header[1] * kStackAlignment, header[2], header[3]);
   }
 
  private:
@@ -461,6 +464,7 @@ class CodeInfo {
   // Invokes the callback with member pointer of each header field.
   template<typename Callback>
   ALWAYS_INLINE static void ForEachHeaderField(Callback callback) {
+    callback(&CodeInfo::flags_);
     callback(&CodeInfo::packed_frame_size_);
     callback(&CodeInfo::core_spill_mask_);
     callback(&CodeInfo::fp_spill_mask_);
@@ -486,6 +490,11 @@ class CodeInfo {
     callback(&CodeInfo::dex_register_catalog_);
   }
 
+  enum Flags {
+    kHasInlineInfo = 1 << 0,
+  };
+
+  uint32_t flags_ = 0;
   uint32_t packed_frame_size_ = 0;  // Frame size in kStackAlignment units.
   uint32_t core_spill_mask_ = 0;
   uint32_t fp_spill_mask_ = 0;
@@ -499,6 +508,8 @@ class CodeInfo {
   BitTable<DexRegisterMapInfo> dex_register_maps_;
   BitTable<DexRegisterInfo> dex_register_catalog_;
   uint32_t size_in_bits_ = 0;
+
+  friend class StackMapStream;
 };
 
 #undef ELEMENT_BYTE_OFFSET_AFTER
diff --git a/runtime/thread.cc b/runtime/thread.cc
index be0e30a..f089c30 100644
--- a/runtime/thread.cc
+++ b/runtime/thread.cc
@@ -3708,7 +3708,6 @@ class ReferenceMapVisitor : public StackVisitor {
     VisitDeclaringClass(m);
     DCHECK(m != nullptr);
     size_t num_regs = shadow_frame->NumberOfVRegs();
-    DCHECK(m->IsNative() || shadow_frame->HasReferenceArray());
     // handle scope for JNI or References for interpreter.
     for (size_t reg = 0; reg < num_regs; ++reg) {
       mirror::Object* ref = shadow_frame->GetVRegReference(reg);
diff --git a/runtime/thread.h b/runtime/thread.h
index ae04600..fe23912 100644
--- a/runtime/thread.h
+++ b/runtime/thread.h
@@ -1130,7 +1130,9 @@ class Thread {
   uint8_t* GetTlabPos() {
     return tlsPtr_.thread_local_pos;
   }
-
+  uint8_t* GetTlabEnd() {
+    return tlsPtr_.thread_local_end;
+  }
   // Remove the suspend trigger for this thread by making the suspend_trigger_ TLS value
   // equal to a valid pointer.
   // TODO: does this need to atomic?  I don't think so.
diff --git a/test/004-JniTest/jni_test.cc b/test/004-JniTest/jni_test.cc
index 540e6ce..405ccd4 100644
--- a/test/004-JniTest/jni_test.cc
+++ b/test/004-JniTest/jni_test.cc
@@ -33,7 +33,7 @@ static JavaVM* jvm = nullptr;
 static jint Java_Main_intFastNativeMethod(JNIEnv*, jclass, jint a, jint b, jint c);
 static jint Java_Main_intCriticalNativeMethod(jint a, jint b, jint c);
 
-static JNINativeMethod sMainMethods[] = {
+static const JNINativeMethod sMainMethods[] = {
   {"intFastNativeMethod", "(III)I", reinterpret_cast<void*>(Java_Main_intFastNativeMethod) },
   {"intCriticalNativeMethod", "(III)I", reinterpret_cast<void*>(Java_Main_intCriticalNativeMethod) },
 };
diff --git a/test/139-register-natives/regnative.cc b/test/139-register-natives/regnative.cc
index d9c8b31..083c14c 100644
--- a/test/139-register-natives/regnative.cc
+++ b/test/139-register-natives/regnative.cc
@@ -22,7 +22,7 @@ namespace art {
 static void foo(JNIEnv*, jclass) {
 }
 
-static JNINativeMethod gMethods[] = {
+static const JNINativeMethod gMethods[] = {
     { "foo", "()V", reinterpret_cast<void*>(foo) }
 };
 
diff --git a/test/442-checker-constant-folding/src/Main.java b/test/442-checker-constant-folding/src/Main.java
index 3d92943..45157ab 100644
--- a/test/442-checker-constant-folding/src/Main.java
+++ b/test/442-checker-constant-folding/src/Main.java
@@ -864,6 +864,23 @@ public class Main {
     return lhs & rhs;
   }
 
+  /// CHECK-START: int Main.AndSelfNegated(int) constant_folding (before)
+  /// CHECK-DAG:     <<Arg:i\d+>>     ParameterValue
+  /// CHECK-DAG:     <<Not:i\d+>>     Not [<<Arg>>]
+  /// CHECK-DAG:     <<And:i\d+>>     And [<<Not>>,<<Arg>>]
+  /// CHECK-DAG:                      Return [<<And>>]
+
+  /// CHECK-START: int Main.AndSelfNegated(int) constant_folding (after)
+  /// CHECK-DAG:     <<Const0:i\d+>>  IntConstant 0
+  /// CHECK-DAG:                      Return [<<Const0>>]
+
+  /// CHECK-START: int Main.AndSelfNegated(int) constant_folding (after)
+  /// CHECK-NOT:                      And
+
+  public static int AndSelfNegated(int arg) {
+    return arg & ~arg;
+  }
+
 
   /**
    * Exercise constant folding on logical or.
diff --git a/test/521-checker-array-set-null/src/Main.java b/test/521-checker-array-set-null/src/Main.java
index 74bb73f..f166b92 100644
--- a/test/521-checker-array-set-null/src/Main.java
+++ b/test/521-checker-array-set-null/src/Main.java
@@ -22,19 +22,19 @@ public class Main {
   }
 
   /// CHECK-START: void Main.testWithNull(java.lang.Object[]) disassembly (after)
-  /// CHECK-NOT:      pAputObject
+  /// CHECK:          ArraySet needs_type_check:false
   public static void testWithNull(Object[] o) {
     o[0] = null;
   }
 
   /// CHECK-START: void Main.testWithUnknown(java.lang.Object[], java.lang.Object) disassembly (after)
-  /// CHECK:          pAputObject
+  /// CHECK:          ArraySet needs_type_check:true
   public static void testWithUnknown(Object[] o, Object obj) {
     o[0] = obj;
   }
 
   /// CHECK-START: void Main.testWithSame(java.lang.Object[]) disassembly (after)
-  /// CHECK-NOT:      pAputObject
+  /// CHECK:          ArraySet needs_type_check:false
   public static void testWithSame(Object[] o) {
     o[0] = o[1];
   }
diff --git a/test/536-checker-intrinsic-optimization/src/Main.java b/test/536-checker-intrinsic-optimization/src/Main.java
index 83a89a6..f15618e 100644
--- a/test/536-checker-intrinsic-optimization/src/Main.java
+++ b/test/536-checker-intrinsic-optimization/src/Main.java
@@ -18,6 +18,20 @@ import java.lang.reflect.Method;
 
 public class Main {
   public static boolean doThrow = false;
+  public static String smallString = generateString(100);
+  public static String mediumString = generateString(300);
+  public static String largeString = generateString(2000);
+
+  public static String generateString(int length) {
+    // Generate a string in the ASCII range that will
+    // use string compression.
+    StringBuilder sb = new StringBuilder();
+    for (int i = 0; i < length; i++) {
+      // Generate repeating alphabet.
+      sb.append(Character.valueOf((char)('a' + (i % 26))));
+    }
+    return sb.toString();
+  }
 
   public static void assertIntEquals(int expected, int result) {
     if (expected != result) {
@@ -37,6 +51,12 @@ public class Main {
     }
   }
 
+  public static void assertStringEquals(String expected, String result) {
+    if (!expected.equals(result)) {
+      throw new Error("Expected: " + expected + ", found: " + result);
+    }
+  }
+
   public static void assertStringContains(String searchTerm, String result) {
     if (result == null || !result.contains(searchTerm)) {
       throw new Error("Search term: " + searchTerm + ", not found in: " + result);
@@ -61,6 +81,30 @@ public class Main {
     assertCharEquals('c', $opt$noinline$stringCharAt("abc", 2));
     assertCharEquals('7', $opt$noinline$stringCharAt("0123456789", 7));
 
+    // Single character.
+    assertStringEquals("a", stringGetCharsAndBack("a"));
+    // Strings < 8 characters.
+    assertStringEquals("foobar", stringGetCharsAndBack("foobar"));
+    // Strings > 8 characters of various lengths.
+    assertStringEquals(smallString, stringGetCharsAndBack(smallString));
+    assertStringEquals(mediumString, stringGetCharsAndBack(mediumString));
+    assertStringEquals(largeString, stringGetCharsAndBack(largeString));
+
+    // Get only a substring:
+    // Substring < 8 characters.
+    assertStringEquals(smallString.substring(5, 10), stringGetCharsRange(smallString, 5, 10, 0));
+    // Substring > 8 characters.
+    assertStringEquals(smallString.substring(7, 28), stringGetCharsRange(smallString, 7, 28, 0));
+
+    // Get full string with offset in the char array.
+    assertStringEquals(smallString, stringGetCharsAndBackOffset(smallString, 17));
+
+    // Get a substring with an offset in the char array.
+    // Substring < 8 characters.
+    assertStringEquals(smallString.substring(5, 10), stringGetCharsRange(smallString, 5, 10, 17));
+    // Substring > 8 characters.
+    assertStringEquals(smallString.substring(7, 28), stringGetCharsRange(smallString, 7, 28, 17));
+
     try {
       $opt$noinline$stringCharAt("abc", -1);
       throw new Error("Should throw SIOOB.");
@@ -440,4 +484,22 @@ public class Main {
       throw new Error(ex);
     }
   }
+
+  public static String stringGetCharsAndBack(String src) {
+    char[] dst = new char[src.length()];
+    src.getChars(0, src.length(), dst, 0);
+    return new String(dst);
+  }
+
+  public static String stringGetCharsAndBackOffset(String src, int offset) {
+    char[] dst = new char[src.length() + offset];
+    src.getChars(0, src.length(), dst, offset);
+    return new String(dst, offset, src.length());
+  }
+
+  public static String stringGetCharsRange(String src, int srcBegin, int srcEnd, int offset) {
+    char[] dst = new char[srcEnd - srcBegin + offset];
+    src.getChars(srcBegin, srcEnd, dst, offset);
+    return new String(dst, offset, srcEnd - srcBegin);
+  }
 }
diff --git a/test/597-deopt-busy-loop/expected.txt b/test/597-deopt-busy-loop/expected.txt
index f993efc..6e2fc8e 100644
--- a/test/597-deopt-busy-loop/expected.txt
+++ b/test/597-deopt-busy-loop/expected.txt
@@ -1,2 +1,3 @@
 JNI_OnLoad called
-Finishing
+Simple loop finishing
+Float loop finishing
diff --git a/test/597-deopt-busy-loop/src/Main.java b/test/597-deopt-busy-loop/src/Main.java
index 46b6bbf..d15c6c7 100644
--- a/test/597-deopt-busy-loop/src/Main.java
+++ b/test/597-deopt-busy-loop/src/Main.java
@@ -14,56 +14,22 @@
  * limitations under the License.
  */
 
-public class Main implements Runnable {
-    static final int numberOfThreads = 2;
-    volatile static boolean sExitFlag = false;
-    volatile static boolean sEntered = false;
-    int threadIndex;
+public class Main {
 
-    private static native void deoptimizeAll();
-    private static native void assertIsInterpreted();
-    private static native void assertIsManaged();
+    public static native void deoptimizeAll();
+    public static native void undeoptimizeAll();
+    public static native void assertIsInterpreted();
+    public static native void assertIsManaged();
     private static native void ensureJitCompiled(Class<?> cls, String methodName);
 
-    Main(int index) {
-        threadIndex = index;
-    }
-
     public static void main(String[] args) throws Exception {
         System.loadLibrary(args[0]);
 
-        final Thread[] threads = new Thread[numberOfThreads];
-        for (int t = 0; t < threads.length; t++) {
-            threads[t] = new Thread(new Main(t));
-            threads[t].start();
-        }
-        for (Thread t : threads) {
-            t.join();
-        }
-        System.out.println("Finishing");
-    }
-
-    public void $noinline$busyLoop() {
-        assertIsManaged();
-        sEntered = true;
-        for (;;) {
-            if (sExitFlag) {
-                break;
-            }
-        }
-        assertIsInterpreted();
-    }
+        ensureJitCompiled(SimpleLoop.class, "$noinline$busyLoop");
+        SimpleLoop.main();
 
-    public void run() {
-        if (threadIndex == 0) {
-            while (!sEntered) {
-              Thread.yield();
-            }
-            deoptimizeAll();
-            sExitFlag = true;
-        } else {
-            ensureJitCompiled(Main.class, "$noinline$busyLoop");
-            $noinline$busyLoop();
-        }
+        undeoptimizeAll();
+        ensureJitCompiled(FloatLoop.class, "$noinline$busyLoop");
+        FloatLoop.main();
     }
 }
diff --git a/test/624-checker-stringops/smali/Smali.smali b/test/624-checker-stringops/smali/Smali.smali
index f8b9275..3252cde 100644
--- a/test/624-checker-stringops/smali/Smali.smali
+++ b/test/624-checker-stringops/smali/Smali.smali
@@ -47,17 +47,17 @@
 ## CHECK-START: int Smali.builderLen2() instruction_simplifier (before)
 ## CHECK-DAG: <<New:l\d+>>     NewInstance
 ## CHECK-DAG: <<String1:l\d+>> LoadString
-## CHECK-DAG: <<Append1:l\d+>> InvokeVirtual [<<New>>,<<String1>>]     intrinsic:StringBuilderAppend
+## CHECK-DAG: <<Append1:l\d+>> InvokeVirtual [<<New>>,<<String1>>]     intrinsic:StringBuilderAppendString
 ## CHECK-DAG: <<String2:l\d+>> LoadString
-## CHECK-DAG: <<Append2:l\d+>> InvokeVirtual [<<Append1>>,<<String2>>] intrinsic:StringBuilderAppend
+## CHECK-DAG: <<Append2:l\d+>> InvokeVirtual [<<Append1>>,<<String2>>] intrinsic:StringBuilderAppendString
 ## CHECK-DAG:                  InvokeVirtual [<<Append2>>]             intrinsic:StringBuilderLength
 
 ## CHECK-START: int Smali.builderLen2() instruction_simplifier (after)
 ## CHECK-DAG: <<New:l\d+>>     NewInstance
 ## CHECK-DAG: <<String1:l\d+>> LoadString
-## CHECK-DAG: <<Append1:l\d+>> InvokeVirtual [<<New>>,<<String1>>] intrinsic:StringBuilderAppend
+## CHECK-DAG: <<Append1:l\d+>> InvokeVirtual [<<New>>,<<String1>>] intrinsic:StringBuilderAppendString
 ## CHECK-DAG: <<String2:l\d+>> LoadString
-## CHECK-DAG: <<Append2:l\d+>> InvokeVirtual [<<New>>,<<String2>>] intrinsic:StringBuilderAppend
+## CHECK-DAG: <<Append2:l\d+>> InvokeVirtual [<<New>>,<<String2>>] intrinsic:StringBuilderAppendString
 ## CHECK-DAG:                  InvokeVirtual [<<New>>]             intrinsic:StringBuilderLength
 .method public static builderLen2()I
     .registers 3
@@ -84,13 +84,13 @@
 ## CHECK-DAG: <<New:l\d+>>     NewInstance                                                           loop:none
 ## CHECK-DAG: <<String1:l\d+>> LoadString                                                            loop:<<Loop:B\d+>>
 ## CHECK-DAG: <<Null1:l\d+>>   NullCheck     [<<New>>]                                               loop:<<Loop>>
-## CHECK-DAG: <<Append1:l\d+>> InvokeVirtual [<<Null1>>,<<String1>>] intrinsic:StringBufferAppend    loop:<<Loop>>
+## CHECK-DAG: <<Append1:l\d+>> InvokeVirtual [<<Null1>>,<<String1>>]   intrinsic:StringBufferAppend  loop:<<Loop>>
 ## CHECK-DAG: <<String2:l\d+>> LoadString                                                            loop:<<Loop>>
 ## CHECK-DAG: <<Append2:l\d+>> InvokeVirtual [<<Append1>>,<<String2>>] intrinsic:StringBufferAppend  loop:<<Loop>>
 ## CHECK-DAG: <<String3:l\d+>> LoadString                                                            loop:<<Loop>>
 ## CHECK-DAG: <<Append3:l\d+>> InvokeVirtual [<<Append2>>,<<String3>>] intrinsic:StringBufferAppend  loop:<<Loop>>
 ## CHECK-DAG: <<Null4:l\d+>>   NullCheck     [<<New>>]                                               loop:none
-## CHECK-DAG:                  InvokeVirtual [<<Null4>>]             intrinsic:StringBufferLength    loop:none
+## CHECK-DAG:                  InvokeVirtual [<<Null4>>]               intrinsic:StringBufferLength  loop:none
 
 ## CHECK-START: int Smali.bufferLoopAppender() instruction_simplifier (after)
 ## CHECK-DAG: <<New:l\d+>>     NewInstance                                                       loop:none
@@ -138,26 +138,26 @@
 .end method
 
 ## CHECK-START: int Smali.builderLoopAppender() instruction_simplifier (before)
-## CHECK-DAG: <<New:l\d+>>     NewInstance                                                           loop:none
-## CHECK-DAG: <<String1:l\d+>> LoadString                                                            loop:<<Loop:B\d+>>
-## CHECK-DAG: <<Null1:l\d+>>   NullCheck     [<<New>>]                                               loop:<<Loop>>
-## CHECK-DAG: <<Append1:l\d+>> InvokeVirtual [<<Null1>>,<<String1>>]   intrinsic:StringBuilderAppend loop:<<Loop>>
-## CHECK-DAG: <<String2:l\d+>> LoadString                                                            loop:<<Loop>>
-## CHECK-DAG: <<Append2:l\d+>> InvokeVirtual [<<Append1>>,<<String2>>] intrinsic:StringBuilderAppend loop:<<Loop>>
-## CHECK-DAG: <<String3:l\d+>> LoadString                                                            loop:<<Loop>>
-## CHECK-DAG: <<Append3:l\d+>> InvokeVirtual [<<Append2>>,<<String3>>] intrinsic:StringBuilderAppend loop:<<Loop>>
-## CHECK-DAG: <<Null4:l\d+>>   NullCheck     [<<New>>]                                               loop:none
-## CHECK-DAG:                  InvokeVirtual [<<Null4>>]               intrinsic:StringBuilderLength loop:none
+## CHECK-DAG: <<New:l\d+>>     NewInstance                                                                 loop:none
+## CHECK-DAG: <<String1:l\d+>> LoadString                                                                  loop:<<Loop:B\d+>>
+## CHECK-DAG: <<Null1:l\d+>>   NullCheck     [<<New>>]                                                     loop:<<Loop>>
+## CHECK-DAG: <<Append1:l\d+>> InvokeVirtual [<<Null1>>,<<String1>>]   intrinsic:StringBuilderAppendString loop:<<Loop>>
+## CHECK-DAG: <<String2:l\d+>> LoadString                                                                  loop:<<Loop>>
+## CHECK-DAG: <<Append2:l\d+>> InvokeVirtual [<<Append1>>,<<String2>>] intrinsic:StringBuilderAppendString loop:<<Loop>>
+## CHECK-DAG: <<String3:l\d+>> LoadString                                                                  loop:<<Loop>>
+## CHECK-DAG: <<Append3:l\d+>> InvokeVirtual [<<Append2>>,<<String3>>] intrinsic:StringBuilderAppendString loop:<<Loop>>
+## CHECK-DAG: <<Null4:l\d+>>   NullCheck     [<<New>>]                                                     loop:none
+## CHECK-DAG:                  InvokeVirtual [<<Null4>>]               intrinsic:StringBuilderLength       loop:none
 
 ## CHECK-START: int Smali.builderLoopAppender() instruction_simplifier (after)
-## CHECK-DAG: <<New:l\d+>>     NewInstance                                                       loop:none
-## CHECK-DAG: <<String1:l\d+>> LoadString                                                        loop:<<Loop:B\d+>>
-## CHECK-DAG: <<Append1:l\d+>> InvokeVirtual [<<New>>,<<String1>>] intrinsic:StringBuilderAppend loop:<<Loop>>
-## CHECK-DAG: <<String2:l\d+>> LoadString                                                        loop:<<Loop>>
-## CHECK-DAG: <<Append2:l\d+>> InvokeVirtual [<<New>>,<<String2>>] intrinsic:StringBuilderAppend loop:<<Loop>>
-## CHECK-DAG: <<String3:l\d+>> LoadString                                                        loop:<<Loop>>
-## CHECK-DAG: <<Append3:l\d+>> InvokeVirtual [<<New>>,<<String3>>] intrinsic:StringBuilderAppend loop:<<Loop>>
-## CHECK-DAG:                  InvokeVirtual [<<New>>]             intrinsic:StringBuilderLength loop:none
+## CHECK-DAG: <<New:l\d+>>     NewInstance                                                             loop:none
+## CHECK-DAG: <<String1:l\d+>> LoadString                                                              loop:<<Loop:B\d+>>
+## CHECK-DAG: <<Append1:l\d+>> InvokeVirtual [<<New>>,<<String1>>] intrinsic:StringBuilderAppendString loop:<<Loop>>
+## CHECK-DAG: <<String2:l\d+>> LoadString                                                              loop:<<Loop>>
+## CHECK-DAG: <<Append2:l\d+>> InvokeVirtual [<<New>>,<<String2>>] intrinsic:StringBuilderAppendString loop:<<Loop>>
+## CHECK-DAG: <<String3:l\d+>> LoadString                                                              loop:<<Loop>>
+## CHECK-DAG: <<Append3:l\d+>> InvokeVirtual [<<New>>,<<String3>>] intrinsic:StringBuilderAppendString loop:<<Loop>>
+## CHECK-DAG:                  InvokeVirtual [<<New>>]             intrinsic:StringBuilderLength       loop:none
 .method public static builderLoopAppender()I
     .registers 4
 
diff --git a/test/624-checker-stringops/src/Main.java b/test/624-checker-stringops/src/Main.java
index f52d81a..055a4d7 100644
--- a/test/624-checker-stringops/src/Main.java
+++ b/test/624-checker-stringops/src/Main.java
@@ -136,17 +136,17 @@ public class Main {
   /// CHECK-START: int Main.builderLen2() instruction_simplifier (before)
   /// CHECK-DAG: <<New:l\d+>>     NewInstance
   /// CHECK-DAG: <<String1:l\d+>> LoadString
-  /// CHECK-DAG: <<Append1:l\d+>> InvokeVirtual [<<New>>,<<String1>>]  intrinsic:StringBuilderAppend
+  /// CHECK-DAG: <<Append1:l\d+>> InvokeVirtual [<<New>>,<<String1>>]  intrinsic:StringBuilderAppendString
   /// CHECK-DAG: <<String2:l\d+>> LoadString
-  /// CHECK-DAG: <<Append2:l\d+>> InvokeVirtual [{{l\d+}},<<String2>>] intrinsic:StringBuilderAppend
+  /// CHECK-DAG: <<Append2:l\d+>> InvokeVirtual [{{l\d+}},<<String2>>] intrinsic:StringBuilderAppendString
   /// CHECK-DAG:                  InvokeVirtual [{{l\d+}}]             intrinsic:StringBuilderLength
   //
   /// CHECK-START: int Main.builderLen2() instruction_simplifier (after)
   /// CHECK-DAG: <<New:l\d+>>     NewInstance
   /// CHECK-DAG: <<String1:l\d+>> LoadString
-  /// CHECK-DAG: <<Append1:l\d+>> InvokeVirtual [<<New>>,<<String1>>] intrinsic:StringBuilderAppend
+  /// CHECK-DAG: <<Append1:l\d+>> InvokeVirtual [<<New>>,<<String1>>] intrinsic:StringBuilderAppendString
   /// CHECK-DAG: <<String2:l\d+>> LoadString
-  /// CHECK-DAG: <<Append2:l\d+>> InvokeVirtual [<<New>>,<<String2>>] intrinsic:StringBuilderAppend
+  /// CHECK-DAG: <<Append2:l\d+>> InvokeVirtual [<<New>>,<<String2>>] intrinsic:StringBuilderAppendString
   /// CHECK-DAG:                  InvokeVirtual [<<New>>]             intrinsic:StringBuilderLength
   static int builderLen2() {
     StringBuilder s = new StringBuilder();
@@ -200,25 +200,25 @@ public class Main {
   // Similar situation in a loop.
   //
   /// CHECK-START: int Main.builderLoopAppender() instruction_simplifier (before)
-  /// CHECK-DAG: <<New:l\d+>>     NewInstance                                                         loop:none
-  /// CHECK-DAG: <<String1:l\d+>> LoadString                                                          loop:<<Loop:B\d+>>
-  /// CHECK-DAG: <<Null1:l\d+>>   NullCheck     [<<New>>]                                             loop:<<Loop>>
-  /// CHECK-DAG: <<Append1:l\d+>> InvokeVirtual [<<Null1>>,<<String1>>] intrinsic:StringBuilderAppend loop:<<Loop>>
-  /// CHECK-DAG: <<String2:l\d+>> LoadString                                                          loop:<<Loop>>
-  /// CHECK-DAG: <<Append2:l\d+>> InvokeVirtual [{{l\d+}},<<String2>>]  intrinsic:StringBuilderAppend loop:<<Loop>>
-  /// CHECK-DAG: <<String3:l\d+>> LoadString                                                          loop:<<Loop>>
-  /// CHECK-DAG: <<Append3:l\d+>> InvokeVirtual [{{l\d+}},<<String3>>]  intrinsic:StringBuilderAppend loop:<<Loop>>
-  /// CHECK-DAG:                  InvokeVirtual [{{l\d+}}]              intrinsic:StringBuilderLength loop:none
+  /// CHECK-DAG: <<New:l\d+>>     NewInstance                                                               loop:none
+  /// CHECK-DAG: <<String1:l\d+>> LoadString                                                                loop:<<Loop:B\d+>>
+  /// CHECK-DAG: <<Null1:l\d+>>   NullCheck     [<<New>>]                                                   loop:<<Loop>>
+  /// CHECK-DAG: <<Append1:l\d+>> InvokeVirtual [<<Null1>>,<<String1>>] intrinsic:StringBuilderAppendString loop:<<Loop>>
+  /// CHECK-DAG: <<String2:l\d+>> LoadString                                                                loop:<<Loop>>
+  /// CHECK-DAG: <<Append2:l\d+>> InvokeVirtual [{{l\d+}},<<String2>>]  intrinsic:StringBuilderAppendString loop:<<Loop>>
+  /// CHECK-DAG: <<String3:l\d+>> LoadString                                                                loop:<<Loop>>
+  /// CHECK-DAG: <<Append3:l\d+>> InvokeVirtual [{{l\d+}},<<String3>>]  intrinsic:StringBuilderAppendString loop:<<Loop>>
+  /// CHECK-DAG:                  InvokeVirtual [{{l\d+}}]              intrinsic:StringBuilderLength       loop:none
   //
   /// CHECK-START: int Main.builderLoopAppender() instruction_simplifier (after)
-  /// CHECK-DAG: <<New:l\d+>>     NewInstance                                                       loop:none
-  /// CHECK-DAG: <<String1:l\d+>> LoadString                                                        loop:<<Loop:B\d+>>
-  /// CHECK-DAG: <<Append1:l\d+>> InvokeVirtual [<<New>>,<<String1>>] intrinsic:StringBuilderAppend loop:<<Loop>>
-  /// CHECK-DAG: <<String2:l\d+>> LoadString                                                        loop:<<Loop>>
-  /// CHECK-DAG: <<Append2:l\d+>> InvokeVirtual [<<New>>,<<String2>>] intrinsic:StringBuilderAppend loop:<<Loop>>
-  /// CHECK-DAG: <<String3:l\d+>> LoadString                                                        loop:<<Loop>>
-  /// CHECK-DAG: <<Append3:l\d+>> InvokeVirtual [<<New>>,<<String3>>] intrinsic:StringBuilderAppend loop:<<Loop>>
-  /// CHECK-DAG:                  InvokeVirtual [<<New>>]             intrinsic:StringBuilderLength loop:none
+  /// CHECK-DAG: <<New:l\d+>>     NewInstance                                                             loop:none
+  /// CHECK-DAG: <<String1:l\d+>> LoadString                                                              loop:<<Loop:B\d+>>
+  /// CHECK-DAG: <<Append1:l\d+>> InvokeVirtual [<<New>>,<<String1>>] intrinsic:StringBuilderAppendString loop:<<Loop>>
+  /// CHECK-DAG: <<String2:l\d+>> LoadString                                                              loop:<<Loop>>
+  /// CHECK-DAG: <<Append2:l\d+>> InvokeVirtual [<<New>>,<<String2>>] intrinsic:StringBuilderAppendString loop:<<Loop>>
+  /// CHECK-DAG: <<String3:l\d+>> LoadString                                                              loop:<<Loop>>
+  /// CHECK-DAG: <<Append3:l\d+>> InvokeVirtual [<<New>>,<<String3>>] intrinsic:StringBuilderAppendString loop:<<Loop>>
+  /// CHECK-DAG:                  InvokeVirtual [<<New>>]             intrinsic:StringBuilderLength       loop:none
   static int builderLoopAppender() {
     StringBuilder b = new StringBuilder();
     for (int i = 0; i < 10; i++) {
diff --git a/test/661-checker-simd-reduc/src/Main.java b/test/661-checker-simd-reduc/src/Main.java
index 7b6f957..c31b17c 100644
--- a/test/661-checker-simd-reduc/src/Main.java
+++ b/test/661-checker-simd-reduc/src/Main.java
@@ -71,6 +71,12 @@ public class Main {
   /// CHECK-DAG:                 Add [<<I>>,<<Cons>>]          loop:<<Loop>>      outer_loop:none
   /// CHECK-DAG: <<Red:d\d+>>    VecReduce [<<Phi>>]           loop:none
   /// CHECK-DAG: <<Extr:i\d+>>   VecExtractScalar [<<Red>>]    loop:none
+
+  //  Check that full 128-bit Q-Register are saved across SuspendCheck slow path.
+  /// CHECK-START-ARM64: int Main.reductionInt(int[]) disassembly (after)
+  /// CHECK:                     SuspendCheckSlowPathARM64
+  /// CHECK:                       stur q<<RegNo:\d+>>, [sp, #<<Offset:\d+>>]
+  /// CHECK:                       ldur q<<RegNo>>, [sp, #<<Offset>>]
   private static int reductionInt(int[] x) {
     int sum = 0;
     for (int i = 0; i < x.length; i++) {
diff --git a/test/706-checker-scheduler/src/Main.java b/test/706-checker-scheduler/src/Main.java
index af18193..5a66fbb 100644
--- a/test/706-checker-scheduler/src/Main.java
+++ b/test/706-checker-scheduler/src/Main.java
@@ -322,7 +322,7 @@ public class Main {
   // but has more complex chains of transforming the original references:
   // ParameterValue --> BoundType --> NullCheck --> ArrayGet.
   // ParameterValue --> BoundType --> NullCheck --> IntermediateAddress --> ArraySet.
-  // After using LSA to analyze the orginal references, the scheduler should be able
+  // After using LSA to analyze the original references, the scheduler should be able
   // to find out that 'a' and 'b' may alias, hence unable to schedule these ArraGet/Set.
 
   /// CHECK-START-ARM64: void Main.CrossOverLoop2(java.lang.Object, java.lang.Object) scheduler (before)
@@ -584,9 +584,126 @@ public class Main {
     }
   }
 
+  // Check that instructions having cross iteration dependencies are not
+  // reordered.
+  //
+  /// CHECK-START-{ARM,ARM64}: void Main.testCrossItersDependencies() scheduler (before)
+  /// CHECK:     <<ID1:i\d+>>  Phi [{{i\d+}},<<ID3:i\d+>>]
+  /// CHECK:     <<ID2:i\d+>>  Phi [{{i\d+}},<<ID4:i\d+>>]
+  //
+  /// CHECK:     <<ID3>>  Sub [<<ID1>>,<<ID2>>]
+  /// CHECK:     <<ID4>>  Add [<<ID2>>,{{i\d+}}]
+
+  /// CHECK-START-{ARM,ARM64}: void Main.testCrossItersDependencies() scheduler (after)
+  /// CHECK:     <<ID1:i\d+>>  Phi [{{i\d+}},<<ID3:i\d+>>]
+  /// CHECK:     <<ID2:i\d+>>  Phi [{{i\d+}},<<ID4:i\d+>>]
+  //
+  /// CHECK:     <<ID3>>  Sub [<<ID1>>,<<ID2>>]
+  /// CHECK:     <<ID4>>  Add [<<ID2>>,{{i\d+}}]
+
+  /// CHECK-START-ARM: void Main.testCrossItersDependencies() disassembly (after)
+  /// CHECK:     subs
+  /// CHECK:     add
+  /// CHECK:     adds
+  /// CHECK:     ldrh
+  /// CHECK:     cmp
+  /// CHECK:     beq
+
+  /// CHECK-START-ARM64: void Main.testCrossItersDependencies() disassembly (after)
+  /// CHECK:     sub
+  /// CHECK:     add
+  /// CHECK:     add
+  /// CHECK:     ldrh
+  /// CHECK:     cbz
+  private static void testCrossItersDependencies() {
+    int[] data = {1, 2, 3, 0};
+    int sub = 0;
+    int sum = data[0];
+    for (int i = 1; data[i] != 0; ++i) {
+      sub -= sum;
+      sum += data[i];
+    }
+    expectEquals(sub, -4);
+    expectEquals(sum, 6);
+  }
+
+  // Check instructions defining values for the next iteration don't become
+  // self-dependent in a scheduling graph which prevents valid reordering.
+  //
+  /// CHECK-START-{ARM,ARM64}: void Main.testNoSelfDependantSchedNode(int) scheduler (before)
+  /// CHECK:     IntermediateAddress
+  /// CHECK:     ArrayGet
+  /// CHECK:     LessThanOrEqual
+  /// CHECK:     Select
+  /// CHECK:     IntermediateAddress
+  /// CHECK:     ArraySet
+  /// CHECK:     Add
+
+  /// CHECK-START-{ARM,ARM64}: void Main.testNoSelfDependantSchedNode(int) scheduler (after)
+  /// CHECK:     IntermediateAddress
+  /// CHECK:     ArrayGet
+  /// CHECK:     IntermediateAddress
+  /// CHECK:     LessThanOrEqual
+  /// CHECK:     Select
+  /// CHECK:     ArraySet
+  /// CHECK:     Add
+  //
+  // Parameter n is to prevent unrolling of the main loop.
+  private static void testNoSelfDependantSchedNode(int n) {
+    final int MAX = 2;
+    int[] a = {1, 2, 3};
+    int[] b = new int[a.length];
+    n = Math.min(n, a.length);
+    for (int i = 0; i < n; ++i) {
+      int j = a[i];
+      b[i] = (j > MAX ? MAX : 0);
+    }
+    expectEquals(b[0], 0);
+    expectEquals(b[1], 0);
+    expectEquals(b[2], 2);
+  }
+
+  // In case of cross iteration dependencies when a value for the next iteration is also used on
+  // the current iteration a MOV instruction is generated anyway. In such cases setting dependency
+  // between scheduling nodes will not eliminate MOV.
+  // In the test 'i+1' is such an example.
+  // The test checks that a dependency between scheduling nodes (first ArrayGet and Add) is not
+  // setup and Add is scheduled before ArrayGet.
+  //
+  /// CHECK-START-{ARM,ARM64}: void Main.testNonPreventingSchedulingCrossItersDeps(int) scheduler (before)
+  /// CHECK:          IntermediateAddress
+  /// CHECK-NEXT:     ArrayGet
+  /// CHECK-NEXT:     Add
+  /// CHECK-NEXT:     ArrayGet
+
+  /// CHECK-START-{ARM,ARM64}: void Main.testNonPreventingSchedulingCrossItersDeps(int) scheduler (after)
+  /// CHECK:          IntermediateAddress
+  /// CHECK-NEXT:     Add
+  /// CHECK-NEXT:     ArrayGet
+  /// CHECK-NEXT:     ArrayGet
+  //
+  // Parameter n is to prevent unrolling of the main loop.
+  private static void testNonPreventingSchedulingCrossItersDeps(int n) {
+    int[] a = {1, 2, 3};
+    n = Math.min(n, a.length);
+    for (int i = 0; i < n - 1; ++i) {
+      if (a[i] < a[i + 1]) {
+        int tmp = a[i];
+        a[i] = a[i + 1];
+        a[i + 1] = tmp;
+      }
+    }
+    expectEquals(a[0], 2);
+    expectEquals(a[1], 3);
+    expectEquals(a[2], 1);
+  }
+
   public static void main(String[] args) {
     testVecSetScalars();
     testVecReplicateScalar();
+    testCrossItersDependencies();
+    testNoSelfDependantSchedNode(3);
+    testNonPreventingSchedulingCrossItersDeps(3);
     if ((arrayAccess() + intDiv(10)) != -35) {
       System.out.println("FAIL");
     }
diff --git a/test/knownfailures.json b/test/knownfailures.json
index b38d4d4..2432a9d 100644
--- a/test/knownfailures.json
+++ b/test/knownfailures.json
@@ -942,6 +942,7 @@
           "574-irreducible-and-constant-area",
           "575-checker-string-init-alias",
           "580-checker-string-fact-intrinsics",
+          "580-fp16",
           "585-inline-unresolved",
           "586-checker-null-array-get",
           "587-inline-class-error",
